{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fef3d3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ll16598/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import math\n",
    "from gensim.models import coherencemodel\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gc\n",
    "import pickle\n",
    "from scipy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities #calc all similarities at once, from http://radimrehurek.com/gensim/tut3.html\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from random import seed, sample\n",
    "import seaborn as sns\n",
    "from ksvd import ApproximateKSVD \n",
    "import random\n",
    "import ast\n",
    "import html\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "#np.set_printoptions(threshold=np.inf) #set to print full output\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "#os.chdir('/home/ll16598/Documents/Altered_States_Reddit/model_pipeline')\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel  # Import AutoTokenizer and AutoModel\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from scipy.stats import zscore\n",
    "#nltk.download('stopwords')\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import umap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy.stats as stats\n",
    "import gc  # Garbage collection module\n",
    "from scipy.stats import sem\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "#sys.path.append('/home/ll16598/Documents/Altered_States_Reddit/model_pipeline/__pycache__')\n",
    "#from quality import reconst_qual, topic_diversity, coherence_centroid, coherence_pairwise #written for this jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dd1c0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi as gd\n",
    "import gudhi.representations\n",
    "\n",
    "def compute_persistence_diagram(data):\n",
    "    # Compute the Rips complex\n",
    "    rips_complex = gd.RipsComplex(points=data, max_edge_length=2.0)\n",
    "    # Construct a simplex tree\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "    # Compute persistent homology\n",
    "    persistence = simplex_tree.persistence()\n",
    "    # Plot persistence diagram`\n",
    "    gd.plot_persistence_diagram(persistence)\n",
    "    gd.plot_persistence_barcode(persistence)\n",
    "\n",
    "    plt.show()\n",
    "    return persistence\n",
    "\n",
    "def add_geometric_centroid(data):\n",
    "    \"\"\"\n",
    "    Given an (N, D) array 'data' of N points in D dimensions,\n",
    "    computes the mean (geometric centroid) and appends it\n",
    "    as an extra row at the end.\n",
    "\n",
    "    Returns:\n",
    "      data_with_centroid: an (N+1, D) array,\n",
    "        where the last row is the centroid.\n",
    "      centroid_index: the integer index of the new centroid row.\n",
    "    \"\"\"\n",
    "    centroid = np.mean(data, axis=0)             # shape (D,)\n",
    "    data_with_centroid = np.vstack([data, centroid])\n",
    "    centroid_index = data_with_centroid.shape[0] - 1\n",
    "    return data_with_centroid, centroid_index\n",
    "\n",
    "def build_centroid_distance_matrix(data_with_centroid, centroid_index, large_val=1e6):\n",
    "    \"\"\"\n",
    "    Creates a distance matrix where only edges from the 'centroid_index'\n",
    "    to other points have the real Euclidean distance.\n",
    "    All other pairwise distances are set to 'large_val'.\n",
    "    \"\"\"\n",
    "    N = data_with_centroid.shape[0]\n",
    "    dist_matrix = np.full((N, N), large_val, dtype=float)\n",
    "\n",
    "    # Diagonal = 0\n",
    "    np.fill_diagonal(dist_matrix, 0.0)\n",
    "\n",
    "    # Compute distances between centroid and each other point\n",
    "    for i in range(N):\n",
    "        if i == centroid_index:\n",
    "            continue\n",
    "        # Real distance from centroid -> i\n",
    "        dist = cosine_similarity(data_with_centroid[centroid_index].reshape(1,-1),\\\n",
    "                                 data_with_centroid[i].reshape(1,-1))\n",
    "        dist_matrix[centroid_index, i] = dist\n",
    "        dist_matrix[i, centroid_index] = dist\n",
    "\n",
    "    return dist_matrix\n",
    "\n",
    "def compute_persistence_centroid(data, max_edge_length=2.0, plotting=True):\n",
    "    \"\"\"\n",
    "    1) Compute the geometric centroid of 'data' and append it as an extra row.\n",
    "    2) Build a distance matrix such that only the centroid can connect to other points.\n",
    "    3) Construct the Rips complex using the custom distance matrix.\n",
    "    4) Compute and plot the persistence diagram and barcode.\n",
    "\n",
    "    Returns:\n",
    "      persistence: The list of (dim, (birth, death)) intervals from GUDHI\n",
    "    \"\"\"\n",
    "    # 1) Add the centroid\n",
    "    data_with_centroid, centroid_index = add_geometric_centroid(data)\n",
    "\n",
    "    # 2) Build the custom distance matrix\n",
    "    dist_matrix = build_centroid_distance_matrix(data_with_centroid, centroid_index, large_val=1e6)\n",
    "\n",
    "    # 3) Create RipsComplex from the distance matrix\n",
    "    rips_complex = gd.RipsComplex(distance_matrix=dist_matrix, max_edge_length=max_edge_length)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    del dist_matrix  # Delete objects\n",
    "    gc.collect()  # Force garbage collection to free memory\n",
    "    # 4) Compute persistent homology\n",
    "    persistence = simplex_tree.persistence()\n",
    "    if not plotting:\n",
    "        return rips_complex, simplex_tree, persistence\n",
    "    # 5) Plot the persistence diagram & barcode\n",
    "    gd.plot_persistence_diagram(persistence)\n",
    "    gd.plot_persistence_barcode(persistence)\n",
    "    plt.show()\n",
    "\n",
    "    return rips_complex, simplex_tree, persistence\n",
    "\n",
    "\n",
    "def get_alive_components_over_scales(births, deaths, step=0.025):\n",
    "    \"\"\"\n",
    "    Given lists of (birth, death) intervals for a particular homology dimension,\n",
    "    compute how many such features (e.g., connected components if D=0, loops if D=1, etc.)\n",
    "    are 'alive' at increments of 'step' from 0 up to max(deaths).\n",
    "\n",
    "    Returns:\n",
    "    - scales: list of scale values (0, 0.025, 0.05, ...)\n",
    "    - alive_counts: corresponding list of how many features are alive at each scale\n",
    "    \"\"\"\n",
    "    if len(births) == 0:\n",
    "        # No intervals => no features\n",
    "        return [], []\n",
    "    \n",
    "    max_death = max(deaths)\n",
    "    scales = np.arange(0, max_death + 1e-9, step)\n",
    "    \n",
    "    alive_counts = []\n",
    "    for s in scales:\n",
    "        # Count intervals that are alive: birth <= s < death\n",
    "        count_alive = sum(1 for (b, d) in zip(births, deaths) if b <= s < d)\n",
    "        alive_counts.append(count_alive)\n",
    "    \n",
    "    return list(scales), alive_counts\n",
    "\n",
    "def get_rips_time(df, embeddings='sentence_embeddings_pca', step=0.05, D=0):\n",
    "    \"\"\"\n",
    "    For each row in df, build a Rips complex, extract dimension-D intervals\n",
    "    (e.g., D=0 => connected components, D=1 => loops, etc.),\n",
    "    then compute how many such features are 'alive' at increments of 'step'.\n",
    "\n",
    "    Creates two new columns in df:\n",
    "    - f\"scales_dim{D}\": The scale values\n",
    "    - f\"alive_dim{D}\": The counts of alive features at each scale\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid SettingWithCopy warnings\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Prepare two new columns (lists)\n",
    "    df[f'scales_dim{D}'] = None\n",
    "    df[f'alive_dim{D}'] = None\n",
    "    df['rt'] = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Get the embeddings for this row\n",
    "        embed = row[embeddings]\n",
    "        if not isinstance(embed, (list, np.ndarray)) or len(embed) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Build the Rips Complex\n",
    "        rips_complex = gd.RipsComplex(points=embed, max_edge_length=5.0)\n",
    "        simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "\n",
    "        # Extract dimension-D intervals from persistence\n",
    "        persistence = simplex_tree.persistence()\n",
    "        \n",
    "        births_dimD = []\n",
    "        deaths_dimD = []\n",
    "        \n",
    "        for dim, (b, d) in persistence:\n",
    "            if dim == D and d != float('inf'):  # ignoring infinite intervals\n",
    "                births_dimD.append(b)\n",
    "                deaths_dimD.append(d)\n",
    "\n",
    "        # Compute how many features are alive at each scale\n",
    "        scales, alive_components = get_alive_components_over_scales(births_dimD, deaths_dimD, step=step)\n",
    "        if len(deaths_dimD)>0:\n",
    "            df.at[idx, f\"rt\"] = max(deaths_dimD)\n",
    "\n",
    "        # Store these lists in the new columns\n",
    "        df.at[idx, f\"scales_dim{D}\"] = scales\n",
    "        df.at[idx, f\"alive_dim{D}\"] = alive_components\n",
    "\n",
    "    return df\n",
    "def get_rips_time_centroid(df, embeddings='sentence_embeddings_pca', step=0.05, D=0):\n",
    "    \"\"\"\n",
    "    For each row in df, build a Rips complex, extract dimension-D intervals\n",
    "    (e.g., D=0 => connected components, D=1 => loops, etc.),\n",
    "    then compute how many such features are 'alive' at increments of 'step'.\n",
    "\n",
    "    Creates two new columns in df:\n",
    "    - f\"scales_dim{D}\": The scale values\n",
    "    - f\"alive_dim{D}\": The counts of alive features at each scale\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid SettingWithCopy warnings\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Prepare two new columns (lists)\n",
    "    df[f'centroid_scales_dim{D}'] = None\n",
    "    df[f'centroid_alive_dim{D}'] = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "#         if idx in [26, 27, 28]:\n",
    "#             print('warning: skipping a computationally intensive sample')\n",
    "#             continue\n",
    "        # Get the embeddings for this row\n",
    "        embed = row[embeddings]\n",
    "        if not isinstance(embed, (list, np.ndarray)) or len(embed) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Build the Rips Complex\n",
    "        rips_complex, simplex_tree,persistence= compute_persistence_centroid(embed,max_edge_length=4,plotting=False)\n",
    "        births_dimD = []\n",
    "        deaths_dimD = []\n",
    "        \n",
    "        for dim, (b, d) in persistence:\n",
    "            if dim == D and d != float('inf'):  # ignoring infinite intervals\n",
    "                births_dimD.append(b)\n",
    "                deaths_dimD.append(d)\n",
    "\n",
    "        # Compute how many features are alive at each scale\n",
    "        scales, alive_components = get_alive_components_over_scales(births_dimD, deaths_dimD, step=step)\n",
    "        \n",
    "        # Store these lists in the new columns\n",
    "        df.at[idx, f\"centroid_scales_dim{D}\"] = scales\n",
    "        df.at[idx, f\"centroid_alive_dim{D}\"] = alive_components\n",
    "        del rips_complex, simplex_tree, persistence, births_dimD, deaths_dimD, scales, alive_components  # Delete objects\n",
    "        gc.collect()  # Force garbage collection to free memory\n",
    "        \n",
    "        #time.sleep(0.2)  # Pause for 0.5 seconds\n",
    "    return df\n",
    "\n",
    "import hypernetx as hnx  # Hypergraph library\n",
    "import itertools\n",
    "def get_rips_complex_G(df, embedding=str('sentence_embeddings')):\n",
    "    df['graph']=None\n",
    "    df['density']=None\n",
    "    df['simplex_tree']=None\n",
    "    df['hypergraph'] = None\n",
    "    for idx, row in df.iterrows():\n",
    "        G=nx.Graph()\n",
    "        H = {}  # Hypergraph as a dictionary: {hyperedge_id: [vertices]}\n",
    "        embed = row[embedding] \n",
    "        edge_length=row['rt']\n",
    "        if not edge_length:\n",
    "              continue\n",
    "        rips_complex = gd.RipsComplex(points=embed, max_edge_length=edge_length)\n",
    "        simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "        hyperedges = set()\n",
    "        \n",
    "        for simplex, fvalue in simplex_tree.get_filtration():  # Get all simplices\n",
    "            if len(simplex) > 1:  # Ignore single nodes\n",
    "                hyperedges.add(frozenset(simplex))  # Store as a hyperedge\n",
    "        \n",
    "        # Convert hyperedges to a hypergraph representation\n",
    "        H = {f'he_{i}': list(hyperedge) for i, hyperedge in enumerate(hyperedges)}\n",
    "        hypergraph = hnx.Hypergraph(H)\n",
    "\n",
    "\n",
    "        # Store in DataFrame\n",
    "        df.at[idx, 'hypergraph'] = hypergraph\n",
    "        #df.at[idx, 'hyper_density'] = hypergraph.edge_size()\n",
    "        \n",
    "        \n",
    "        edges = []\n",
    "        \n",
    "        for simplex, fvalue in simplex_tree.get_skeleton(3):\n",
    "            if len(simplex) >= 2:\n",
    "                # Create every pair of vertices in this simplex\n",
    "                for (i, j) in itertools.combinations(simplex, 2):\n",
    "                    # You can choose which filtration value to assign as weight\n",
    "                    # Here, we use the simplex's overall fvalue.\n",
    "                    # Alternatively, you might want the actual distance between i,j if known.\n",
    "                    G.add_edge(i, j, weight=fvalue)\n",
    "                    edges.append((i, j, fvalue))\n",
    "        df['graph'].loc[idx]=G\n",
    "        df['density'].loc[idx]=nx.density(G)\n",
    "        df['simplex_tree'].loc[idx]=simplex_tree\n",
    "\n",
    "    return df#df\n",
    "\n",
    "def compute_euler_characteristic(simplex_tree, max_dim=3):\n",
    "    \"\"\"\n",
    "    Compute the Euler characteristic of a simplicial complex represented by a GUDHI simplex tree.\n",
    "    \n",
    "    Parameters:\n",
    "      simplex_tree: A GUDHI simplex tree containing simplices up to dimension max_dim.\n",
    "      max_dim: Maximum dimension to consider (e.g., 3 for tetrahedra).\n",
    "    \n",
    "    Returns:\n",
    "      euler: The Euler characteristic computed as \n",
    "             f0 - f1 + f2 - f3 + ... (up to max_dim).\n",
    "    \"\"\"\n",
    "    if not simplex_tree:\n",
    "        return None\n",
    "    # Dictionary to store counts for each dimension\n",
    "    simplex_counts = {}\n",
    "    for d in range(max_dim + 1):\n",
    "        # In GUDHI, a d-simplex is a simplex with d+1 vertices.\n",
    "        simplices_d = [simplex for simplex, filt in simplex_tree.get_skeleton(d) if len(simplex) == d + 1]\n",
    "        simplex_counts[d] = len(simplices_d)\n",
    "    #    print(f\"Number of {d}-simplices (f_{d}): {simplex_counts[d]}\")\n",
    "    \n",
    "    # Euler characteristic: sum_{d=0}^{max_dim} (-1)^d * f_d\n",
    "    euler = sum(((-1) ** d) * simplex_counts[d] for d in range(max_dim + 1))\n",
    "    return euler\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "def get_node_join_fvalues(G):\n",
    "    \"\"\"\n",
    "    Extracts the filtration value (fvalue) at which each node joins the next node\n",
    "    in the sorted order of nodes.\n",
    "\n",
    "    Parameters:\n",
    "    - G: networkx.Graph (a graph with weighted edges where weights are fvalues)\n",
    "\n",
    "    Returns:\n",
    "    - fvalue_list: List of tuples (node_i, node_j, fvalue) representing\n",
    "      the filtration value at which node_i joins node_j.\n",
    "    \"\"\"\n",
    "    if not G:\n",
    "        return None, None\n",
    "    sorted_nodes = sorted(G.nodes())  # Sort nodes in order\n",
    "    fvalue_list = []\n",
    "\n",
    "    for i in range(len(sorted_nodes) - 1):\n",
    "        n1, n2 = sorted_nodes[i], sorted_nodes[i + 1]  # Get consecutive nodes\n",
    "\n",
    "        if G.has_edge(n1, n2):  # Check if edge exists\n",
    "            fvalue = G[n1][n2]['weight']  # Get filtration value (edge weight)\n",
    "            fvalue_list.append(fvalue)\n",
    "\n",
    "    return sorted_nodes,fvalue_list\n",
    "\n",
    "def compute_correlations(sorted_nodes, fvalues):\n",
    "    \"\"\"\n",
    "    Computes Pearson and Spearman correlation coefficients between node indices and f-values.\n",
    "\n",
    "    Parameters:\n",
    "    - sorted_nodes: List of node indices (sorted order)\n",
    "    - fvalues: List of f-values corresponding to edges between consecutive nodes\n",
    "\n",
    "    Returns:\n",
    "    - pearson_corr: Pearson correlation coefficient\n",
    "    - spearman_corr: Spearman correlation coefficient\n",
    "    \"\"\"\n",
    "    if not fvalues:\n",
    "        return np.nan, np.nan\n",
    "    if len(fvalues) < 2:  # Correlations need at least two data points\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    indices = np.arange(len(fvalues))  # Create index positions (0,1,2,...) for correlation\n",
    "\n",
    "    pearson_corr, _ = pearsonr(indices, fvalues)\n",
    "    spearman_corr, _ = spearmanr(indices, fvalues)\n",
    "\n",
    "    return pearson_corr, spearman_corr\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import community  # for Louvain modularity detection (python-louvain)\n",
    "from networkx.algorithms import approximation\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "def ff3(x):\n",
    "    return x*(x-1)*(x-2)\n",
    "\n",
    "def avg_tetr_cc(g):\n",
    "    tetrahedra = itertools.islice(itertools.groupby(\n",
    "        nx.enumerate_all_cliques(g), len), 3, 4)\n",
    "    try:\n",
    "        tetrahedra = next(tetrahedra)[1]\n",
    "    except StopIteration:\n",
    "        return 0\n",
    "    cnts = collections.Counter(itertools.chain(*tetrahedra))\n",
    "    return 6 * sum(cnt / ff3(g.degree[v]) for v, cnt in cnts.items()) / len(g)\n",
    "\n",
    "\n",
    "\n",
    "from scipy.linalg import eigvalsh\n",
    "\n",
    "def compute_graph_metrics(G):\n",
    "    \"\"\"\n",
    "    Computes various network metrics for a given graph G, including Laplacian eigenvalues.\n",
    "    \n",
    "    Metrics:\n",
    "    - Shortest Path (Weighted & Unweighted)\n",
    "    - Number of Triangles\n",
    "    - Number of Tetrahedra (4-cliques)\n",
    "    - Modularity using Louvain (Weighted)\n",
    "    - Clustering Coefficient\n",
    "    - Max & Mean Degree\n",
    "    - Max & Mean Betweenness Centrality\n",
    "    - Max & Mean Strength (Weighted Degree)\n",
    "    - Second Smallest Laplacian Eigenvalue (Fiedler Value)\n",
    "    - Largest Laplacian Eigenvalue\n",
    "\n",
    "    Parameters:\n",
    "    - G (networkx.Graph): A 3-skeleton graph with weighted edges.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with computed graph metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"shortest_path_unweighted\": np.nan,\n",
    "        \"shortest_path_weighted\": np.nan,\n",
    "        \"num_triangles\": np.nan,\n",
    "        \"num_tetrahedra\": np.nan, #maybe also area of these\n",
    "        \"modularity_louvain\": np.nan,\n",
    "        \"clustering_coefficient\": np.nan,\n",
    "        \"max_degree\": np.nan,\n",
    "        \"mean_degree\": np.nan,\n",
    "        \"max_betweenness\": np.nan,\n",
    "        \"mean_betweenness\": np.nan,\n",
    "        \"max_strength\": np.nan,\n",
    "        \"mean_strength\": np.nan,\n",
    "        \"fiedler_value\": np.nan,\n",
    "        \"largest_laplacian_eigenvalue\": np.nan\n",
    "    }\n",
    "\n",
    "    if not G or G.number_of_nodes() < 2:\n",
    "        return metrics\n",
    "\n",
    "    # Sorted nodes\n",
    "    sorted_nodes = sorted(G.nodes())\n",
    "\n",
    "    # Shortest Path (Unweighted & Weighted)\n",
    "    first_node, last_node = sorted_nodes[0], sorted_nodes[-1]\n",
    "    if nx.has_path(G, first_node, last_node):\n",
    "        metrics[\"shortest_path_unweighted\"] = nx.shortest_path_length(G, source=first_node, target=last_node)\n",
    "        metrics[\"shortest_path_weighted\"] = nx.shortest_path_length(G, source=first_node, target=last_node, weight='weight')\n",
    "\n",
    "    # Number of triangles (3-cliques)\n",
    "    metrics[\"num_triangles\"] = sum(nx.triangles(G).values()) // 3  # Each triangle counted 3 times\n",
    "\n",
    "    # Number of tetrahedra (4-cliques)\n",
    "    metrics[\"num_tetrahedra\"] = avg_tetr_cc(G)  # Ensure avg_tetr_cc is defined\n",
    "\n",
    "    # Louvain Modularity (Weighted)\n",
    "    comms = nx.community.louvain_communities(G, weight='weight')\n",
    "    metrics[\"modularity_louvain\"] = nx.community.modularity(G, comms, weight='weight')\n",
    "\n",
    "    # Clustering Coefficient\n",
    "    metrics[\"clustering_coefficient\"] = nx.average_clustering(G, weight='weight')\n",
    "\n",
    "    # Degree (Max & Mean)\n",
    "    degrees = dict(G.degree())\n",
    "    metrics[\"max_degree\"] = max(degrees.values())\n",
    "    metrics[\"mean_degree\"] = np.mean(list(degrees.values()))\n",
    "\n",
    "    # Betweenness Centrality (Max & Mean)\n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    metrics[\"max_betweenness\"] = max(betweenness.values())\n",
    "    metrics[\"mean_betweenness\"] = np.mean(list(betweenness.values()))\n",
    "\n",
    "    # Strength (Weighted Degree) (Max & Mean)\n",
    "    strength = {node: sum(G[node][nbr].get('weight', 1) for nbr in G[node]) for node in G.nodes()}\n",
    "    metrics[\"max_strength\"] = max(strength.values())\n",
    "    metrics[\"mean_strength\"] = np.mean(list(strength.values()))\n",
    "\n",
    "    # Compute Laplacian Eigenvalues\n",
    "    L = nx.laplacian_matrix(G).toarray()  # Convert sparse matrix to dense NumPy array\n",
    "    eigenvalues = eigvalsh(L)  # Compute eigenvalues\n",
    "\n",
    "    if len(eigenvalues) > 1:  # Ensure there are at least two eigenvalues\n",
    "        metrics[\"fiedler_value\"] = eigenvalues[1]  # Second smallest eigenvalue (λ₂)\n",
    "        metrics[\"largest_laplacian_eigenvalue\"] = eigenvalues[-1]  # Largest eigenvalue (λ_max)\n",
    "\n",
    "    return metrics\n",
    "import hypernetx as hnx\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.linalg import eigvalsh\n",
    "\n",
    "def compute_hypergraph_metrics(H):\n",
    "    \"\"\"\n",
    "    Computes various hypergraph metrics for a given HyperNetX hypergraph H.\n",
    "\n",
    "    Metrics:\n",
    "    - Number of Nodes\n",
    "    - Number of Hyperedges\n",
    "    - Max & Mean Hyperedge Size\n",
    "    - Max & Mean Node Degree\n",
    "    - Max & Mean Betweenness Centrality\n",
    "    - Fiedler Value (Second Smallest Eigenvalue of Hypergraph Laplacian)\n",
    "    - Largest Laplacian Eigenvalue\n",
    "\n",
    "    Parameters:\n",
    "    - H (hnx.Hypergraph): A HyperNetX hypergraph.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with computed hypergraph metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"num_hyper_nodes\": np.nan,\n",
    "        \"num_hyperedges\": np.nan,\n",
    "        \"max_hyperedge_size\": np.nan,\n",
    "        \"mean_hyperedge_size\": np.nan,\n",
    "        \"hyper_max_node_degree\": np.nan,\n",
    "        \"hyper_mean_node_degree\": np.nan,\n",
    "        \"hyper_max_betweenness\": np.nan,\n",
    "        \"hyper_mean_betweenness\": np.nan,\n",
    "        \"hyper_fiedler_value\": np.nan,\n",
    "        \"hyper_largest_laplacian_eigenvalue\": np.nan\n",
    "    }\n",
    "\n",
    "    if not H or len(H.nodes) == 0:\n",
    "        return metrics\n",
    "\n",
    "    # Number of nodes and hyperedges\n",
    "    metrics[\"num_hyper_nodes\"] = len(H.nodes)\n",
    "    metrics[\"num_hyperedges\"] = len(H.edges)\n",
    "\n",
    "    # Hyperedge size statistics (how many nodes in each hyperedge)\n",
    "    hyperedge_sizes = [len(H.edges[he]) for he in H.edges]\n",
    "    if hyperedge_sizes:\n",
    "        metrics[\"max_hyperedge_size\"] = max(hyperedge_sizes)\n",
    "        metrics[\"mean_hyperedge_size\"] = np.mean(hyperedge_sizes)\n",
    "\n",
    "    # Node degree statistics (how many hyperedges each node belongs to)\n",
    "    node_degrees = {node: len(H.nodes[node]) for node in H.nodes}\n",
    "    if node_degrees:\n",
    "        metrics[\"hyper_max_node_degree\"] = max(node_degrees.values())\n",
    "        metrics[\"hyper_mean_node_degree\"] = np.mean(list(node_degrees.values()))\n",
    "\n",
    "    # Convert hypergraph to bipartite graph for centrality analysis\n",
    "    B = H.bipartite()  # Creates a bipartite graph (nodes + hyperedges)\n",
    "\n",
    "    # Betweenness centrality on the bipartite representation\n",
    "    if len(B.nodes) > 1:  # Ensure we have enough nodes\n",
    "        betweenness = nx.betweenness_centrality(B, weight='weight')\n",
    "        metrics[\"hyper_max_betweenness\"] = max(betweenness.values())\n",
    "        metrics[\"hyper_mean_betweenness\"] = np.mean(list(betweenness.values()))\n",
    "\n",
    "        # Compute Laplacian eigenvalues\n",
    "        L = nx.laplacian_matrix(B).toarray()  # Get Laplacian matrix\n",
    "        eigenvalues = eigvalsh(L)  # Compute eigenvalues\n",
    "\n",
    "        if len(eigenvalues) > 1:\n",
    "            metrics[\"hyper_fiedler_value\"] = eigenvalues[1]  # Second smallest eigenvalue (λ₂)\n",
    "            metrics[\"hyper_largest_laplacian_eigenvalue\"] = eigenvalues[-1]  # Largest eigenvalue (λ_max)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_distribution_stats(births, deaths, persistences):\n",
    "    \"\"\"\n",
    "    Given arrays/lists of births, deaths, and persistences, compute summary stats.\n",
    "    Returns a dict of named metrics.\n",
    "    \"\"\"\n",
    "    births = np.array(births, dtype=float)\n",
    "    deaths = np.array(deaths, dtype=float)\n",
    "    pers   = np.array(persistences, dtype=float)\n",
    "    \n",
    "    if len(pers) == 0:\n",
    "        return dict.fromkeys([\n",
    "            'birth_rate','death_rate','mean_persistence','max_persistence',\n",
    "            'std_persistence','skewness','kurtosis','entropy'\n",
    "        ], np.nan)\n",
    "    \n",
    "    birth_rate = births.mean()\n",
    "    death_rate = deaths.mean()\n",
    "    mean_persistence = pers.mean()\n",
    "    max_persistence  = pers.max()\n",
    "    std_persistence  = pers.std(ddof=1)\n",
    "    skewness = stats.skew(pers, bias=False)\n",
    "    kurt = stats.kurtosis(pers, bias=False)\n",
    "    number=len(pers)\n",
    "    # If you truly want entropy of the raw \"pers\" values (not a histogram):\n",
    "    # be aware that stats.entropy(pers) is not standard (it’s for discrete pmf).\n",
    "    # Typically you'd do a histogram-based approach, but for demonstration:\n",
    "    #   ent = stats.entropy(pers)\n",
    "    # Or, a histogram-based approach:\n",
    "    #   hist, _ = np.histogram(pers, bins='auto', density=True)\n",
    "    #   ent = stats.entropy(hist) if np.any(hist > 0) else 0.0\n",
    "    \n",
    "    ent = stats.entropy(pers)  # Just following your snippet, though it's unusual\n",
    "    \n",
    "    return {\n",
    "        'birth_rate': birth_rate,\n",
    "        'death_rate': death_rate,\n",
    "        'mean_persistence': mean_persistence,\n",
    "        'max_persistence': max_persistence,\n",
    "        'std_persistence': std_persistence,\n",
    "        'skewness': skewness,\n",
    "        'kurtosis': kurt,\n",
    "        'entropy': ent,\n",
    "        'number':number\n",
    "    }\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed for 3D plotting\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def visualize_rips_simplicial_complex(embed, dataset_name, entry, max_edge_length=3.0):\n",
    "    \"\"\"\n",
    "    1) Builds a Rips complex (via GUDHI) from a set of high-dimensional points.\n",
    "    2) Extracts simplices (up to dimension 2) from the simplex tree.\n",
    "       - Edges (1-simplices) and triangles (2-simplices).\n",
    "    3) Uses PCA to reduce the points to 3D.\n",
    "    4) Plots a 3D visualization:\n",
    "       - Nodes are shown as a scatter plot.\n",
    "       - Edges are drawn as lines.\n",
    "       - Triangles are drawn as filled polygons (using Poly3DCollection).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed : np.ndarray of shape (N, D)\n",
    "        The high-dimensional point cloud.\n",
    "    max_edge_length : float\n",
    "        The maximum edge length used in the Rips complex.\n",
    "    \"\"\"\n",
    "    # 1) Build the Rips complex and create the simplex tree\n",
    "    rips_complex = gd.RipsComplex(points=embed, max_edge_length=max_edge_length)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "    \n",
    "    # 2) Extract simplices:\n",
    "    edges = []\n",
    "    triangles = []\n",
    "    \n",
    "    # get_skeleton(2) returns all simplices up to dimension 2\n",
    "    for simplex, fvalue in simplex_tree.get_skeleton(2):\n",
    "        if len(simplex) == 2:\n",
    "            # 1-simplices: edges\n",
    "            edges.append(simplex)\n",
    "        elif len(simplex) == 3:\n",
    "            # 2-simplices: triangles\n",
    "            triangles.append(simplex)\n",
    "    \n",
    "    # 3) Use PCA to reduce the point cloud to 3D\n",
    "    pca = PCA(n_components=3)\n",
    "    coords_3d = pca.fit_transform(embed)  # shape (N, 3)\n",
    "    n_points = coords_3d.shape[0]\n",
    "    \n",
    "    # Prepare colormap for nodes (using 'magma_r')\n",
    "    norm = plt.Normalize(vmin=0, vmax=n_points - 1)\n",
    "    cmap = plt.get_cmap('plasma_r')\n",
    "    node_colors = cmap(norm(np.arange(n_points)))\n",
    "    \n",
    "    # 4) Create the 3D plot\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot nodes\n",
    "    sc = ax.scatter(coords_3d[:, 0], coords_3d[:, 1], coords_3d[:, 2],\n",
    "                    c=node_colors, s=30, alpha=0.9)\n",
    "    \n",
    "    # Plot edges as lines\n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        x_vals = [coords_3d[i, 0], coords_3d[j, 0]]\n",
    "        y_vals = [coords_3d[i, 1], coords_3d[j, 1]]\n",
    "        z_vals = [coords_3d[i, 2], coords_3d[j, 2]]\n",
    "        # Optionally, color edge based on one endpoint's index or the average.\n",
    "        avg_idx = int(np.mean(edge))\n",
    "        edge_color = cmap(norm(avg_idx))\n",
    "        ax.plot(x_vals, y_vals, z_vals, color=edge_color, alpha=0.8, linewidth=1.5)\n",
    "    \n",
    "    # Plot triangles as filled faces\n",
    "    face_polys = []\n",
    "    face_colors = []\n",
    "    for tri in triangles:\n",
    "        # Get the 3 vertices for this triangle\n",
    "        pts = [coords_3d[idx] for idx in tri]\n",
    "        face_polys.append(pts)\n",
    "        # Color can be computed from the average index of the triangle's vertices\n",
    "        avg_idx = int(np.mean(tri))\n",
    "        face_colors.append(cmap(norm(avg_idx)))\n",
    "    \n",
    "    # Create a Poly3DCollection for the triangles with a set transparency (alpha)\n",
    "    poly_collection = Poly3DCollection(face_polys, alpha=0.3, edgecolor='k')\n",
    "    poly_collection.set_facecolor(face_colors)\n",
    "    ax.add_collection3d(poly_collection)\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(f\"\", pad=20)\n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")\n",
    "    ax.set_zlabel(\"PCA 3\")\n",
    "    \n",
    "    # Add colorbar for node indices\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, pad=0.1)\n",
    "    cbar.set_label(\"Node Index\")\n",
    "        # Define three different viewing angles\n",
    "    angles = [(15, 180), (30, 90), (45, 0)]  # (elevation, azimuth) in degrees\n",
    "    dir_fig_save=working_dir+f'rips_skeletons/{dataset_name}_{entry}/'\n",
    "    os.makedirs(dir_fig_save, exist_ok=True)\n",
    "\n",
    "    # Save figures from different angles\n",
    "    for i, (elev, azim) in enumerate(angles):\n",
    "        ax.view_init(elev=elev, azim=azim)  # Set camera angle\n",
    "        filename = dir_fig_save+f\"{window}_{embedding_step}_{i}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')  # Save figure\n",
    "       # print(f\"Saved: {filename}\")\n",
    "\n",
    "    #plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9913cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window=128\n",
    "step=int(window*0.125)\n",
    "topics=500\n",
    "nonzeroes=5\n",
    "embedding_step=int(window*0.5)\n",
    "#infile = open(f'/home/ll16598/Documents/POSTDOC/Context-DATM/sentenceBERT_context_models_{window}_{embedding_step}/{topics}comp{nonzeroes}nonzeros_dictionary_','rb')\n",
    "#dictionary=pickle.load(infile)\n",
    "#infile.close()\n",
    "\n",
    "save_thresh=244\n",
    "threshold=save_thresh\n",
    "# infile = open(f'/home/ll16598/Documents/POSTDOC/Context-DATM/sentenceBERT_cluster_dicts_{window}_{embedding_step}/cluster_dictionary_{save_thresh}','rb')\n",
    "# cluster_dictionary=pickle.load(infile)\n",
    "# infile.close()\n",
    "\n",
    "dir_array='/home/ll16598/Documents/POSTDOC/vector_assigned_dfs'\n",
    "dir_atom_dfs='/home/ll16598/Documents/POSTDOC/Context-DATM/atom_assigned_dfs'\n",
    "\n",
    "#df_drug=pd.read_csv(f'.{}/df_monolog_{threshold}.csv')\n",
    "\n",
    "df_monologs=pd.read_csv(f'{dir_atom_dfs}/df_monolog_{threshold}.csv')\n",
    "\n",
    "layers='last'\n",
    "with open(f'{dir_array}/{window}_{step}_SER_monologs_sentence_embeddings_arrays.pkl', 'rb') as f:\n",
    "    df_monologs['sentence_embeddings'] = pickle.load(f)\n",
    "    \n",
    "df_SER=pd.read_csv(f'{dir_atom_dfs}/df_SER_{threshold}.csv')\n",
    "with open(f'{dir_array}/{window}_{step}_SER1_sentence_embeddings_arrays.pkl', 'rb') as f:\n",
    "    df_SER['sentence_embeddings'] = pickle.load(f)\n",
    "df_SER2=pd.read_csv(f'{dir_atom_dfs}/df_SER2_{threshold}.csv')\n",
    "with open(f'{dir_array}/{window}_{step}_SER_IPSP_sentence_embeddings_arrays.pkl', 'rb') as f:\n",
    "    SER2_array= pickle.load(f)\n",
    "    df_SER2['sentence_embeddings'] =SER2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[df_monologs, df_SER, df_SER2]\n",
    "df_names=['monolog', 'SER', 'SER2']\n",
    "for df_no, df_monolog in enumerate(dfs):\n",
    "    df_name=df_names[df_no]\n",
    "    df_monolog['token_embeddings']=None\n",
    "    embeddings='sentence_embeddings'\n",
    "    drugs=sorted(list(set(df_monolog['Drug'])))\n",
    "    Participants=sorted(list(set(df_monolog['Participant'])))\n",
    "    working_dir='/home/ll16598/Documents/POSTDOC/Context-DATM/'\n",
    "    data_save_dir=working_dir+'TDA_output/'\n",
    "    os.makedirs(data_save_dir, exist_ok=True)\n",
    "\n",
    "    df_monolog=get_rips_time_centroid(df_monolog,embeddings=embeddings)\n",
    "    for D in [0,1]:\n",
    "        df_monolog=get_rips_time(df_monolog,embeddings=embeddings, D=D)\n",
    "\n",
    "        df_exploded = df_monolog.explode([f\"scales_dim{D}\", f'alive_dim{D}'])\n",
    "        df_exploded[f\"scales_dim{D}\"] = pd.to_numeric(df_exploded[f\"scales_dim{D}\"])\n",
    "        df_exploded[f'alive_dim{D}'] = pd.to_numeric(df_exploded[f'alive_dim{D}'])\n",
    "        grouped = df_exploded.groupby([\"Drug\", f\"scales_dim{D}\"], as_index=False).agg(\n",
    "            alive_mean=(f'alive_dim{D}', \"mean\"),\n",
    "            alive_se=(f'alive_dim{D}', sem)  # standard error\n",
    "        )\n",
    "        df_exploded.to_csv(data_save_dir+f'{df_name}_{D}_simplices_over_time.csv')\n",
    "\n",
    "\n",
    "    df_centroid_exploded = df_monolog.explode([\"centroid_scales_dim0\", 'centroid_alive_dim0'])\n",
    "    df_centroid_exploded[f\"centroid_scales_dim0\"] = pd.to_numeric(df_centroid_exploded[f\"centroid_scales_dim0\"])\n",
    "    df_centroid_exploded[f'centroid_alive_dim0'] = pd.to_numeric(df_centroid_exploded[f'centroid_alive_dim0'])\n",
    "    grouped_centroid = df_centroid_exploded.groupby([\"Drug\", f\"centroid_scales_dim0\"], as_index=False).agg(\n",
    "        alive_mean=(f'centroid_alive_dim0', \"mean\"),\n",
    "        alive_se=(f'centroid_scales_dim0', sem)  # standard error\n",
    "    )\n",
    "    df_centroid_exploded.to_csv(data_save_dir+f'{df_name}_centroid_simplices_over_time.csv')\n",
    "\n",
    "\n",
    "    df_monolog=get_rips_time(df_monolog,embeddings=embeddings, D=0)\n",
    "\n",
    "    df_with_graph=get_rips_complex_G(df_monolog)\n",
    "    # Apply functions to each graph in the dataframe\n",
    "    df_with_graph['sorted_nodes'], df_with_graph['fvalues_between_nodes'] = zip(\n",
    "        *df_with_graph['graph'].apply(get_node_join_fvalues)\n",
    "    )\n",
    "\n",
    "    # Compute Pearson & Spearman correlations and store them in new columns\n",
    "    df_with_graph[['pearson_corr', 'spearman_corr']] = df_with_graph.apply(\n",
    "        lambda row: compute_correlations(row['sorted_nodes'], row['fvalues_between_nodes']), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "    df_with_graph['euler'] = df_with_graph['simplex_tree'].apply(lambda st: compute_euler_characteristic(st, max_dim=3))\n",
    "\n",
    "\n",
    "    # Apply the function to each graph in df_with_graph\n",
    "    graph_metrics = df_with_graph['graph'].apply(compute_graph_metrics)\n",
    "\n",
    "    # Convert dictionary results into separate columns\n",
    "    graph_metrics_df = pd.DataFrame(graph_metrics.tolist())\n",
    "\n",
    "    # Merge with original dataframe\n",
    "    df_with_graph = pd.concat([df_with_graph, graph_metrics_df], axis=1)\n",
    "\n",
    "    hyper_graph_metrics = df_with_graph['hypergraph'].apply(compute_hypergraph_metrics)\n",
    "\n",
    "    # Convert dictionary results into separate columns\n",
    "    hyper_graph_metrics_df = pd.DataFrame(hyper_graph_metrics.tolist())\n",
    "\n",
    "    # Merge with original dataframe\n",
    "    df_with_graph = pd.concat([df_with_graph, hyper_graph_metrics_df], axis=1)\n",
    "\n",
    "\n",
    "    # List out the homology dimensions you want\n",
    "    dimensions = [0, 1, 2]\n",
    "\n",
    "    # We'll accumulate new rows in a list of dicts\n",
    "    new_rows = []\n",
    "\n",
    "    for idx, row in df_with_graph.iterrows():\n",
    "        # You might have an \"embedding\" column with the data. \n",
    "        # Or, if you have multiple columns, you might do something like:\n",
    "        # embed = row[embeddings].values (depending on your structure).\n",
    "        embed = row[embeddings]  # Adjust as needed\n",
    "\n",
    "        # Build the Rips Complex for *this row only*\n",
    "        rips_complex = gd.RipsComplex(points=embed, max_edge_length=5.0)\n",
    "        simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "        persistence = simplex_tree.persistence()\n",
    "\n",
    "        # We’ll store births, deaths, pers LENGTHS in a dict keyed by dimension\n",
    "        dim_dict = {\n",
    "            dim: {'births': [], 'deaths': [], 'pers': []}\n",
    "            for dim in dimensions\n",
    "        }\n",
    "\n",
    "        # Collect intervals by dimension\n",
    "        for dim, (b, d) in persistence:\n",
    "            if d == float('inf'):\n",
    "                continue\n",
    "            if dim in dimensions:\n",
    "                dim_dict[dim]['births'].append(b)\n",
    "                dim_dict[dim]['deaths'].append(d)\n",
    "                dim_dict[dim]['pers'].append(d - b)\n",
    "\n",
    "        # Now compute summary stats for each dimension.\n",
    "        # We can store them in one of two ways:\n",
    "        # Option A: Put *all dimensions* stats in separate columns of a single row\n",
    "        # Option B: Create separate rows for each dimension (then you keep dimension as a column)\n",
    "        #\n",
    "        # Let's do Option A: store dimension 0, 1, 2 in separate columns (like birth_rate_dim0, etc.).\n",
    "\n",
    "        row_dict = row.to_dict()  # Start with original row's columns\n",
    "\n",
    "        for dim in dimensions:\n",
    "            bdp = dim_dict[dim]\n",
    "            stats_dict = compute_distribution_stats(bdp['births'], bdp['deaths'], bdp['pers'])\n",
    "            # prefix each stat key with dim\n",
    "            for stat_key, stat_val in stats_dict.items():\n",
    "                row_dict[f\"{stat_key}_dim{dim}\"] = stat_val\n",
    "\n",
    "        # Add row_dict to new_rows\n",
    "        new_rows.append(row_dict)\n",
    "\n",
    "    # Create a new DataFrame\n",
    "    df_with_tda = pd.DataFrame(new_rows)\n",
    "    df_with_tda.to_csv(data_save_dir, f'{df_name}_TDA_results')\n",
    "    print(f'completed! {df_name}')\n",
    "    for entry in range(0, len(df_with_tda['sentence_embeddings'])):\n",
    "        if not df_with_tda['graph'][entry]:\n",
    "            continue\n",
    "        else:\n",
    "            visualize_rips_simplicial_complex(df_with_tda['sentence_embeddings'][entry], df_name, entry, max_edge_length=df_with_tda['rt'][entry])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
