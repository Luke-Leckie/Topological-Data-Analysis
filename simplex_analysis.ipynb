{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef3d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import gc\n",
    "import pickle\n",
    "from scipy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from random import seed, sample\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import sem\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import hypernetx as hnx  # Hypergraph library\n",
    "import itertools\n",
    "from scipy.linalg import eigvalsh\n",
    "\n",
    "#sys.path.append('/home/ll16598/Documents/Altered_States_Reddit/model_pipeline/__pycache__')\n",
    "#from quality import reconst_qual, topic_diversity, coherence_centroid, coherence_pairwise #written for this jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8aae3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi as gd\n",
    "import gudhi.representations\n",
    "def compute_persistence_diagram(data):\n",
    "    # Compute the Rips complex\n",
    "    rips_complex = gd.RipsComplex(points=data, max_edge_length=ML)\n",
    "    # Construct a simplex tree\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "    # Compute persistent homology\n",
    "    persistence = simplex_tree.persistence()\n",
    "    # Plot persistence diagram`\n",
    "    gd.plot_persistence_diagram(persistence)\n",
    "    gd.plot_persistence_barcode(persistence)\n",
    "\n",
    "    plt.show()\n",
    "    return persistence\n",
    "\n",
    "def add_geometric_centroid(data):\n",
    "    \"\"\"\n",
    "    Given an (N, D) array 'data' of N points in D dimensions,\n",
    "    computes the mean (geometric centroid) and appends it\n",
    "    as an extra row at the end.\n",
    "\n",
    "    Returns:\n",
    "      data_with_centroid: an (N+1, D) array,\n",
    "        where the last row is the centroid.\n",
    "      centroid_index: the integer index of the new centroid row.\n",
    "    \"\"\"\n",
    "    centroid = np.mean(data, axis=0)             # shape (D,)\n",
    "    data_with_centroid = np.vstack([data, centroid])\n",
    "    centroid_index = data_with_centroid.shape[0] - 1\n",
    "    return data_with_centroid, centroid_index\n",
    "\n",
    "def build_centroid_distance_matrix(data_with_centroid, centroid_index, large_val=1e6):\n",
    "    \"\"\"\n",
    "    Creates a distance matrix where only edges from the 'centroid_index'\n",
    "    to other points have the real Euclidean distance.\n",
    "    All other pairwise distances are set to 'large_val'.\n",
    "    \"\"\"\n",
    "    N = data_with_centroid.shape[0]\n",
    "    dist_matrix = np.full((N, N), large_val, dtype=float)\n",
    "\n",
    "    # Diagonal = 0\n",
    "    np.fill_diagonal(dist_matrix, 0.0)\n",
    "\n",
    "    # Compute distances between centroid and each other point\n",
    "    for i in range(N):\n",
    "        if i == centroid_index:\n",
    "            continue\n",
    "        # Real distance from centroid -> i\n",
    "        dist = cosine_similarity(data_with_centroid[centroid_index].reshape(1,-1),\\\n",
    "                                 data_with_centroid[i].reshape(1,-1))\n",
    "        dist_matrix[centroid_index, i] = dist\n",
    "        dist_matrix[i, centroid_index] = dist\n",
    "\n",
    "    return dist_matrix\n",
    "\n",
    "def compute_persistence_centroid(data, max_edge_length=3.0, plotting=True):\n",
    "    \"\"\"\n",
    "    1) Compute the geometric centroid of 'data' and append it as an extra row.\n",
    "    2) Build a distance matrix such that only the centroid can connect to other points.\n",
    "    3) Construct the Rips complex using the custom distance matrix.\n",
    "    4) Compute and plot the persistence diagram and barcode.\n",
    "\n",
    "    Returns:\n",
    "      persistence: The list of (dim, (birth, death)) intervals from GUDHI\n",
    "    \"\"\"\n",
    "    # 1) Add the centroid\n",
    "    data_with_centroid, centroid_index = add_geometric_centroid(data)\n",
    "\n",
    "    # 2) Build the custom distance matrix\n",
    "    dist_matrix = build_centroid_distance_matrix(data_with_centroid, centroid_index, large_val=1e6)\n",
    "\n",
    "    # 3) Create RipsComplex from the distance matrix\n",
    "    rips_complex = gd.RipsComplex(distance_matrix=dist_matrix, max_edge_length=max_edge_length)\n",
    "    \n",
    "    if SPARSE:\n",
    "        rips_complex = gd.RipsComplex(distance_matrix=dist_matrix, max_edge_length=max_edge_length, sparse=sparse_param)\n",
    "    else:\n",
    "        rips_complex = gd.RipsComplex(distance_matrix=dist_matrix, max_edge_length=max_edge_length)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "    del dist_matrix  # Delete objects\n",
    "    gc.collect()  # Force garbage collection to free memory\n",
    "    # 4) Compute persistent homology\n",
    "    persistence = simplex_tree.persistence()\n",
    "    if not plotting:\n",
    "        return rips_complex, simplex_tree, persistence\n",
    "    # 5) Plot the persistence diagram & barcode\n",
    "    gd.plot_persistence_diagram(persistence)\n",
    "    gd.plot_persistence_barcode(persistence)\n",
    "    plt.show()\n",
    "\n",
    "    return rips_complex, simplex_tree, persistence\n",
    "\n",
    "\n",
    "def get_alive_components_over_scales(births, deaths, step=0.025):\n",
    "    \"\"\"\n",
    "    Given lists of (birth, death) intervals for a particular homology dimension,\n",
    "    compute how many such features (e.g., connected components if D=0, loops if D=1, etc.)\n",
    "    are 'alive' at increments of 'step' from 0 up to max(deaths).\n",
    "\n",
    "    Returns:\n",
    "    - scales: list of scale values (0, 0.025, 0.05, ...)\n",
    "    - alive_counts: corresponding list of how many features are alive at each scale\n",
    "    \"\"\"\n",
    "    if len(births) == 0:\n",
    "        # No intervals => no features\n",
    "        return [], []\n",
    "    \n",
    "    max_death = max(deaths)\n",
    "    scales = np.arange(0, max_death + 1e-9, step)\n",
    "    \n",
    "    alive_counts = []\n",
    "    for s in scales:\n",
    "        # Count intervals that are alive: birth <= s < death\n",
    "        count_alive = sum(1 for (b, d) in zip(births, deaths) if b <= s < d)\n",
    "        alive_counts.append(count_alive)\n",
    "    \n",
    "    return list(scales), alive_counts\n",
    "\n",
    "def get_rips_time_centroid(df, embeddings='sentence_embeddings_pca', step=0.025, D=0):\n",
    "    \"\"\"\n",
    "    For each row in df, build a Rips complex, extract dimension-D intervals\n",
    "    (e.g., D=0 => connected components, D=1 => loops, etc.),\n",
    "    then compute how many such features are 'alive' at increments of 'step'.\n",
    "\n",
    "    Creates two new columns in df:\n",
    "    - f\"scales_dim{D}\": The scale values\n",
    "    - f\"alive_dim{D}\": The counts of alive features at each scale\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid SettingWithCopy warnings\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Prepare two new columns (lists)\n",
    "    df[f'centroid_scales_dim{D}'] = None\n",
    "    df[f'centroid_alive_dim{D}'] = None\n",
    "    df[f'rt_centroid'] = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "#         if idx in [26, 27, 28]:\n",
    "#             print('warning: skipping a computationally intensive sample')\n",
    "#             continue\n",
    "        # Get the embeddings for this row\n",
    "        embed = row[embeddings]\n",
    "        if not isinstance(embed, (list, np.ndarray)) or len(embed) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Build the Rips Complex\n",
    "        rips_complex, simplex_tree,persistence= compute_persistence_centroid(embed,max_edge_length=ML,plotting=False)\n",
    "        births_dimD = []\n",
    "        deaths_dimD = []\n",
    "        \n",
    "        for dim, (b, d) in persistence:\n",
    "            if dim == D and d != float('inf'):  # ignoring infinite intervals\n",
    "                births_dimD.append(b)\n",
    "                deaths_dimD.append(d)\n",
    "\n",
    "        # Compute how many features are alive at each scale\n",
    "        scales, alive_components = get_alive_components_over_scales(births_dimD, deaths_dimD, step=step)\n",
    "        df.at[idx, f\"rt_centroid\"] = max(deaths_dimD)\n",
    "        # Store these lists in the new columns\n",
    "        df.at[idx, f\"centroid_scales_dim{D}\"] = scales\n",
    "        df.at[idx, f\"centroid_alive_dim{D}\"] = alive_components\n",
    "        del rips_complex, simplex_tree, persistence, births_dimD, deaths_dimD, scales, alive_components  # Delete objects\n",
    "        gc.collect()  # Force garbage collection to free memory\n",
    "        \n",
    "        #time.sleep(0.2)  # Pause for 0.5 seconds\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import community  # for Louvain modularity detection (python-louvain)\n",
    "from networkx.algorithms import approximation\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "def ff3(x):\n",
    "    return x*(x-1)*(x-2)\n",
    "\n",
    "def avg_tetr_cc(g):\n",
    "    tetrahedra = itertools.islice(itertools.groupby(\n",
    "        nx.enumerate_all_cliques(g), len), 3, 4)\n",
    "    try:\n",
    "        tetrahedra = next(tetrahedra)[1]\n",
    "    except StopIteration:\n",
    "        return 0\n",
    "    cnts = collections.Counter(itertools.chain(*tetrahedra))\n",
    "    return 6 * sum(cnt / ff3(g.degree[v]) for v, cnt in cnts.items()) / len(g)\n",
    "\n",
    "\n",
    "\n",
    "def compute_graph_metrics(G):\n",
    "    \"\"\"\n",
    "    Computes various network metrics for a given graph G, including Laplacian eigenvalues.\n",
    "    \n",
    "    Metrics:\n",
    "    - Shortest Path (Weighted & Unweighted)\n",
    "    - Number of Triangles\n",
    "    - Number of Tetrahedra (4-cliques)\n",
    "    - Modularity using Louvain (Weighted)\n",
    "    - Clustering Coefficient\n",
    "    - Max & Mean Degree\n",
    "    - Max & Mean Betweenness Centrality\n",
    "    - Max & Mean Strength (Weighted Degree)\n",
    "    - Second Smallest Laplacian Eigenvalue (Fiedler Value)\n",
    "    - Largest Laplacian Eigenvalue\n",
    "\n",
    "    Parameters:\n",
    "    - G (networkx.Graph): A 3-skeleton graph with weighted edges.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with computed graph metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"shortest_path_unweighted\": np.nan,\n",
    "        \"nodes\":np.nan,\n",
    "        \"shortest_path_weighted\": np.nan,\n",
    "        \"num_triangles\": np.nan,\n",
    "        \"num_tetrahedra\": np.nan, #maybe also area of these\n",
    "        \"modularity_louvain\": np.nan,\n",
    "        \"clustering_coefficient\": np.nan,\n",
    "        \"max_degree\": np.nan,\n",
    "        \"mean_degree\": np.nan,\n",
    "        \"max_betweenness\": np.nan,\n",
    "        \"mean_betweenness\": np.nan,\n",
    "        \"max_strength\": np.nan,\n",
    "        \"mean_strength\": np.nan,\n",
    "        \"fiedler_value\": np.nan,\n",
    "        \"largest_laplacian_eigenvalue\": np.nan\n",
    "    }\n",
    "\n",
    "    if not G or G.number_of_nodes() < 2:\n",
    "        return metrics\n",
    "\n",
    "    # Sorted nodes\n",
    "    sorted_nodes = sorted(G.nodes())\n",
    "\n",
    "    # Shortest Path (Unweighted & Weighted)\n",
    "    first_node, last_node = sorted_nodes[0], sorted_nodes[-1]\n",
    "    if nx.has_path(G, first_node, last_node):\n",
    "        metrics[\"shortest_path_unweighted\"] = nx.shortest_path_length(G, source=first_node, target=last_node)\n",
    "        metrics[\"shortest_path_weighted\"] = nx.shortest_path_length(G, source=first_node, target=last_node, weight='weight')\n",
    "\n",
    "    # Number of triangles (3-cliques)\n",
    "    metrics[\"num_triangles\"] = sum(nx.triangles(G).values()) // 3  # Each triangle counted 3 times\n",
    "\n",
    "    # Number of tetrahedra (4-cliques)\n",
    "    metrics[\"num_tetrahedra\"] = avg_tetr_cc(G)  # Ensure avg_tetr_cc is defined\n",
    "\n",
    "\n",
    "    # Clustering Coefficient\n",
    "    metrics[\"clustering_coefficient\"] = nx.average_clustering(G, weight='weight')\n",
    "\n",
    "    # Degree (Max & Mean)\n",
    "    degrees = dict(G.degree())\n",
    "    metrics[\"max_degree\"] = max(degrees.values())\n",
    "    metrics[\"mean_degree\"] = np.mean(list(degrees.values()))\n",
    "\n",
    "    # Betweenness Centrality (Max & Mean)\n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    metrics[\"max_betweenness\"] = max(betweenness.values())\n",
    "    metrics[\"mean_betweenness\"] = np.mean(list(betweenness.values()))\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        original_weight = data.get('weight', 1)  # default to 1 if no weight provided\n",
    "        # Avoid division by zero:\n",
    "        if original_weight != 0:\n",
    "            data['inv_weight'] = 1 / original_weight\n",
    "        else:\n",
    "            data['inv_weight'] = 0  # or some default value that makes sense for your case\n",
    "\n",
    "    # Louvain Modularity (Weighted)\n",
    "    comms = nx.community.louvain_communities(G, weight='inv_weight')\n",
    "    metrics[\"modularity_louvain\"] = nx.community.modularity(G, comms, weight='inv_weight')\n",
    "\n",
    "    # Strength (Weighted Degree) (Max & Mean)\n",
    "    strength = {node: sum(G[node][nbr].get('inv_weight', 1) for nbr in G[node]) for node in G.nodes()}\n",
    "    metrics[\"max_strength\"] = max(strength.values())\n",
    "    metrics[\"mean_strength\"] = np.mean(list(strength.values()))\n",
    "    metrics['nodes']=len(G.nodes())\n",
    "    # Compute Laplacian Eigenvalues\n",
    "    L = nx.laplacian_matrix(G, weight='inv_weight').toarray()  # Convert sparse matrix to dense NumPy array\n",
    "    eigenvalues = eigvalsh(L)  # Compute eigenvalues\n",
    "\n",
    "    if len(eigenvalues) > 1:  # Ensure there are at least two eigenvalues\n",
    "        metrics[\"fiedler_value\"] = eigenvalues[1]  # Second smallest eigenvalue (λ₂)\n",
    "        metrics[\"largest_laplacian_eigenvalue\"] = eigenvalues[-1]  # Largest eigenvalue (λ_max)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_distribution_stats(births, deaths, persistences):\n",
    "    \"\"\"\n",
    "    Given arrays/lists of births, deaths, and persistences, compute summary stats.\n",
    "    Returns a dict of named metrics.\n",
    "    \"\"\"\n",
    "    births = np.array(births, dtype=float)\n",
    "    deaths = np.array(deaths, dtype=float)\n",
    "    pers   = np.array(persistences, dtype=float)\n",
    "    \n",
    "    if len(pers) == 0:\n",
    "        return dict.fromkeys([\n",
    "            'birth_rate','death_rate','mean_persistence','max_persistence',\n",
    "            'std_persistence','skewness','kurtosis','entropy'\n",
    "        ], np.nan)\n",
    "    \n",
    "    birth_rate = births.mean()\n",
    "    death_rate = deaths.mean()\n",
    "    mean_persistence = pers.mean()\n",
    "    max_persistence  = pers.max()\n",
    "    std_persistence  = pers.std(ddof=1)\n",
    "    skewness = stats.skew(pers, bias=False)\n",
    "    kurt = stats.kurtosis(pers, bias=False)\n",
    "    number=len(pers)\n",
    "    # If you truly want entropy of the raw \"pers\" values (not a histogram):\n",
    "    # be aware that stats.entropy(pers) is not standard (it’s for discrete pmf).\n",
    "    # Typically you'd do a histogram-based approach, but for demonstration:\n",
    "    #   ent = stats.entropy(pers)\n",
    "    # Or, a histogram-based approach:\n",
    "    #   hist, _ = np.histogram(pers, bins='auto', density=True)\n",
    "    #   ent = stats.entropy(hist) if np.any(hist > 0) else 0.0\n",
    "    \n",
    "    ent = stats.entropy(pers)  # Just following your snippet, though it's unusual\n",
    "    \n",
    "    return {\n",
    "        'birth_rate': birth_rate,\n",
    "        'death_rate': death_rate,\n",
    "        'mean_persistence': mean_persistence,\n",
    "        'max_persistence': max_persistence,\n",
    "        'std_persistence': std_persistence,\n",
    "        'skewness': skewness,\n",
    "        'kurtosis': kurt,\n",
    "        'entropy': ent,\n",
    "        'number':number\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def visualize_rips_simplicial_complex(embed, dataset_name, entry, max_edge_length=3):\n",
    "    \"\"\"\n",
    "    1) Builds a Rips complex (via GUDHI) from a set of high-dimensional points.\n",
    "    2) Extracts simplices (up to dimension 2) from the simplex tree.\n",
    "       - Edges (1-simplices) and triangles (2-simplices).\n",
    "    3) Uses PCA to reduce the points to 3D.\n",
    "    4) Plots a 3D visualization:\n",
    "       - Nodes are shown as a scatter plot.\n",
    "       - Edges are drawn as lines.\n",
    "       - Triangles are drawn as filled polygons (using Poly3DCollection).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed : np.ndarray of shape (N, D)\n",
    "        The high-dimensional point cloud.\n",
    "    max_edge_length : float\n",
    "        The maximum edge length used in the Rips complex.\n",
    "    \"\"\"\n",
    "    # 1) Build the Rips complex and create the simplex tree\n",
    "    rips_complex = gd.RipsComplex(points=embed, max_edge_length=max_edge_length)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "    \n",
    "    # 2) Extract simplices:\n",
    "    edges = []\n",
    "    triangles = []\n",
    "    \n",
    "    # get_skeleton(2) returns all simplices up to dimension 2\n",
    "    for simplex, fvalue in simplex_tree.get_skeleton(4):\n",
    "        if len(simplex) == 2:\n",
    "            # 1-simplices: edges\n",
    "            edges.append(simplex)\n",
    "        elif len(simplex) == 3:\n",
    "            # 2-simplices: triangles\n",
    "            triangles.append(simplex)\n",
    "    # 3) Use PCA to reduce the point cloud to 3D\n",
    "    pca = PCA(n_components=3)\n",
    "    coords_3d = pca.fit_transform(embed)  # shape (N, 3)\n",
    "    n_points = coords_3d.shape[0]\n",
    "    \n",
    "    # Prepare colormap for nodes (using 'magma_r')\n",
    "    norm = plt.Normalize(vmin=0, vmax=n_points - 1)\n",
    "    cmap = plt.get_cmap('plasma_r')\n",
    "    node_colors = cmap(norm(np.arange(n_points)))\n",
    "    \n",
    "    # 4) Create the 3D plot\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot nodes\n",
    "    sc = ax.scatter(coords_3d[:, 0], coords_3d[:, 1], coords_3d[:, 2],\n",
    "                    c=node_colors, s=30, alpha=0.9)\n",
    "    \n",
    "    # Plot edges as lines\n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        x_vals = [coords_3d[i, 0], coords_3d[j, 0]]\n",
    "        y_vals = [coords_3d[i, 1], coords_3d[j, 1]]\n",
    "        z_vals = [coords_3d[i, 2], coords_3d[j, 2]]\n",
    "        # Optionally, color edge based on one endpoint's index or the average.\n",
    "        avg_idx = int(np.mean(edge))\n",
    "        edge_color = cmap(norm(avg_idx))\n",
    "        ax.plot(x_vals, y_vals, z_vals, color=edge_color, alpha=0.8, linewidth=1.5)\n",
    "    \n",
    "    # Plot triangles as filled faces\n",
    "    face_polys = []\n",
    "    face_colors = []\n",
    "    for tri in triangles:\n",
    "        # Get the 3 vertices for this triangle\n",
    "        pts = [coords_3d[idx] for idx in tri]\n",
    "        face_polys.append(pts)\n",
    "        # Color can be computed from the average index of the triangle's vertices\n",
    "        avg_idx = int(np.mean(tri))\n",
    "        face_colors.append(cmap(norm(avg_idx)))\n",
    "    \n",
    "    # Create a Poly3DCollection for the triangles with a set transparency (alpha)\n",
    "    poly_collection = Poly3DCollection(face_polys, alpha=0.3, edgecolor='k')\n",
    "    poly_collection.set_facecolor(face_colors)\n",
    "    ax.add_collection3d(poly_collection)\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(f\"\", pad=20)\n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")\n",
    "    ax.set_zlabel(\"PCA 3\")\n",
    "    \n",
    "    # Add colorbar for node indices\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, pad=0.1)\n",
    "    cbar.set_label(\"Node Index\")\n",
    "        # Define three different viewing angles\n",
    "    angles = [(15, 180), (30, 90), (45, 0)]  # (elevation, azimuth) in degrees\n",
    "    dir_fig_save=working_dir+f'rips_skeletons/{dataset_name}_{window}_{step}/'\n",
    "    os.makedirs(dir_fig_save, exist_ok=True)\n",
    "\n",
    "    # Save figures from different angles\n",
    "    for i, (elev, azim) in enumerate(angles):\n",
    "        ax.view_init(elev=elev, azim=azim)  # Set camera angle\n",
    "        filename = dir_fig_save+f\"{entry}_{i}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')  # Save figure\n",
    "       # print(f\"Saved: {filename}\")\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def compute_euler_characteristic(simplex_tree, max_dim=4):\n",
    "    \"\"\"\n",
    "    Compute the Euler characteristic of a simplicial complex represented by a GUDHI simplex tree.\n",
    "    \n",
    "    Parameters:\n",
    "      simplex_tree: A GUDHI simplex tree containing simplices up to dimension max_dim.\n",
    "      max_dim: Maximum dimension to consider (e.g., 3 for tetrahedra).\n",
    "    \n",
    "    Returns:\n",
    "      euler: The Euler characteristic computed as \n",
    "             f0 - f1 + f2 - f3 + ... (up to max_dim).\n",
    "    \"\"\"\n",
    "    if not simplex_tree:\n",
    "        return None\n",
    "    # Dictionary to store counts for each dimension\n",
    "    simplex_counts = {}\n",
    "    for d in range(max_dim + 1):\n",
    "        # In GUDHI, a d-simplex is a simplex with d+1 vertices.\n",
    "        simplices_d = [simplex for simplex, filt in simplex_tree.get_skeleton(d) if len(simplex) == d + 1]\n",
    "        simplex_counts[d] = len(simplices_d)\n",
    "    #    print(f\"Number of {d}-simplices (f_{d}): {simplex_counts[d]}\")\n",
    "    \n",
    "    # Euler characteristic: sum_{d=0}^{max_dim} (-1)^d * f_d\n",
    "    euler = sum(((-1) ** d) * simplex_counts[d] for d in range(max_dim + 1))\n",
    "    return euler\n",
    "\n",
    "def get_rips_complex_G(df, embedding=str('sentence_embeddings')):\n",
    "    df['graph']=None\n",
    "    df['density']=None\n",
    "    df['edges']=None\n",
    "    df['tris']=None\n",
    "    df['tetra']=None\n",
    "    df['penta']=None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        G=nx.Graph()\n",
    "        \n",
    "        H = {}  # Hypergraph as a dictionary: {hyperedge_id: [vertices]}\n",
    "        embed = row[embedding] \n",
    "        rips_complex = row['rt_rips']\n",
    "        if not rips_complex:\n",
    "            continue\n",
    "        simplex_tree =  rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "\n",
    "        edges2 = []\n",
    "        triangles = []\n",
    "        tetrahedrons = []\n",
    "        fives=[]\n",
    "        # get_skeleton(2) returns all simplices up to dimension 2\n",
    "        for simplex, fvalue in simplex_tree.get_skeleton(4):\n",
    "            if len(simplex) >= 2:\n",
    "                for (i, j) in itertools.combinations(simplex, 2):\n",
    "                    G.add_edge(i, j, weight=fvalue)\n",
    "            if len(simplex) == 2:\n",
    "                edges2.append(simplex)\n",
    "            elif len(simplex) == 3:\n",
    "                triangles.append(simplex)\n",
    "            elif len(simplex) == 4:\n",
    "                tetrahedrons.append(simplex)\n",
    "            elif len(simplex) == 5:\n",
    "                fives.append(simplex)\n",
    "\n",
    "                    \n",
    "        print(len(G.nodes()), 'nodes')\n",
    "        print(len(embed), 'embed len')\n",
    "        df['edges'].loc[idx]=len(edges2)\n",
    "        df['tris'].loc[idx]=len(triangles)\n",
    "        df['tetra'].loc[idx]=len(tetrahedrons)\n",
    "        df['penta'].loc[idx]=len(tetrahedrons)\n",
    "        df['graph'].loc[idx]=G\n",
    "        df['density'].loc[idx]=nx.density(G)\n",
    "\n",
    "        \n",
    "    return df#df\n",
    "\n",
    "    ####\n",
    "import psutil\n",
    "\n",
    "# Create a psutil Process object for memory checks\n",
    "process = psutil.Process()\n",
    "memory_threshold_mb=7000\n",
    "def get_rips_time(df, embeddings='sentence_embeddings', step=0.025):\n",
    "    \"\"\"\n",
    "    For each row in df, build a Rips complex, extract dimension-D intervals\n",
    "    (e.g., D=0 => connected components, D=1 => loops, etc.),\n",
    "    then compute how many such features are 'alive' at increments of 'step'.\n",
    "\n",
    "    Creates two new columns in df:\n",
    "    - f\"scales_dim{D}\": The scale values\n",
    "    - f\"alive_dim{D}\": The counts of alive features at each scale\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid SettingWithCopy warnings\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Prepare two new columns (lists)\n",
    "    df[f'scales_dim0'] = None\n",
    "    df[f'alive_dim0'] = None\n",
    "    df[f'scales_dim1'] = None\n",
    "    df[f'alive_dim1'] = None\n",
    "    df[f'scales_dim2'] = None\n",
    "    df[f'alive_dim2'] = None\n",
    "    df['rt'] = None\n",
    "    df[\"rt_rips\"]=None\n",
    "    df[\"rips\"]=None\n",
    "    for idx, row in df.iterrows():\n",
    "        # Get the embeddings for this row\n",
    "        embed = row[embeddings]\n",
    "        if not isinstance(embed, (list, np.ndarray)) or len(embed) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Build the Rips Complex\n",
    "        if SPARSE:\n",
    "            rips_complex = gd.RipsComplex(points=embed, max_edge_length=3, sparse=sparse_param)\n",
    "        else:\n",
    "            rips_complex = gd.RipsComplex(points=embed, max_edge_length=3)\n",
    "            \n",
    "        current_mem_mb = process.memory_info().rss / (1024 * 1024)\n",
    "    #    print('rips mem',idx, current_mem_mb)\n",
    "        simplex_tree = rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "        current_mem_mb = process.memory_info().rss / (1024 * 1024)\n",
    "     #   print('simplex mem', current_mem_mb)\n",
    "        if current_mem_mb > memory_threshold_mb:\n",
    "            continue\n",
    "        # Extract dimension-D intervals from persistence\n",
    "        persistence = simplex_tree.persistence()\n",
    "        \n",
    "        current_mem_mb = process.memory_info().rss / (1024 * 1024)\n",
    "   #     print('persistence mem', current_mem_mb)\n",
    "        if current_mem_mb > memory_threshold_mb:\n",
    "            continue\n",
    "        for D in [0,1,2]:\n",
    "            births_dimD = []\n",
    "            deaths_dimD = []\n",
    "\n",
    "            for dim, (b, d) in persistence:\n",
    "                if dim == D and d != float('inf'):  # ignoring infinite intervals\n",
    "                    births_dimD.append(b)\n",
    "                    deaths_dimD.append(d)\n",
    "\n",
    "            # Compute how many features are alive at each scale\n",
    "            scales, alive_components = get_alive_components_over_scales(births_dimD, deaths_dimD, step=step)\n",
    "            if len(deaths_dimD)>0:\n",
    "                df.at[idx, f\"rt\"] = max(deaths_dimD)\n",
    "            # Store these lists in the new columns\n",
    "            df.at[idx, f\"scales_dim{D}\"] = scales\n",
    "            df.at[idx, f\"alive_dim{D}\"] = alive_components\n",
    "        rips_complex_max = gd.RipsComplex(points=embed, max_edge_length=df[\"rt\"].loc[idx], \\\n",
    "                                          sparse=sparse_param)\n",
    "        current_mem_mb = process.memory_info().rss / (1024 * 1024)\n",
    "      #  print('rips max mem', current_mem_mb)\n",
    "#         if current_mem_mb > memory_threshold_mb:\n",
    "#             continue\n",
    "        #simplex_tree_max = rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "        df.at[idx, f\"rt_rips\"] = rips_complex_max\n",
    "        df.at[idx, f\"rips\"] = rips_complex\n",
    "\n",
    "    return df\n",
    "\n",
    "import math\n",
    "\n",
    "def count_new_simplices_by_dimension_in_bins(simplex_tree, dimensions=(2, 3, 4, 5), bin_size=0.025):\n",
    "    # Initialize a dictionary for each dimension of interest.\n",
    "    counts = {dim: {} for dim in dimensions}\n",
    "    \n",
    "    # Get the (simplex, filtration) pairs from the simplex tree.\n",
    "    simplex_filtration = simplex_tree.get_filtration()\n",
    "    \n",
    "    for simplex, filt in simplex_filtration:\n",
    "        # Determine the dimension of the simplex.\n",
    "        dim = len(simplex) - 1\n",
    "        # Only process simplices with a dimension in the provided set.\n",
    "        if dim in dimensions:\n",
    "            bin_start = math.floor(filt / bin_size) * bin_size\n",
    "            counts[dim][bin_start] = counts[dim].get(bin_start, 0) + 1\n",
    "            \n",
    "    return counts\n",
    "def get_simplices_over_time(df, max_dimension=4,simplex_tree_type='rt_simplex_tree'):\n",
    "    \"\"\"\n",
    "    For each row in the dataframe (which contains a simplex tree in the 'simplex_tree' column),\n",
    "    compute the simplex counts at different filtration values for dimensions 2, 3, and 4.\n",
    "    \n",
    "    The filtration values and the corresponding counts are stored in separate columns.\n",
    "    \n",
    "    New columns added:\n",
    "      - simplex_time_dim{D}_filtration: the list of filtration values.\n",
    "      - simplex_time_dim{D}_count: the list of simplex counts for that dimension.\n",
    "      \n",
    "    Parameters:\n",
    "        df: pandas DataFrame that contains a column 'simplex_tree'\n",
    "        max_dimension: maximum dimension to be passed to count_simplices (not used in this snippet,\n",
    "                       but can be used if you want to generalize further).\n",
    "    \n",
    "    Returns:\n",
    "        The dataframe with additional columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # For each dimension, create two new columns for the filtration values and counts.\n",
    "    for D in [2, 3, 4]:\n",
    "        df[f'simplex_time_dim{D}_filtration'] = None\n",
    "        df[f'simplex_time_dim{D}_count'] = None\n",
    "\n",
    "    # Process each row in the dataframe.\n",
    "    for idx, row in df.iterrows():\n",
    "        # Calculate the counts for the current simplex tree.\n",
    "        simplex_counts = count_new_simplices_by_dimension_in_bins(row[simplex_tree_type])\n",
    "        for D in [2,3,4]:\n",
    "            # Get the dictionary for the current dimension D.\n",
    "            # (If there are no simplices of this dimension, we set empty lists.)\n",
    "            if D in simplex_counts and simplex_counts[D]:\n",
    "                # Sort the bins so that the lists are ordered by increasing filtration value.\n",
    "                bins = sorted(simplex_counts[D].keys())\n",
    "                counts_list = [simplex_counts[D][b] for b in bins]\n",
    "            else:\n",
    "                bins, counts_list = [], []\n",
    "            df.at[idx, f'simplex_time_dim{D}_filtration'] = bins\n",
    "            df.at[idx, f'simplex_time_dim{D}_count'] = counts_list\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mean pooling\n",
      "130\n",
      "130\n",
      "performing TDA on  PEM_df  span:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 nodes\n",
      "82 embed len\n",
      "75 nodes\n",
      "77 embed len\n",
      "71 nodes\n",
      "71 embed len\n",
      "68 nodes\n",
      "68 embed len\n",
      "52 nodes\n",
      "52 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   4%|▉                        | 1/26 [00:33<13:53, 33.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 nodes\n",
      "58 embed len\n",
      "87 nodes\n",
      "91 embed len\n",
      "70 nodes\n",
      "71 embed len\n",
      "65 nodes\n",
      "66 embed len\n",
      "55 nodes\n",
      "55 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   8%|█▉                       | 2/26 [01:22<17:06, 42.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 nodes\n",
      "68 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "94 nodes\n",
      "99 embed len\n",
      "76 nodes\n",
      "76 embed len\n",
      "76 nodes\n",
      "77 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  12%|██▉                      | 3/26 [02:36<21:53, 57.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 nodes\n",
      "78 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "88 nodes\n",
      "90 embed len\n",
      "88 nodes\n",
      "89 embed len\n",
      "54 nodes\n",
      "56 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  15%|███▊                     | 4/26 [03:24<19:32, 53.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 nodes\n",
      "104 embed len\n",
      "92 nodes\n",
      "93 embed len\n",
      "110 nodes\n",
      "111 embed len\n",
      "78 nodes\n",
      "86 embed len\n",
      "67 nodes\n",
      "68 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  19%|████▊                    | 5/26 [05:19<26:27, 75.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 nodes\n",
      "65 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "63 nodes\n",
      "64 embed len\n",
      "77 nodes\n",
      "80 embed len\n",
      "69 nodes\n",
      "71 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  23%|█████▊                   | 6/26 [05:59<21:10, 63.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 nodes\n",
      "69 embed len\n",
      "120 nodes\n",
      "123 embed len\n",
      "63 nodes\n",
      "64 embed len\n",
      "44 nodes\n",
      "47 embed len\n",
      "85 nodes\n",
      "88 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▋                  | 7/26 [08:14<27:27, 86.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 nodes\n",
      "83 embed len\n",
      "68 nodes\n",
      "69 embed len\n",
      "89 nodes\n",
      "98 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "59 nodes\n",
      "59 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  31%|███████▋                 | 8/26 [08:59<22:02, 73.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 nodes\n",
      "90 embed len\n",
      "101 nodes\n",
      "101 embed len\n",
      "75 nodes\n",
      "77 embed len\n",
      "78 nodes\n",
      "79 embed len\n",
      "33 nodes\n",
      "33 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  35%|████████▋                | 9/26 [10:22<21:42, 76.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 nodes\n",
      "98 embed len\n",
      "69 nodes\n",
      "70 embed len\n",
      "128 nodes\n",
      "128 embed len\n",
      "111 nodes\n",
      "112 embed len\n",
      "57 nodes\n",
      "57 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  38%|████████▊              | 10/26 [13:25<29:09, 109.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 nodes\n",
      "140 embed len\n",
      "85 nodes\n",
      "85 embed len\n",
      "82 nodes\n",
      "84 embed len\n",
      "126 nodes\n",
      "128 embed len\n",
      "59 nodes\n",
      "63 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  42%|█████████▋             | 11/26 [18:26<42:02, 168.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 nodes\n",
      "56 embed len\n",
      "51 nodes\n",
      "52 embed len\n",
      "95 nodes\n",
      "99 embed len\n",
      "62 nodes\n",
      "62 embed len\n",
      "102 nodes\n",
      "102 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  46%|██████████▌            | 12/26 [19:43<32:43, 140.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 nodes\n",
      "56 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "58 nodes\n",
      "63 embed len\n",
      "74 nodes\n",
      "75 embed len\n",
      "55 nodes\n",
      "56 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|███████████▌           | 13/26 [20:10<22:57, 105.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 nodes\n",
      "59 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "76 nodes\n",
      "83 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "91 nodes\n",
      "93 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  54%|████████████▍          | 14/26 [22:03<21:37, 108.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 nodes\n",
      "67 embed len\n",
      "67 nodes\n",
      "68 embed len\n",
      "96 nodes\n",
      "99 embed len\n",
      "64 nodes\n",
      "65 embed len\n",
      "60 nodes\n",
      "60 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  58%|█████████████▊          | 15/26 [23:19<18:02, 98.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 nodes\n",
      "126 embed len\n",
      "76 nodes\n",
      "76 embed len\n",
      "88 nodes\n",
      "90 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "93 nodes\n",
      "94 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  62%|██████████████▏        | 16/26 [26:15<20:19, 121.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 nodes\n",
      "72 embed len\n",
      "100 nodes\n",
      "100 embed len\n",
      "107 nodes\n",
      "108 embed len\n",
      "82 nodes\n",
      "83 embed len\n",
      "87 nodes\n",
      "88 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  65%|███████████████        | 17/26 [31:10<26:05, 173.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 nodes\n",
      "73 embed len\n",
      "88 nodes\n",
      "92 embed len\n",
      "70 nodes\n",
      "71 embed len\n",
      "85 nodes\n",
      "88 embed len\n",
      "83 nodes\n",
      "87 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  69%|███████████████▉       | 18/26 [33:06<20:50, 156.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 nodes\n",
      "55 embed len\n",
      "65 nodes\n",
      "66 embed len\n",
      "60 nodes\n",
      "62 embed len\n",
      "81 nodes\n",
      "84 embed len\n",
      "84 nodes\n",
      "88 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|████████████████▊      | 19/26 [34:15<15:11, 130.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 nodes\n",
      "43 embed len\n",
      "65 nodes\n",
      "69 embed len\n",
      "47 nodes\n",
      "47 embed len\n",
      "79 nodes\n",
      "79 embed len\n",
      "83 nodes\n",
      "87 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  77%|█████████████████▋     | 20/26 [35:27<11:16, 112.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 nodes\n",
      "67 embed len\n",
      "84 nodes\n",
      "84 embed len\n",
      "62 nodes\n",
      "63 embed len\n",
      "74 nodes\n",
      "79 embed len\n",
      "57 nodes\n",
      "62 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  81%|██████████████████▌    | 21/26 [36:43<08:27, 101.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 nodes\n",
      "89 embed len\n",
      "65 nodes\n",
      "67 embed len\n",
      "107 nodes\n",
      "107 embed len\n",
      "93 nodes\n",
      "93 embed len\n",
      "69 nodes\n",
      "71 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  85%|███████████████████▍   | 22/26 [39:21<07:53, 118.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 nodes\n",
      "66 embed len\n",
      "87 nodes\n",
      "87 embed len\n",
      "61 nodes\n",
      "64 embed len\n",
      "65 nodes\n",
      "65 embed len\n",
      "95 nodes\n",
      "95 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  88%|████████████████████▎  | 23/26 [41:04<05:42, 114.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 nodes\n",
      "38 embed len\n",
      "43 nodes\n",
      "45 embed len\n",
      "91 nodes\n",
      "93 embed len\n",
      "89 nodes\n",
      "90 embed len\n",
      "118 nodes\n",
      "119 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  92%|█████████████████████▏ | 24/26 [44:31<04:44, 142.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 nodes\n",
      "55 embed len\n",
      "74 nodes\n",
      "82 embed len\n",
      "44 nodes\n",
      "45 embed len\n",
      "86 nodes\n",
      "87 embed len\n",
      "88 nodes\n",
      "90 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  96%|██████████████████████ | 25/26 [46:08<02:08, 128.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 nodes\n",
      "80 embed len\n",
      "83 nodes\n",
      "87 embed len\n",
      "111 nodes\n",
      "111 embed len\n",
      "72 nodes\n",
      "72 embed len\n",
      "59 nodes\n",
      "59 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|███████████████████████| 26/26 [48:45<00:00, 112.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! PEM_df span: 1\n",
      "101\n",
      "101\n",
      "performing TDA on  SER_monologs  span:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 nodes\n",
      "18 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "21 nodes\n",
      "21 embed len\n",
      "33 nodes\n",
      "37 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   5%|█▎                       | 1/20 [00:02<00:51,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 nodes\n",
      "36 embed len\n",
      "32 nodes\n",
      "32 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "33 nodes\n",
      "33 embed len\n",
      "53 nodes\n",
      "54 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  10%|██▌                      | 2/20 [00:10<01:46,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 nodes\n",
      "54 embed len\n",
      "26 nodes\n",
      "27 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "31 nodes\n",
      "31 embed len\n",
      "33 nodes\n",
      "33 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  15%|███▊                     | 3/20 [00:16<01:40,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 nodes\n",
      "49 embed len\n",
      "34 nodes\n",
      "37 embed len\n",
      "27 nodes\n",
      "27 embed len\n",
      "34 nodes\n",
      "36 embed len\n",
      "49 nodes\n",
      "50 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  20%|█████                    | 4/20 [00:25<01:54,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 nodes\n",
      "59 embed len\n",
      "54 nodes\n",
      "57 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "56 nodes\n",
      "57 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  25%|██████▎                  | 5/20 [00:38<02:17,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 nodes\n",
      "70 embed len\n",
      "64 nodes\n",
      "68 embed len\n",
      "47 nodes\n",
      "47 embed len\n",
      "28 nodes\n",
      "28 embed len\n",
      "43 nodes\n",
      "44 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  30%|███████▌                 | 6/20 [01:02<03:19, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 nodes\n",
      "38 embed len\n",
      "38 nodes\n",
      "39 embed len\n",
      "40 nodes\n",
      "42 embed len\n",
      "21 nodes\n",
      "21 embed len\n",
      "33 nodes\n",
      "43 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  35%|████████▊                | 7/20 [01:06<02:21, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 nodes\n",
      "60 embed len\n",
      "47 nodes\n",
      "50 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "50 nodes\n",
      "50 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  40%|██████████               | 8/20 [01:25<02:39, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 nodes\n",
      "47 embed len\n",
      "44 nodes\n",
      "45 embed len\n",
      "109 nodes\n",
      "109 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "57 nodes\n",
      "57 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  45%|███████████▎             | 9/20 [02:41<06:01, 32.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 nodes\n",
      "66 embed len\n",
      "32 nodes\n",
      "32 embed len\n",
      "27 nodes\n",
      "27 embed len\n",
      "28 nodes\n",
      "30 embed len\n",
      "24 nodes\n",
      "24 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████            | 10/20 [03:03<04:55, 29.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 nodes\n",
      "41 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "44 nodes\n",
      "45 embed len\n",
      "27 nodes\n",
      "28 embed len\n",
      "53 nodes\n",
      "53 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  55%|█████████████▏          | 11/20 [03:14<03:35, 23.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 nodes\n",
      "42 embed len\n",
      "68 nodes\n",
      "70 embed len\n",
      "62 nodes\n",
      "70 embed len\n",
      "99 nodes\n",
      "101 embed len\n",
      "15 nodes\n",
      "15 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  60%|██████████████▍         | 12/20 [04:38<05:37, 42.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 nodes\n",
      "33 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "37 nodes\n",
      "38 embed len\n",
      "41 nodes\n",
      "41 embed len\n",
      "11 nodes\n",
      "11 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  65%|███████████████▌        | 13/20 [04:43<03:36, 30.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 nodes\n",
      "15 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "24 nodes\n",
      "24 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "21 nodes\n",
      "21 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  70%|████████████████▊       | 14/20 [04:44<02:11, 21.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 nodes\n",
      "26 embed len\n",
      "27 nodes\n",
      "28 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "31 nodes\n",
      "31 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  75%|██████████████████      | 15/20 [04:47<01:20, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 nodes\n",
      "28 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "40 nodes\n",
      "40 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  80%|███████████████████▏    | 16/20 [04:52<00:51, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 nodes\n",
      "37 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "30 nodes\n",
      "30 embed len\n",
      "26 nodes\n",
      "27 embed len\n",
      "23 nodes\n",
      "23 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  85%|████████████████████▍   | 17/20 [04:56<00:30, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 nodes\n",
      "35 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "39 nodes\n",
      "40 embed len\n",
      "24 nodes\n",
      "24 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  90%|█████████████████████▌  | 18/20 [04:58<00:15,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 nodes\n",
      "63 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "24 nodes\n",
      "26 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▊ | 19/20 [05:06<00:07,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 nodes\n",
      "29 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "17 nodes\n",
      "18 embed len\n",
      "41 nodes\n",
      "42 embed len\n",
      "39 nodes\n",
      "40 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 20/20 [05:13<00:00, 15.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER_monologs span: 1\n",
      "107\n",
      "107\n",
      "performing TDA on  SER_IPSP  span:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 nodes\n",
      "37 embed len\n",
      "59 nodes\n",
      "59 embed len\n",
      "45 nodes\n",
      "46 embed len\n",
      "53 nodes\n",
      "53 embed len\n",
      "36 nodes\n",
      "37 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   5%|█▏                       | 1/22 [00:12<04:12, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 nodes\n",
      "40 embed len\n",
      "52 nodes\n",
      "53 embed len\n",
      "44 nodes\n",
      "54 embed len\n",
      "58 nodes\n",
      "62 embed len\n",
      "57 nodes\n",
      "59 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   9%|██▎                      | 2/22 [00:34<06:01, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 nodes\n",
      "46 embed len\n",
      "77 nodes\n",
      "82 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "55 nodes\n",
      "55 embed len\n",
      "45 nodes\n",
      "47 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▍                     | 3/22 [00:57<06:26, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 nodes\n",
      "53 embed len\n",
      "50 nodes\n",
      "52 embed len\n",
      "47 nodes\n",
      "47 embed len\n",
      "46 nodes\n",
      "48 embed len\n",
      "74 nodes\n",
      "83 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  18%|████▌                    | 4/22 [01:54<10:30, 35.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 nodes\n",
      "61 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "35 nodes\n",
      "39 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "39 nodes\n",
      "44 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  23%|█████▋                   | 5/22 [02:05<07:26, 26.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 nodes\n",
      "67 embed len\n",
      "64 nodes\n",
      "66 embed len\n",
      "70 nodes\n",
      "71 embed len\n",
      "47 nodes\n",
      "53 embed len\n",
      "64 nodes\n",
      "66 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▊                  | 6/22 [02:48<08:31, 31.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 nodes\n",
      "47 embed len\n",
      "33 nodes\n",
      "36 embed len\n",
      "37 nodes\n",
      "38 embed len\n",
      "20 nodes\n",
      "21 embed len\n",
      "33 nodes\n",
      "34 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  32%|███████▉                 | 7/22 [02:53<05:47, 23.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 nodes\n",
      "60 embed len\n",
      "32 nodes\n",
      "34 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "49 nodes\n",
      "52 embed len\n",
      "53 nodes\n",
      "53 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  36%|█████████                | 8/22 [03:17<05:25, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 nodes\n",
      "43 embed len\n",
      "58 nodes\n",
      "59 embed len\n",
      "47 nodes\n",
      "47 embed len\n",
      "74 nodes\n",
      "75 embed len\n",
      "56 nodes\n",
      "58 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  41%|██████████▏              | 9/22 [03:59<06:19, 29.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 nodes\n",
      "76 embed len\n",
      "77 nodes\n",
      "79 embed len\n",
      "79 nodes\n",
      "83 embed len\n",
      "60 nodes\n",
      "61 embed len\n",
      "57 nodes\n",
      "57 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  45%|██████████▉             | 10/22 [05:31<09:43, 48.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 nodes\n",
      "44 embed len\n",
      "35 nodes\n",
      "36 embed len\n",
      "49 nodes\n",
      "51 embed len\n",
      "37 nodes\n",
      "39 embed len\n",
      "38 nodes\n",
      "39 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████            | 11/22 [05:45<06:58, 38.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 nodes\n",
      "20 embed len\n",
      "42 nodes\n",
      "46 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "45 nodes\n",
      "46 embed len\n",
      "37 nodes\n",
      "39 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  55%|█████████████           | 12/22 [05:53<04:50, 29.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 nodes\n",
      "58 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "58 nodes\n",
      "63 embed len\n",
      "58 nodes\n",
      "58 embed len\n",
      "78 nodes\n",
      "80 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  59%|██████████████▏         | 13/22 [06:47<05:27, 36.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 nodes\n",
      "100 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "52 nodes\n",
      "56 embed len\n",
      "57 nodes\n",
      "58 embed len\n",
      "35 nodes\n",
      "35 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  64%|███████████████▎        | 14/22 [07:51<05:58, 44.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 nodes\n",
      "50 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "25 nodes\n",
      "28 embed len\n",
      "19 nodes\n",
      "19 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  68%|████████████████▎       | 15/22 [08:06<04:09, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 nodes\n",
      "78 embed len\n",
      "90 nodes\n",
      "99 embed len\n",
      "75 nodes\n",
      "84 embed len\n",
      "25 nodes\n",
      "26 embed len\n",
      "23 nodes\n",
      "25 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|█████████████████▍      | 16/22 [09:14<04:33, 45.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 nodes\n",
      "23 embed len\n",
      "59 nodes\n",
      "60 embed len\n",
      "59 nodes\n",
      "60 embed len\n",
      "49 nodes\n",
      "50 embed len\n",
      "74 nodes\n",
      "74 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  77%|██████████████████▌     | 17/22 [10:05<03:55, 47.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 nodes\n",
      "78 embed len\n",
      "82 nodes\n",
      "85 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "61 nodes\n",
      "63 embed len\n",
      "40 nodes\n",
      "50 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  82%|███████████████████▋    | 18/22 [11:11<03:31, 52.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 nodes\n",
      "25 embed len\n",
      "23 nodes\n",
      "23 embed len\n",
      "55 nodes\n",
      "56 embed len\n",
      "56 nodes\n",
      "59 embed len\n",
      "63 nodes\n",
      "68 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  86%|████████████████████▋   | 19/22 [11:32<02:09, 43.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 nodes\n",
      "35 embed len\n",
      "28 nodes\n",
      "28 embed len\n",
      "30 nodes\n",
      "30 embed len\n",
      "53 nodes\n",
      "54 embed len\n",
      "71 nodes\n",
      "74 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  91%|█████████████████████▊  | 20/22 [12:00<01:17, 38.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 nodes\n",
      "69 embed len\n",
      "30 nodes\n",
      "30 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "49 nodes\n",
      "53 embed len\n",
      "43 nodes\n",
      "46 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▉ | 21/22 [12:23<00:34, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 nodes\n",
      "49 embed len\n",
      "62 nodes\n",
      "64 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 22/22 [12:32<00:00, 34.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER_IPSP span: 1\n",
      "108\n",
      "108\n",
      "performing TDA on  SER1  span:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   5%|█▏                       | 1/22 [00:00<00:14,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 nodes\n",
      "15 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   9%|██▎                      | 2/22 [00:01<00:18,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "9 embed len\n",
      "5 nodes\n",
      "9 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▍                     | 3/22 [00:02<00:17,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 nodes\n",
      "12 embed len\n",
      "7 nodes\n",
      "9 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  18%|████▌                    | 4/22 [00:03<00:17,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  23%|█████▋                   | 5/22 [00:04<00:16,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▊                  | 6/22 [00:05<00:15,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 nodes\n",
      "9 embed len\n",
      "8 nodes\n",
      "9 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  32%|███████▉                 | 7/22 [00:06<00:13,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "10 embed len\n",
      "7 nodes\n",
      "8 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  36%|█████████                | 8/22 [00:07<00:12,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "10 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "14 nodes\n",
      "15 embed len\n",
      "20 nodes\n",
      "20 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  41%|██████████▏              | 9/22 [00:08<00:13,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 nodes\n",
      "12 embed len\n",
      "23 nodes\n",
      "23 embed len\n",
      "21 nodes\n",
      "21 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "12 nodes\n",
      "12 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  45%|██████████▉             | 10/22 [00:10<00:14,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 nodes\n",
      "9 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "16 nodes\n",
      "17 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "6 nodes\n",
      "6 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████            | 11/22 [00:11<00:12,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 nodes\n",
      "12 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "6 nodes\n",
      "6 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  55%|█████████████           | 12/22 [00:12<00:11,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  59%|██████████████▏         | 13/22 [00:13<00:10,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "13 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "8 nodes\n",
      "9 embed len\n",
      "4 nodes\n",
      "4 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  64%|███████████████▎        | 14/22 [00:14<00:09,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "15 nodes\n",
      "15 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  68%|████████████████▎       | 15/22 [00:16<00:08,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "8 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "10 nodes\n",
      "11 embed len\n",
      "5 nodes\n",
      "6 embed len\n",
      "3 nodes\n",
      "3 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|█████████████████▍      | 16/22 [00:16<00:06,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "6 embed len\n",
      "7 nodes\n",
      "8 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  77%|██████████████████▌     | 17/22 [00:17<00:05,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "10 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "8 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  82%|███████████████████▋    | 18/22 [00:18<00:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  86%|████████████████████▋   | 19/22 [00:19<00:03,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 nodes\n",
      "9 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  91%|█████████████████████▊  | 20/22 [00:20<00:02,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "10 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "8 nodes\n",
      "10 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▉ | 21/22 [00:22<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 nodes\n",
      "7 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "11 nodes\n",
      "13 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 22/22 [00:22<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER1 span: 1\n",
      "66\n",
      "66\n",
      "performing TDA on  MASM  span:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 nodes\n",
      "56 embed len\n",
      "49 nodes\n",
      "56 embed len\n",
      "63 nodes\n",
      "72 embed len\n",
      "52 nodes\n",
      "54 embed len\n",
      "36 nodes\n",
      "38 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   7%|█▊                       | 1/14 [00:14<03:05, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 nodes\n",
      "67 embed len\n",
      "50 nodes\n",
      "57 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "54 nodes\n",
      "58 embed len\n",
      "41 nodes\n",
      "49 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▌                     | 2/14 [00:27<02:45, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 nodes\n",
      "44 embed len\n",
      "35 nodes\n",
      "41 embed len\n",
      "78 nodes\n",
      "83 embed len\n",
      "80 nodes\n",
      "91 embed len\n",
      "80 nodes\n",
      "90 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  21%|█████▎                   | 3/14 [01:25<06:13, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 nodes\n",
      "42 embed len\n",
      "50 nodes\n",
      "56 embed len\n",
      "57 nodes\n",
      "62 embed len\n",
      "64 nodes\n",
      "76 embed len\n",
      "50 nodes\n",
      "54 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  29%|███████▏                 | 4/14 [01:47<04:50, 29.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 nodes\n",
      "70 embed len\n",
      "55 nodes\n",
      "72 embed len\n",
      "56 nodes\n",
      "72 embed len\n",
      "78 nodes\n",
      "85 embed len\n",
      "66 nodes\n",
      "74 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  36%|████████▉                | 5/14 [02:22<04:42, 31.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 nodes\n",
      "52 embed len\n",
      "29 nodes\n",
      "29 embed len\n",
      "18 nodes\n",
      "21 embed len\n",
      "34 nodes\n",
      "40 embed len\n",
      "34 nodes\n",
      "38 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  43%|██████████▋              | 6/14 [02:26<02:55, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 nodes\n",
      "23 embed len\n",
      "47 nodes\n",
      "53 embed len\n",
      "51 nodes\n",
      "59 embed len\n",
      "25 nodes\n",
      "26 embed len\n",
      "50 nodes\n",
      "52 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████▌            | 7/14 [02:34<02:00, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 nodes\n",
      "30 embed len\n",
      "64 nodes\n",
      "73 embed len\n",
      "50 nodes\n",
      "55 embed len\n",
      "131 nodes\n",
      "155 embed len\n",
      "129 nodes\n",
      "145 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  57%|██████████████▎          | 8/14 [06:58<09:35, 95.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 nodes\n",
      "60 embed len\n",
      "48 nodes\n",
      "50 embed len\n",
      "59 nodes\n",
      "66 embed len\n",
      "46 nodes\n",
      "51 embed len\n",
      "43 nodes\n",
      "47 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  64%|████████████████         | 9/14 [07:18<06:01, 72.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 nodes\n",
      "57 embed len\n",
      "45 nodes\n",
      "47 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "40 nodes\n",
      "45 embed len\n",
      "85 nodes\n",
      "98 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  71%|█████████████████▏      | 10/14 [07:47<03:55, 58.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 nodes\n",
      "77 embed len\n",
      "48 nodes\n",
      "57 embed len\n",
      "32 nodes\n",
      "32 embed len\n",
      "46 nodes\n",
      "49 embed len\n",
      "45 nodes\n",
      "54 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  79%|██████████████████▊     | 11/14 [08:04<02:18, 46.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 nodes\n",
      "50 embed len\n",
      "34 nodes\n",
      "36 embed len\n",
      "67 nodes\n",
      "78 embed len\n",
      "43 nodes\n",
      "47 embed len\n",
      "73 nodes\n",
      "81 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  86%|████████████████████▌   | 12/14 [08:36<01:23, 41.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 nodes\n",
      "53 embed len\n",
      "42 nodes\n",
      "51 embed len\n",
      "43 nodes\n",
      "45 embed len\n",
      "67 nodes\n",
      "72 embed len\n",
      "46 nodes\n",
      "50 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  93%|██████████████████████▎ | 13/14 [08:52<00:33, 33.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 nodes\n",
      "54 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 14/14 [08:55<00:00, 38.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! MASM span: 1\n",
      "73\n",
      "73\n",
      "performing TDA on  cleaned_DEI  span:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 nodes\n",
      "16 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   7%|█▋                       | 1/15 [00:01<00:25,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 nodes\n",
      "14 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "16 nodes\n",
      "17 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "21 nodes\n",
      "21 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  13%|███▎                     | 2/15 [00:03<00:22,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 nodes\n",
      "23 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  20%|█████                    | 3/15 [00:05<00:19,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 nodes\n",
      "13 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "10 nodes\n",
      "11 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▋                  | 4/15 [00:05<00:14,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 nodes\n",
      "15 embed len\n",
      "17 nodes\n",
      "23 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "11 nodes\n",
      "11 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  33%|████████▎                | 5/15 [00:07<00:12,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 nodes\n",
      "7 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "14 nodes\n",
      "14 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  40%|██████████               | 6/15 [00:08<00:10,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 nodes\n",
      "11 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "12 nodes\n",
      "13 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "11 nodes\n",
      "11 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  47%|███████████▋             | 7/15 [00:09<00:10,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 nodes\n",
      "14 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "23 nodes\n",
      "23 embed len\n",
      "16 nodes\n",
      "16 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  53%|█████████████▎           | 8/15 [00:11<00:09,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "9 nodes\n",
      "12 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  60%|███████████████          | 9/15 [00:12<00:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 nodes\n",
      "18 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "20 nodes\n",
      "20 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  67%|████████████████        | 10/15 [00:13<00:06,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "6 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "17 nodes\n",
      "17 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|█████████████████▌      | 11/15 [00:14<00:05,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 nodes\n",
      "13 embed len\n",
      "13 nodes\n",
      "14 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "16 nodes\n",
      "16 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  80%|███████████████████▏    | 12/15 [00:16<00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 nodes\n",
      "19 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "12 nodes\n",
      "12 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  87%|████████████████████▊   | 13/15 [00:17<00:02,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "10 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "22 nodes\n",
      "22 embed len\n",
      "15 nodes\n",
      "15 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  93%|██████████████████████▍ | 14/15 [00:18<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 nodes\n",
      "13 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 15/15 [00:19<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! cleaned_DEI span: 1\n",
      "130\n",
      "130\n",
      "performing TDA on  PEM_df  span:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 nodes\n",
      "81 embed len\n",
      "76 nodes\n",
      "76 embed len\n",
      "70 nodes\n",
      "70 embed len\n",
      "67 nodes\n",
      "67 embed len\n",
      "51 nodes\n",
      "51 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   4%|▉                        | 1/26 [01:01<25:46, 61.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 nodes\n",
      "57 embed len\n",
      "90 nodes\n",
      "90 embed len\n",
      "70 nodes\n",
      "70 embed len\n",
      "65 nodes\n",
      "65 embed len\n",
      "54 nodes\n",
      "54 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   8%|█▉                       | 2/26 [02:15<27:35, 69.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 nodes\n",
      "67 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "98 nodes\n",
      "98 embed len\n",
      "75 nodes\n",
      "75 embed len\n",
      "76 nodes\n",
      "76 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  12%|██▊                     | 3/26 [04:34<38:33, 100.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 nodes\n",
      "77 embed len\n",
      "42 nodes\n",
      "42 embed len\n",
      "89 nodes\n",
      "89 embed len\n",
      "88 nodes\n",
      "88 embed len\n",
      "55 nodes\n",
      "55 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  15%|███▋                    | 4/26 [06:29<39:01, 106.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 nodes\n",
      "103 embed len\n",
      "92 nodes\n",
      "92 embed len\n",
      "110 nodes\n",
      "110 embed len\n",
      "85 nodes\n",
      "85 embed len\n",
      "67 nodes\n",
      "67 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  19%|████▌                   | 5/26 [09:38<47:40, 136.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 nodes\n",
      "64 embed len\n",
      "53 nodes\n",
      "53 embed len\n",
      "63 nodes\n",
      "63 embed len\n",
      "79 nodes\n",
      "79 embed len\n",
      "70 nodes\n",
      "70 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  23%|█████▌                  | 6/26 [10:38<36:49, 110.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 nodes\n",
      "68 embed len\n",
      "122 nodes\n",
      "122 embed len\n",
      "63 nodes\n",
      "63 embed len\n",
      "46 nodes\n",
      "46 embed len\n",
      "87 nodes\n",
      "87 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▍                 | 7/26 [12:53<37:25, 118.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 nodes\n",
      "82 embed len\n",
      "68 nodes\n",
      "68 embed len\n",
      "97 nodes\n",
      "97 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "58 nodes\n",
      "58 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  31%|███████▍                | 8/26 [14:28<33:18, 111.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 nodes\n",
      "89 embed len\n",
      "100 nodes\n",
      "100 embed len\n",
      "76 nodes\n",
      "76 embed len\n",
      "78 nodes\n",
      "78 embed len\n",
      "32 nodes\n",
      "32 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  35%|████████▎               | 9/26 [16:07<30:20, 107.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 nodes\n",
      "97 embed len\n",
      "69 nodes\n",
      "69 embed len\n",
      "127 nodes\n",
      "127 embed len\n",
      "111 nodes\n",
      "111 embed len\n",
      "56 nodes\n",
      "56 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  38%|████████▊              | 10/26 [20:56<43:34, 163.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 nodes\n",
      "139 embed len\n",
      "84 nodes\n",
      "84 embed len\n",
      "83 nodes\n",
      "83 embed len\n",
      "127 nodes\n",
      "127 embed len\n",
      "62 nodes\n",
      "62 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  42%|████████▉            | 11/26 [31:39<1:17:30, 310.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 nodes\n",
      "55 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "98 nodes\n",
      "98 embed len\n",
      "61 nodes\n",
      "61 embed len\n",
      "101 nodes\n",
      "101 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  46%|██████████▌            | 12/26 [33:40<58:58, 252.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 nodes\n",
      "55 embed len\n",
      "53 nodes\n",
      "53 embed len\n",
      "62 nodes\n",
      "62 embed len\n",
      "74 nodes\n",
      "74 embed len\n",
      "55 nodes\n",
      "55 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|███████████▌           | 13/26 [34:32<41:35, 191.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 nodes\n",
      "58 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "82 nodes\n",
      "82 embed len\n",
      "48 nodes\n",
      "48 embed len\n",
      "92 nodes\n",
      "92 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  54%|████████████▍          | 14/26 [35:51<31:33, 157.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 nodes\n",
      "66 embed len\n",
      "67 nodes\n",
      "67 embed len\n",
      "98 nodes\n",
      "98 embed len\n",
      "64 nodes\n",
      "64 embed len\n",
      "59 nodes\n",
      "59 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  58%|█████████████▎         | 15/26 [37:26<25:27, 138.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 nodes\n",
      "125 embed len\n",
      "75 nodes\n",
      "75 embed len\n",
      "89 nodes\n",
      "89 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "93 nodes\n",
      "93 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  62%|██████████████▏        | 16/26 [41:13<27:31, 165.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 nodes\n",
      "71 embed len\n",
      "99 nodes\n",
      "99 embed len\n",
      "107 nodes\n",
      "107 embed len\n",
      "82 nodes\n",
      "82 embed len\n",
      "87 nodes\n",
      "87 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  65%|███████████████        | 17/26 [43:33<23:40, 157.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 nodes\n",
      "72 embed len\n",
      "91 nodes\n",
      "91 embed len\n",
      "70 nodes\n",
      "70 embed len\n",
      "87 nodes\n",
      "87 embed len\n",
      "86 nodes\n",
      "86 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  69%|███████████████▉       | 18/26 [45:16<18:49, 141.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 nodes\n",
      "54 embed len\n",
      "65 nodes\n",
      "65 embed len\n",
      "61 nodes\n",
      "61 embed len\n",
      "83 nodes\n",
      "83 embed len\n",
      "87 nodes\n",
      "87 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|████████████████▊      | 19/26 [46:00<13:03, 111.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 nodes\n",
      "42 embed len\n",
      "68 nodes\n",
      "68 embed len\n",
      "46 nodes\n",
      "46 embed len\n",
      "78 nodes\n",
      "78 embed len\n",
      "86 nodes\n",
      "86 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  77%|██████████████████▍     | 20/26 [46:46<09:13, 92.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 nodes\n",
      "66 embed len\n",
      "83 nodes\n",
      "83 embed len\n",
      "62 nodes\n",
      "62 embed len\n",
      "78 nodes\n",
      "78 embed len\n",
      "61 nodes\n",
      "61 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  81%|███████████████████▍    | 21/26 [47:24<06:20, 76.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 nodes\n",
      "88 embed len\n",
      "66 nodes\n",
      "66 embed len\n",
      "106 nodes\n",
      "106 embed len\n",
      "92 nodes\n",
      "92 embed len\n",
      "70 nodes\n",
      "70 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  85%|████████████████████▎   | 22/26 [49:04<05:32, 83.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 nodes\n",
      "65 embed len\n",
      "86 nodes\n",
      "86 embed len\n",
      "63 nodes\n",
      "63 embed len\n",
      "64 nodes\n",
      "64 embed len\n",
      "94 nodes\n",
      "94 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  88%|█████████████████████▏  | 23/26 [50:04<03:48, 76.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 nodes\n",
      "37 embed len\n",
      "42 nodes\n",
      "44 embed len\n",
      "92 nodes\n",
      "92 embed len\n",
      "89 nodes\n",
      "89 embed len\n",
      "118 nodes\n",
      "118 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  92%|██████████████████████▏ | 24/26 [51:35<02:41, 80.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 nodes\n",
      "54 embed len\n",
      "80 nodes\n",
      "81 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "86 nodes\n",
      "86 embed len\n",
      "89 nodes\n",
      "89 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  96%|███████████████████████ | 25/26 [52:38<01:15, 75.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 nodes\n",
      "79 embed len\n",
      "86 nodes\n",
      "86 embed len\n",
      "110 nodes\n",
      "110 embed len\n",
      "71 nodes\n",
      "71 embed len\n",
      "58 nodes\n",
      "58 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|███████████████████████| 26/26 [54:49<00:00, 126.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! PEM_df span: 2\n",
      "101\n",
      "101\n",
      "performing TDA on  SER_monologs  span:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 nodes\n",
      "17 embed len\n",
      "33 nodes\n",
      "33 embed len\n",
      "24 nodes\n",
      "24 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "36 nodes\n",
      "36 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   5%|█▎                       | 1/20 [00:01<00:20,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 nodes\n",
      "35 embed len\n",
      "31 nodes\n",
      "31 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "32 nodes\n",
      "32 embed len\n",
      "53 nodes\n",
      "53 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  10%|██▌                      | 2/20 [00:03<00:36,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 nodes\n",
      "53 embed len\n",
      "25 nodes\n",
      "26 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "30 nodes\n",
      "30 embed len\n",
      "32 nodes\n",
      "32 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  15%|███▊                     | 3/20 [00:07<00:49,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 nodes\n",
      "48 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "49 nodes\n",
      "49 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  20%|█████                    | 4/20 [00:12<00:56,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 nodes\n",
      "58 embed len\n",
      "56 nodes\n",
      "56 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "56 nodes\n",
      "56 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  25%|██████▎                  | 5/20 [00:18<01:07,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 nodes\n",
      "69 embed len\n",
      "67 nodes\n",
      "67 embed len\n",
      "46 nodes\n",
      "46 embed len\n",
      "27 nodes\n",
      "27 embed len\n",
      "43 nodes\n",
      "43 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  30%|███████▌                 | 6/20 [00:34<01:59,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 nodes\n",
      "37 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "41 nodes\n",
      "41 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "42 nodes\n",
      "42 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  35%|████████▊                | 7/20 [00:36<01:23,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 nodes\n",
      "59 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "53 nodes\n",
      "53 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "49 nodes\n",
      "49 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  40%|██████████               | 8/20 [00:44<01:22,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 nodes\n",
      "46 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "108 nodes\n",
      "108 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "56 nodes\n",
      "56 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  45%|███████████▎             | 9/20 [01:14<02:33, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 nodes\n",
      "65 embed len\n",
      "31 nodes\n",
      "31 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "29 nodes\n",
      "29 embed len\n",
      "23 nodes\n",
      "23 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████            | 10/20 [01:19<01:52, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 nodes\n",
      "40 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "27 nodes\n",
      "27 embed len\n",
      "52 nodes\n",
      "52 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  55%|█████████████▏          | 11/20 [01:24<01:22,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 nodes\n",
      "41 embed len\n",
      "69 nodes\n",
      "69 embed len\n",
      "67 nodes\n",
      "69 embed len\n",
      "100 nodes\n",
      "100 embed len\n",
      "12 nodes\n",
      "14 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  60%|██████████████▍         | 12/20 [01:53<02:03, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 nodes\n",
      "32 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  65%|███████████████▌        | 13/20 [01:58<01:25, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 nodes\n",
      "14 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "23 nodes\n",
      "23 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "20 nodes\n",
      "20 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  70%|████████████████▊       | 14/20 [01:59<00:52,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 nodes\n",
      "25 embed len\n",
      "27 nodes\n",
      "27 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "30 nodes\n",
      "30 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  75%|██████████████████      | 15/20 [02:00<00:32,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 nodes\n",
      "27 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "39 nodes\n",
      "39 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  80%|███████████████████▏    | 16/20 [02:03<00:21,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 nodes\n",
      "36 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "29 nodes\n",
      "29 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "22 nodes\n",
      "22 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  85%|████████████████████▍   | 17/20 [02:05<00:12,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 nodes\n",
      "34 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "23 nodes\n",
      "23 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  90%|█████████████████████▌  | 18/20 [02:06<00:06,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 nodes\n",
      "62 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "24 nodes\n",
      "24 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "25 nodes\n",
      "25 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▊ | 19/20 [02:08<00:03,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 nodes\n",
      "28 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "41 nodes\n",
      "41 embed len\n",
      "39 nodes\n",
      "39 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 20/20 [02:10<00:00,  6.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER_monologs span: 2\n",
      "107\n",
      "107\n",
      "performing TDA on  SER_IPSP  span:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 nodes\n",
      "36 embed len\n",
      "58 nodes\n",
      "58 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "36 nodes\n",
      "36 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   5%|█▏                       | 1/22 [00:05<02:03,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 nodes\n",
      "39 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "52 nodes\n",
      "53 embed len\n",
      "61 nodes\n",
      "61 embed len\n",
      "58 nodes\n",
      "58 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   9%|██▎                      | 2/22 [00:18<03:21, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 nodes\n",
      "45 embed len\n",
      "81 nodes\n",
      "81 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "46 nodes\n",
      "46 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▍                     | 3/22 [00:55<07:02, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 nodes\n",
      "52 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "46 nodes\n",
      "46 embed len\n",
      "47 nodes\n",
      "47 embed len\n",
      "82 nodes\n",
      "82 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  18%|████▌                    | 4/22 [01:30<08:11, 27.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 nodes\n",
      "60 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "42 nodes\n",
      "43 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  23%|█████▋                   | 5/22 [01:39<05:48, 20.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 nodes\n",
      "66 embed len\n",
      "65 nodes\n",
      "65 embed len\n",
      "70 nodes\n",
      "70 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "65 nodes\n",
      "65 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▊                  | 6/22 [01:54<05:00, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 nodes\n",
      "46 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "33 nodes\n",
      "33 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  32%|███████▉                 | 7/22 [01:58<03:29, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 nodes\n",
      "59 embed len\n",
      "33 nodes\n",
      "33 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "52 nodes\n",
      "52 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  36%|█████████                | 8/22 [02:06<02:47, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 nodes\n",
      "42 embed len\n",
      "58 nodes\n",
      "58 embed len\n",
      "46 nodes\n",
      "46 embed len\n",
      "74 nodes\n",
      "74 embed len\n",
      "57 nodes\n",
      "57 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  41%|██████████▏              | 9/22 [02:26<03:10, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 nodes\n",
      "75 embed len\n",
      "78 nodes\n",
      "78 embed len\n",
      "82 nodes\n",
      "82 embed len\n",
      "60 nodes\n",
      "60 embed len\n",
      "56 nodes\n",
      "56 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  45%|██████████▉             | 10/22 [03:13<04:54, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 nodes\n",
      "43 embed len\n",
      "34 nodes\n",
      "35 embed len\n",
      "49 nodes\n",
      "50 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "38 nodes\n",
      "38 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████            | 11/22 [03:16<03:19, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 nodes\n",
      "19 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "37 nodes\n",
      "38 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  55%|█████████████           | 12/22 [03:21<02:18, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 nodes\n",
      "57 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "62 nodes\n",
      "62 embed len\n",
      "57 nodes\n",
      "57 embed len\n",
      "79 nodes\n",
      "79 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  59%|██████████████▏         | 13/22 [04:13<03:48, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 nodes\n",
      "99 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "54 nodes\n",
      "55 embed len\n",
      "57 nodes\n",
      "57 embed len\n",
      "34 nodes\n",
      "34 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  64%|███████████████▎        | 14/22 [04:45<03:39, 27.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 nodes\n",
      "49 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "24 nodes\n",
      "24 embed len\n",
      "27 nodes\n",
      "27 embed len\n",
      "18 nodes\n",
      "18 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  68%|████████████████▎       | 15/22 [04:47<02:18, 19.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 nodes\n",
      "77 embed len\n",
      "98 nodes\n",
      "98 embed len\n",
      "83 nodes\n",
      "83 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "24 nodes\n",
      "24 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|█████████████████▍      | 16/22 [05:27<02:35, 25.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 nodes\n",
      "22 embed len\n",
      "59 nodes\n",
      "59 embed len\n",
      "58 nodes\n",
      "59 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "73 nodes\n",
      "73 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  77%|██████████████████▌     | 17/22 [05:38<01:47, 21.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 nodes\n",
      "77 embed len\n",
      "84 nodes\n",
      "84 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "62 nodes\n",
      "62 embed len\n",
      "49 nodes\n",
      "49 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  82%|███████████████████▋    | 18/22 [06:10<01:38, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 nodes\n",
      "24 embed len\n",
      "22 nodes\n",
      "22 embed len\n",
      "55 nodes\n",
      "55 embed len\n",
      "58 nodes\n",
      "58 embed len\n",
      "67 nodes\n",
      "67 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  86%|████████████████████▋   | 19/22 [06:21<01:01, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 nodes\n",
      "34 embed len\n",
      "27 nodes\n",
      "27 embed len\n",
      "29 nodes\n",
      "29 embed len\n",
      "53 nodes\n",
      "53 embed len\n",
      "73 nodes\n",
      "73 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  91%|█████████████████████▊  | 20/22 [06:39<00:39, 19.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 nodes\n",
      "68 embed len\n",
      "29 nodes\n",
      "29 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "45 nodes\n",
      "45 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▉ | 21/22 [06:51<00:17, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 nodes\n",
      "48 embed len\n",
      "63 nodes\n",
      "63 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 22/22 [06:54<00:00, 18.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER_IPSP span: 2\n",
      "108\n",
      "108\n",
      "performing TDA on  SER1  span:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "3 nodes\n",
      "3 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:   9%|██▎                      | 2/22 [00:00<00:08,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 nodes\n",
      "14 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▍                     | 3/22 [00:01<00:07,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "11 embed len\n",
      "7 nodes\n",
      "8 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "4 nodes\n",
      "4 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  18%|████▌                    | 4/22 [00:01<00:08,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "6 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  23%|█████▋                   | 5/22 [00:02<00:07,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "6 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▊                  | 6/22 [00:02<00:07,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "6 nodes\n",
      "6 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  32%|███████▉                 | 7/22 [00:03<00:06,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 nodes\n",
      "9 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  36%|█████████                | 8/22 [00:03<00:06,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 nodes\n",
      "9 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "19 nodes\n",
      "19 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  41%|██████████▏              | 9/22 [00:03<00:05,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 nodes\n",
      "11 embed len\n",
      "22 nodes\n",
      "22 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "11 nodes\n",
      "11 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  45%|██████████▉             | 10/22 [00:04<00:06,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "8 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████            | 11/22 [00:05<00:05,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 nodes\n",
      "11 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  55%|█████████████           | 12/22 [00:05<00:05,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  59%|██████████████▏         | 13/22 [00:06<00:05,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 nodes\n",
      "12 embed len\n",
      "3 nodes\n",
      "3 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "3 nodes\n",
      "3 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  64%|███████████████▎        | 14/22 [00:06<00:04,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "6 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "14 nodes\n",
      "14 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  68%|████████████████▎       | 15/22 [00:07<00:03,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|█████████████████▍      | 16/22 [00:07<00:02,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  77%|██████████████████▌     | 17/22 [00:07<00:02,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 nodes\n",
      "4 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  82%|███████████████████▋    | 18/22 [00:08<00:01,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "8 nodes\n",
      "9 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  86%|████████████████████▋   | 19/22 [00:08<00:01,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  91%|█████████████████████▊  | 20/22 [00:09<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "6 nodes\n",
      "6 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▉ | 21/22 [00:09<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 nodes\n",
      "11 embed len\n",
      "12 nodes\n",
      "12 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 22/22 [00:10<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER1 span: 2\n",
      "66\n",
      "66\n",
      "performing TDA on  MASM  span:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 nodes\n",
      "55 embed len\n",
      "55 nodes\n",
      "55 embed len\n",
      "71 nodes\n",
      "71 embed len\n",
      "53 nodes\n",
      "53 embed len\n",
      "37 nodes\n",
      "37 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   7%|█▊                       | 1/14 [00:07<01:40,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 nodes\n",
      "66 embed len\n",
      "56 nodes\n",
      "56 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "57 nodes\n",
      "57 embed len\n",
      "48 nodes\n",
      "48 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▌                     | 2/14 [00:17<01:47,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 nodes\n",
      "43 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "82 nodes\n",
      "82 embed len\n",
      "90 nodes\n",
      "90 embed len\n",
      "89 nodes\n",
      "89 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  21%|█████▎                   | 3/14 [00:47<03:24, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 nodes\n",
      "41 embed len\n",
      "55 nodes\n",
      "55 embed len\n",
      "61 nodes\n",
      "61 embed len\n",
      "75 nodes\n",
      "75 embed len\n",
      "53 nodes\n",
      "53 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  29%|███████▏                 | 4/14 [01:01<02:49, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 nodes\n",
      "69 embed len\n",
      "71 nodes\n",
      "71 embed len\n",
      "71 nodes\n",
      "71 embed len\n",
      "84 nodes\n",
      "84 embed len\n",
      "73 nodes\n",
      "73 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  36%|████████▉                | 5/14 [01:23<02:47, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 nodes\n",
      "51 embed len\n",
      "28 nodes\n",
      "28 embed len\n",
      "20 nodes\n",
      "20 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "37 nodes\n",
      "37 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  43%|██████████▋              | 6/14 [01:25<01:44, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 nodes\n",
      "22 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "58 nodes\n",
      "58 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "51 nodes\n",
      "51 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████▌            | 7/14 [01:31<01:13, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 nodes\n",
      "29 embed len\n",
      "72 nodes\n",
      "72 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "154 nodes\n",
      "154 embed len\n",
      "144 nodes\n",
      "144 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  57%|██████████████▎          | 8/14 [04:34<06:32, 65.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 nodes\n",
      "59 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "65 nodes\n",
      "65 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "46 nodes\n",
      "46 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  64%|████████████████         | 9/14 [04:41<03:56, 47.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 nodes\n",
      "56 embed len\n",
      "46 nodes\n",
      "46 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "97 nodes\n",
      "97 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  71%|█████████████████▏      | 10/14 [04:56<02:29, 37.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 nodes\n",
      "76 embed len\n",
      "56 nodes\n",
      "56 embed len\n",
      "31 nodes\n",
      "31 embed len\n",
      "48 nodes\n",
      "48 embed len\n",
      "53 nodes\n",
      "53 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  79%|██████████████████▊     | 11/14 [05:04<01:24, 28.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 nodes\n",
      "49 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "77 nodes\n",
      "77 embed len\n",
      "46 nodes\n",
      "46 embed len\n",
      "80 nodes\n",
      "80 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  86%|████████████████████▌   | 12/14 [05:19<00:48, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 nodes\n",
      "52 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "71 nodes\n",
      "71 embed len\n",
      "49 nodes\n",
      "49 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  93%|██████████████████████▎ | 13/14 [05:28<00:19, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 nodes\n",
      "53 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 14/14 [05:31<00:00, 23.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! MASM span: 2\n",
      "73\n",
      "73\n",
      "performing TDA on  cleaned_DEI  span:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 nodes\n",
      "15 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   7%|█▋                       | 1/15 [00:00<00:07,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 nodes\n",
      "13 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "20 nodes\n",
      "20 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  13%|███▎                     | 2/15 [00:01<00:06,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 nodes\n",
      "22 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  20%|█████                    | 3/15 [00:01<00:05,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 nodes\n",
      "12 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▋                  | 4/15 [00:01<00:05,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 nodes\n",
      "14 embed len\n",
      "22 nodes\n",
      "22 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "9 nodes\n",
      "11 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  33%|████████▎                | 5/15 [00:02<00:05,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 nodes\n",
      "6 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "13 nodes\n",
      "13 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  40%|██████████               | 6/15 [00:02<00:04,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "10 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  47%|███████████▋             | 7/15 [00:03<00:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 nodes\n",
      "13 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "22 nodes\n",
      "22 embed len\n",
      "15 nodes\n",
      "15 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  53%|█████████████▎           | 8/15 [00:04<00:03,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "6 nodes\n",
      "6 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  60%|███████████████          | 9/15 [00:04<00:02,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 nodes\n",
      "17 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "24 nodes\n",
      "24 embed len\n",
      "19 nodes\n",
      "19 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  67%|████████████████        | 10/15 [00:05<00:02,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 nodes\n",
      "5 embed len\n",
      "18 nodes\n",
      "19 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "16 nodes\n",
      "16 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|█████████████████▌      | 11/15 [00:05<00:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 nodes\n",
      "12 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "15 nodes\n",
      "15 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  80%|███████████████████▏    | 12/15 [00:05<00:01,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 nodes\n",
      "18 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "6 nodes\n",
      "7 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "11 nodes\n",
      "11 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  87%|████████████████████▊   | 13/15 [00:06<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 nodes\n",
      "9 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "21 nodes\n",
      "21 embed len\n",
      "14 nodes\n",
      "14 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  93%|██████████████████████▍ | 14/15 [00:06<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 nodes\n",
      "12 embed len\n",
      "12 nodes\n",
      "12 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 15/15 [00:07<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! cleaned_DEI span: 2\n",
      "130\n",
      "130\n",
      "performing TDA on  PEM_df  span:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 nodes\n",
      "80 embed len\n",
      "75 nodes\n",
      "75 embed len\n",
      "69 nodes\n",
      "69 embed len\n",
      "66 nodes\n",
      "66 embed len\n",
      "50 nodes\n",
      "50 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   4%|▉                        | 1/26 [00:22<09:33, 22.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 nodes\n",
      "56 embed len\n",
      "89 nodes\n",
      "89 embed len\n",
      "69 nodes\n",
      "69 embed len\n",
      "64 nodes\n",
      "64 embed len\n",
      "53 nodes\n",
      "53 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   8%|█▉                       | 2/26 [01:06<14:03, 35.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 nodes\n",
      "66 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "97 nodes\n",
      "97 embed len\n",
      "74 nodes\n",
      "74 embed len\n",
      "75 nodes\n",
      "75 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  12%|██▉                      | 3/26 [02:18<19:50, 51.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 nodes\n",
      "76 embed len\n",
      "41 nodes\n",
      "41 embed len\n",
      "88 nodes\n",
      "88 embed len\n",
      "87 nodes\n",
      "87 embed len\n",
      "54 nodes\n",
      "54 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  15%|███▊                     | 4/26 [03:22<20:51, 56.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 nodes\n",
      "102 embed len\n",
      "91 nodes\n",
      "91 embed len\n",
      "109 nodes\n",
      "109 embed len\n",
      "84 nodes\n",
      "84 embed len\n",
      "66 nodes\n",
      "66 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  19%|████▊                    | 5/26 [05:33<29:11, 83.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 nodes\n",
      "63 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "62 nodes\n",
      "62 embed len\n",
      "78 nodes\n",
      "78 embed len\n",
      "69 nodes\n",
      "69 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  23%|█████▊                   | 6/26 [06:07<22:13, 66.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 nodes\n",
      "67 embed len\n",
      "121 nodes\n",
      "121 embed len\n",
      "62 nodes\n",
      "62 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "86 nodes\n",
      "86 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▋                  | 7/26 [07:49<24:47, 78.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 nodes\n",
      "81 embed len\n",
      "67 nodes\n",
      "67 embed len\n",
      "96 nodes\n",
      "96 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "57 nodes\n",
      "57 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  31%|███████▋                 | 8/26 [08:35<20:21, 67.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 nodes\n",
      "88 embed len\n",
      "99 nodes\n",
      "99 embed len\n",
      "75 nodes\n",
      "75 embed len\n",
      "77 nodes\n",
      "77 embed len\n",
      "31 nodes\n",
      "31 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  35%|████████▋                | 9/26 [09:27<17:47, 62.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 nodes\n",
      "96 embed len\n",
      "68 nodes\n",
      "68 embed len\n",
      "126 nodes\n",
      "126 embed len\n",
      "110 nodes\n",
      "110 embed len\n",
      "55 nodes\n",
      "55 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  38%|████████▊              | 10/26 [13:45<32:53, 123.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 nodes\n",
      "138 embed len\n",
      "83 nodes\n",
      "83 embed len\n",
      "82 nodes\n",
      "82 embed len\n",
      "126 nodes\n",
      "126 embed len\n",
      "61 nodes\n",
      "61 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  42%|████████▉            | 11/26 [22:38<1:02:09, 248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 nodes\n",
      "54 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "97 nodes\n",
      "97 embed len\n",
      "60 nodes\n",
      "60 embed len\n",
      "100 nodes\n",
      "100 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  46%|██████████▌            | 12/26 [23:43<44:59, 192.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 nodes\n",
      "54 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "61 nodes\n",
      "61 embed len\n",
      "73 nodes\n",
      "73 embed len\n",
      "54 nodes\n",
      "54 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|███████████▌           | 13/26 [24:22<31:38, 146.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 nodes\n",
      "57 embed len\n",
      "42 nodes\n",
      "42 embed len\n",
      "81 nodes\n",
      "81 embed len\n",
      "47 nodes\n",
      "47 embed len\n",
      "91 nodes\n",
      "91 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  54%|████████████▍          | 14/26 [25:24<24:07, 120.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 nodes\n",
      "65 embed len\n",
      "66 nodes\n",
      "66 embed len\n",
      "97 nodes\n",
      "97 embed len\n",
      "63 nodes\n",
      "63 embed len\n",
      "58 nodes\n",
      "58 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  58%|█████████████▎         | 15/26 [26:52<20:21, 111.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 nodes\n",
      "124 embed len\n",
      "74 nodes\n",
      "74 embed len\n",
      "88 nodes\n",
      "88 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "92 nodes\n",
      "92 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  62%|██████████████▏        | 16/26 [29:35<21:04, 126.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 nodes\n",
      "70 embed len\n",
      "98 nodes\n",
      "98 embed len\n",
      "106 nodes\n",
      "106 embed len\n",
      "81 nodes\n",
      "81 embed len\n",
      "86 nodes\n",
      "86 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  65%|███████████████        | 17/26 [31:27<18:19, 122.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 nodes\n",
      "71 embed len\n",
      "90 nodes\n",
      "90 embed len\n",
      "69 nodes\n",
      "69 embed len\n",
      "86 nodes\n",
      "86 embed len\n",
      "85 nodes\n",
      "85 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  69%|███████████████▉       | 18/26 [33:02<15:12, 114.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 nodes\n",
      "53 embed len\n",
      "64 nodes\n",
      "64 embed len\n",
      "60 nodes\n",
      "60 embed len\n",
      "82 nodes\n",
      "82 embed len\n",
      "86 nodes\n",
      "86 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|█████████████████▌      | 19/26 [33:35<10:26, 89.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 nodes\n",
      "41 embed len\n",
      "67 nodes\n",
      "67 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "77 nodes\n",
      "77 embed len\n",
      "85 nodes\n",
      "85 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  77%|██████████████████▍     | 20/26 [34:35<08:04, 80.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 nodes\n",
      "65 embed len\n",
      "82 nodes\n",
      "82 embed len\n",
      "61 nodes\n",
      "61 embed len\n",
      "77 nodes\n",
      "77 embed len\n",
      "60 nodes\n",
      "60 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  81%|███████████████████▍    | 21/26 [35:05<05:27, 65.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 nodes\n",
      "87 embed len\n",
      "65 nodes\n",
      "65 embed len\n",
      "105 nodes\n",
      "105 embed len\n",
      "91 nodes\n",
      "91 embed len\n",
      "69 nodes\n",
      "69 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  85%|████████████████████▎   | 22/26 [36:25<04:39, 69.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 nodes\n",
      "64 embed len\n",
      "85 nodes\n",
      "85 embed len\n",
      "62 nodes\n",
      "62 embed len\n",
      "63 nodes\n",
      "63 embed len\n",
      "93 nodes\n",
      "93 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  88%|█████████████████████▏  | 23/26 [37:51<03:44, 74.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 nodes\n",
      "36 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "91 nodes\n",
      "91 embed len\n",
      "88 nodes\n",
      "88 embed len\n",
      "117 nodes\n",
      "117 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  92%|██████████████████████▏ | 24/26 [39:02<02:27, 73.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 nodes\n",
      "53 embed len\n",
      "80 nodes\n",
      "80 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "85 nodes\n",
      "85 embed len\n",
      "88 nodes\n",
      "88 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  96%|███████████████████████ | 25/26 [39:30<00:59, 59.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 nodes\n",
      "78 embed len\n",
      "85 nodes\n",
      "85 embed len\n",
      "109 nodes\n",
      "109 embed len\n",
      "70 nodes\n",
      "70 embed len\n",
      "57 nodes\n",
      "57 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 26/26 [40:36<00:00, 93.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! PEM_df span: 3\n",
      "101\n",
      "101\n",
      "performing TDA on  SER_monologs  span:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 nodes\n",
      "16 embed len\n",
      "32 nodes\n",
      "32 embed len\n",
      "23 nodes\n",
      "23 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "35 nodes\n",
      "35 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   5%|█▎                       | 1/20 [00:01<00:28,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 nodes\n",
      "34 embed len\n",
      "30 nodes\n",
      "30 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "31 nodes\n",
      "31 embed len\n",
      "52 nodes\n",
      "52 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  10%|██▌                      | 2/20 [00:05<00:49,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 nodes\n",
      "52 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "33 nodes\n",
      "33 embed len\n",
      "29 nodes\n",
      "29 embed len\n",
      "31 nodes\n",
      "31 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  15%|███▊                     | 3/20 [00:08<00:54,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 nodes\n",
      "47 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "48 nodes\n",
      "48 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  20%|█████                    | 4/20 [00:14<01:05,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 nodes\n",
      "57 embed len\n",
      "55 nodes\n",
      "55 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "55 nodes\n",
      "55 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  25%|██████▎                  | 5/20 [00:25<01:38,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 nodes\n",
      "68 embed len\n",
      "66 nodes\n",
      "66 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "42 nodes\n",
      "42 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  30%|███████▌                 | 6/20 [00:50<03:01, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 nodes\n",
      "36 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "41 nodes\n",
      "41 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  35%|████████▊                | 7/20 [00:54<02:09,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 nodes\n",
      "58 embed len\n",
      "48 nodes\n",
      "48 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "42 nodes\n",
      "42 embed len\n",
      "48 nodes\n",
      "48 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  40%|██████████               | 8/20 [01:07<02:09, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 nodes\n",
      "45 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "107 nodes\n",
      "107 embed len\n",
      "48 nodes\n",
      "48 embed len\n",
      "55 nodes\n",
      "55 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  45%|███████████▎             | 9/20 [01:45<03:32, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 nodes\n",
      "64 embed len\n",
      "30 nodes\n",
      "30 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "28 nodes\n",
      "28 embed len\n",
      "22 nodes\n",
      "22 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████            | 10/20 [01:52<02:35, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 nodes\n",
      "39 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "51 nodes\n",
      "51 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  55%|█████████████▏          | 11/20 [01:57<01:51, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 nodes\n",
      "40 embed len\n",
      "68 nodes\n",
      "68 embed len\n",
      "68 nodes\n",
      "68 embed len\n",
      "99 nodes\n",
      "99 embed len\n",
      "13 nodes\n",
      "13 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  60%|██████████████▍         | 12/20 [02:36<02:45, 20.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 nodes\n",
      "31 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "8 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  65%|███████████████▌        | 13/20 [02:39<01:46, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 nodes\n",
      "13 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "22 nodes\n",
      "22 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "19 nodes\n",
      "19 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  70%|████████████████▊       | 14/20 [02:40<01:05, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 nodes\n",
      "24 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "29 nodes\n",
      "29 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  75%|██████████████████      | 15/20 [02:42<00:41,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 nodes\n",
      "26 embed len\n",
      "35 nodes\n",
      "35 embed len\n",
      "33 nodes\n",
      "33 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "38 nodes\n",
      "38 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  80%|███████████████████▏    | 16/20 [02:50<00:32,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 nodes\n",
      "35 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "28 nodes\n",
      "28 embed len\n",
      "25 nodes\n",
      "25 embed len\n",
      "21 nodes\n",
      "21 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  85%|████████████████████▍   | 17/20 [02:53<00:20,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 nodes\n",
      "33 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "14 nodes\n",
      "14 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "22 nodes\n",
      "22 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  90%|█████████████████████▌  | 18/20 [02:55<00:10,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 nodes\n",
      "61 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "23 nodes\n",
      "23 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "24 nodes\n",
      "24 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▊ | 19/20 [02:59<00:04,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 nodes\n",
      "27 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "16 nodes\n",
      "16 embed len\n",
      "40 nodes\n",
      "40 embed len\n",
      "38 nodes\n",
      "38 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 20/20 [03:03<00:00,  9.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER_monologs span: 3\n",
      "107\n",
      "107\n",
      "performing TDA on  SER_IPSP  span:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 nodes\n",
      "35 embed len\n",
      "57 nodes\n",
      "57 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "35 nodes\n",
      "35 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   5%|█▏                       | 1/22 [00:08<02:52,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 nodes\n",
      "38 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "60 nodes\n",
      "60 embed len\n",
      "57 nodes\n",
      "57 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   9%|██▎                      | 2/22 [00:28<05:03, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 nodes\n",
      "44 embed len\n",
      "80 nodes\n",
      "80 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "53 nodes\n",
      "53 embed len\n",
      "45 nodes\n",
      "45 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▍                     | 3/22 [00:45<05:06, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 nodes\n",
      "51 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "46 nodes\n",
      "46 embed len\n",
      "81 nodes\n",
      "81 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  18%|████▌                    | 4/22 [01:06<05:23, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 nodes\n",
      "59 embed len\n",
      "43 nodes\n",
      "43 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "42 nodes\n",
      "42 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  23%|█████▋                   | 5/22 [01:16<04:17, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 nodes\n",
      "65 embed len\n",
      "64 nodes\n",
      "64 embed len\n",
      "69 nodes\n",
      "69 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "64 nodes\n",
      "64 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  27%|██████▊                  | 6/22 [01:42<04:59, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 nodes\n",
      "45 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "32 nodes\n",
      "32 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  32%|███████▉                 | 7/22 [01:46<03:32, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 nodes\n",
      "58 embed len\n",
      "32 nodes\n",
      "32 embed len\n",
      "24 nodes\n",
      "24 embed len\n",
      "50 nodes\n",
      "50 embed len\n",
      "51 nodes\n",
      "51 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  36%|█████████                | 8/22 [01:56<02:55, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 nodes\n",
      "41 embed len\n",
      "57 nodes\n",
      "57 embed len\n",
      "45 nodes\n",
      "45 embed len\n",
      "73 nodes\n",
      "73 embed len\n",
      "56 nodes\n",
      "56 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  41%|██████████▏              | 9/22 [02:20<03:32, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 nodes\n",
      "74 embed len\n",
      "77 nodes\n",
      "77 embed len\n",
      "81 nodes\n",
      "81 embed len\n",
      "59 nodes\n",
      "59 embed len\n",
      "55 nodes\n",
      "55 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  45%|██████████▉             | 10/22 [03:00<04:43, 23.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 nodes\n",
      "42 embed len\n",
      "34 nodes\n",
      "34 embed len\n",
      "49 nodes\n",
      "49 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "37 nodes\n",
      "37 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████            | 11/22 [03:04<03:13, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 nodes\n",
      "18 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "37 nodes\n",
      "37 embed len\n",
      "44 nodes\n",
      "44 embed len\n",
      "37 nodes\n",
      "37 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  55%|█████████████           | 12/22 [03:13<02:29, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 nodes\n",
      "56 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "61 nodes\n",
      "61 embed len\n",
      "56 nodes\n",
      "56 embed len\n",
      "78 nodes\n",
      "78 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  59%|██████████████▏         | 13/22 [03:42<02:53, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 nodes\n",
      "98 embed len\n",
      "13 nodes\n",
      "14 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "56 nodes\n",
      "56 embed len\n",
      "33 nodes\n",
      "33 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  64%|███████████████▎        | 14/22 [04:40<04:06, 30.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 nodes\n",
      "48 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "23 nodes\n",
      "23 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "17 nodes\n",
      "17 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  68%|████████████████▎       | 15/22 [04:42<02:35, 22.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 nodes\n",
      "76 embed len\n",
      "97 nodes\n",
      "97 embed len\n",
      "82 nodes\n",
      "82 embed len\n",
      "24 nodes\n",
      "24 embed len\n",
      "23 nodes\n",
      "23 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  73%|█████████████████▍      | 16/22 [06:11<04:13, 42.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 nodes\n",
      "21 embed len\n",
      "58 nodes\n",
      "58 embed len\n",
      "58 nodes\n",
      "58 embed len\n",
      "48 nodes\n",
      "48 embed len\n",
      "72 nodes\n",
      "72 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  77%|██████████████████▌     | 17/22 [06:28<02:54, 34.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 nodes\n",
      "76 embed len\n",
      "83 nodes\n",
      "83 embed len\n",
      "48 nodes\n",
      "48 embed len\n",
      "61 nodes\n",
      "61 embed len\n",
      "48 nodes\n",
      "48 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  82%|███████████████████▋    | 18/22 [06:58<02:13, 33.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 nodes\n",
      "23 embed len\n",
      "21 nodes\n",
      "21 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "57 nodes\n",
      "57 embed len\n",
      "66 nodes\n",
      "66 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  86%|████████████████████▋   | 19/22 [07:12<01:22, 27.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 nodes\n",
      "33 embed len\n",
      "26 nodes\n",
      "26 embed len\n",
      "28 nodes\n",
      "28 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "72 nodes\n",
      "72 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  91%|█████████████████████▊  | 20/22 [07:35<00:52, 26.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 nodes\n",
      "67 embed len\n",
      "28 nodes\n",
      "28 embed len\n",
      "42 nodes\n",
      "42 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "44 nodes\n",
      "44 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▉ | 21/22 [07:51<00:22, 22.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 nodes\n",
      "47 embed len\n",
      "62 nodes\n",
      "62 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 22/22 [07:57<00:00, 21.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER_IPSP span: 3\n",
      "108\n",
      "108\n",
      "performing TDA on  SER1  span:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "6 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "13 nodes\n",
      "13 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   5%|█▏                       | 1/21 [00:00<00:10,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nodes\n",
      "10 embed len\n",
      "8 nodes\n",
      "9 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  10%|██▍                      | 2/21 [00:00<00:09,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▌                     | 3/21 [00:01<00:08,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "3 nodes\n",
      "3 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  19%|████▊                    | 4/21 [00:01<00:06,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 nodes\n",
      "4 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  24%|█████▉                   | 5/21 [00:02<00:06,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "3 nodes\n",
      "3 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  29%|███████▏                 | 6/21 [00:02<00:05,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  33%|████████▎                | 7/21 [00:02<00:05,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "6 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "11 nodes\n",
      "11 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  38%|█████████▌               | 8/21 [00:03<00:05,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "18 nodes\n",
      "18 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  43%|██████████▋              | 9/21 [00:03<00:05,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 nodes\n",
      "21 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  48%|███████████▍            | 10/21 [00:04<00:05,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 nodes\n",
      "9 embed len\n",
      "15 nodes\n",
      "15 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "10 nodes\n",
      "10 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  52%|████████████▌           | 11/21 [00:05<00:05,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 nodes\n",
      "5 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "6 nodes\n",
      "6 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  57%|█████████████▋          | 12/21 [00:05<00:04,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "17 nodes\n",
      "17 embed len\n",
      "9 nodes\n",
      "9 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "11 nodes\n",
      "11 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  62%|██████████████▊         | 13/21 [00:06<00:04,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  67%|████████████████        | 14/21 [00:06<00:03,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 nodes\n",
      "6 embed len\n",
      "13 nodes\n",
      "13 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "9 nodes\n",
      "9 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  71%|█████████████████▏      | 15/21 [00:07<00:02,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 nodes\n",
      "4 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "3 nodes\n",
      "3 embed len\n",
      "6 nodes\n",
      "6 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  76%|██████████████████▎     | 16/21 [00:07<00:02,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 nodes\n",
      "3 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "3 nodes\n",
      "3 embed len\n",
      "4 nodes\n",
      "4 embed len\n",
      "7 nodes\n",
      "7 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  81%|███████████████████▍    | 17/21 [00:07<00:01,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  86%|████████████████████▌   | 18/21 [00:08<00:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "5 nodes\n",
      "5 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  90%|█████████████████████▋  | 19/21 [00:08<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n",
      "6 nodes\n",
      "6 embed len\n",
      "7 nodes\n",
      "7 embed len\n",
      "8 nodes\n",
      "8 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  95%|██████████████████████▊ | 20/21 [00:09<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 nodes\n",
      "8 embed len\n",
      "5 nodes\n",
      "5 embed len\n",
      "10 nodes\n",
      "10 embed len\n",
      "11 nodes\n",
      "11 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|████████████████████████| 21/21 [00:09<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed! SER1 span: 3\n",
      "66\n",
      "66\n",
      "performing TDA on  MASM  span:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   0%|                                 | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 nodes\n",
      "54 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "70 nodes\n",
      "70 embed len\n",
      "52 nodes\n",
      "52 embed len\n",
      "36 nodes\n",
      "36 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:   7%|█▊                       | 1/14 [00:09<01:59,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 nodes\n",
      "65 embed len\n",
      "55 nodes\n",
      "55 embed len\n",
      "36 nodes\n",
      "36 embed len\n",
      "56 nodes\n",
      "56 embed len\n",
      "47 nodes\n",
      "47 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  14%|███▌                     | 2/14 [00:14<01:20,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 nodes\n",
      "42 embed len\n",
      "39 nodes\n",
      "39 embed len\n",
      "81 nodes\n",
      "81 embed len\n",
      "89 nodes\n",
      "89 embed len\n",
      "88 nodes\n",
      "88 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  21%|█████▎                   | 3/14 [00:48<03:30, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 nodes\n",
      "40 embed len\n",
      "54 nodes\n",
      "54 embed len\n",
      "60 nodes\n",
      "60 embed len\n",
      "74 nodes\n",
      "74 embed len\n",
      "52 nodes\n",
      "52 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  29%|███████▏                 | 4/14 [01:01<02:49, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 nodes\n",
      "68 embed len\n",
      "70 nodes\n",
      "70 embed len\n",
      "70 nodes\n",
      "70 embed len\n",
      "83 nodes\n",
      "83 embed len\n",
      "72 nodes\n",
      "72 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  36%|████████▉                | 5/14 [01:24<02:50, 18.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 nodes\n",
      "50 embed len\n",
      "27 nodes\n",
      "27 embed len\n",
      "19 nodes\n",
      "19 embed len\n",
      "38 nodes\n",
      "38 embed len\n",
      "35 nodes\n",
      "36 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  43%|██████████▋              | 6/14 [01:28<01:50, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 nodes\n",
      "21 embed len\n",
      "51 nodes\n",
      "51 embed len\n",
      "57 nodes\n",
      "57 embed len\n",
      "24 nodes\n",
      "24 embed len\n",
      "50 nodes\n",
      "50 embed len\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Chunks:  50%|████████████▌            | 7/14 [01:32<01:14, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 nodes\n",
      "28 embed len\n",
      "71 nodes\n",
      "71 embed len\n",
      "53 nodes\n",
      "53 embed len\n",
      "153 nodes\n",
      "153 embed len\n",
      "143 nodes\n",
      "143 embed len\n"
     ]
    }
   ],
   "source": [
    "# 3) Define a helper function to transform a single row's embeddings\n",
    "dir_array='/home/ll16598/Documents/POSTDOC/TDA/TDA_cluster/utt_span_vectors'\n",
    "pooling_method='mean'\n",
    "if pooling_method=='max':\n",
    "    ML=5\n",
    "else:\n",
    "    ML=3\n",
    "embeddings='sentence_embeddings'\n",
    "reduce_dims=True\n",
    "SPARSE=True\n",
    "sparse_param=0.5\n",
    "dims_simplex=3\n",
    "chunk_size=5\n",
    "df_monologs=pd.read_csv(f'{dir_atom_dfs}/df_monolog_{threshold}.csv')\n",
    "df_SER2=pd.read_csv(f'{dir_atom_dfs}/df_SER2_{threshold}.csv')\n",
    "df_PEM=pd.read_csv(f'{dir_atom_dfs}/df_PEM.csv')\n",
    "df_SER_MA=pd.read_csv(f'{dir_atom_dfs}/SER1.csv')\n",
    "preprocessed_df_dir='/home/ll16598/Documents/POSTDOC/preprocessed_dfs'\n",
    "df_MASM=pd.read_csv(preprocessed_df_dir + '/MASM.csv')\n",
    "df_DEI=pd.read_csv(preprocessed_df_dir + '/cleaned_DEI.csv')\n",
    "\n",
    "print(f'using {pooling_method} pooling')\n",
    "import gc\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "df_names=['PEM_df','SER_monologs', 'SER_IPSP', 'SER1', 'MASM', 'cleaned_DEI']\n",
    "dfs1=[df_PEM, df_monologs, df_SER2, df_SER_MA, df_MASM, df_DEI]\n",
    "if len(df_names)!=len(dfs1):\n",
    "    raise Exception('MISMATCH IN NAMES/dfs') \n",
    "data_save_dir=working_dir+'TDA_output/'\n",
    "os.makedirs(data_save_dir, exist_ok=True)\n",
    "\n",
    "completed_files=os.listdir(data_save_dir)\n",
    "for span in [1, 2,3]:\n",
    "    layers='last'\n",
    "    dfs=dfs1.copy()\n",
    "    for df_no, df_monolog in enumerate(dfs):\n",
    "\n",
    "        df_name=df_names[df_no]\n",
    "#             if newfilename in completed_files:\n",
    "#                 print(f'Already completed {newfilename}')\n",
    "#                 continue\n",
    "        data_save_dir_name=working_dir+f'TDA_output/{df_name}_{span}_{pooling_method}/'\n",
    "        if os.path.exists(data_save_dir_name):\n",
    "            shutil.rmtree(data_save_dir_name)  # Deletes the entire directory and its contents\n",
    "        os.makedirs(data_save_dir_name, exist_ok=True)\n",
    "\n",
    "        with open(f'{dir_array}/utterance_{span}_{df_name}_{pooling_method}_back_sentence_embeddings_arrays.pkl', 'rb') as f:\n",
    "            embeds= pickle.load(f)\n",
    "            if pooling_method=='max':\n",
    "                embeds=[np.array(i) for i in embeds]\n",
    "        newfilename=f'{df_name}_utterance_distance_results.csv'\n",
    "\n",
    "\n",
    "        if len(embeds)!=len(df_monolog):\n",
    "            raise Exception('MISMATCH IN LENGTH')\n",
    "\n",
    "        df_monolog['sentence_embeddings'] = embeds\n",
    "\n",
    "\n",
    "        print(len(embeds))\n",
    "        print(len(df_monolog))\n",
    "        if len(embeds)!=len(df_monolog):\n",
    "            raise Exception('MISMATCH IN LENGTH')\n",
    "\n",
    "        df_monolog['sentence_embeddings'] = embeds\n",
    "        df_monolog['length'] = [len(i) for i in embeds]\n",
    "        if test_mode:\n",
    "            df_monolog=df_monolog[0:10]\n",
    "        df_monolog = df_monolog[\n",
    "            df_monolog[\"sentence_embeddings\"].apply(\n",
    "                lambda x: (\n",
    "                    not isinstance(x, float)               # exclude floats\n",
    "                    and isinstance(x, (list, tuple, np.ndarray))  # must be list/tuple/np.ndarray\n",
    "                    and len(x) >= 3                        # length >= 3\n",
    "                )\n",
    "            )]\n",
    "\n",
    "        # 4) Apply that function to each row -> produce a new column\n",
    "        if reduce_dims:\n",
    "            all_vecs = []\n",
    "            for row in df_monolog['sentence_embeddings']:\n",
    "                arr = np.array(row)  \n",
    "                all_vecs.append(arr)\n",
    "            big_matrix = np.concatenate(all_vecs, axis=0)\n",
    "            pca = PCA(n_components=50)\n",
    "            pca.fit(big_matrix)\n",
    "            def transform_embeddings(emb_list):\n",
    "                emb_array = np.array(emb_list)   # shape (k_i, 384)\n",
    "                emb_pca = pca.transform(emb_array)  # shape (k_i, 50)\n",
    "                return emb_pca\n",
    "            df_monolog['sentence_embeddings'] = df_monolog['sentence_embeddings'].apply(transform_embeddings)\n",
    "        df_monolog['token_embeddings']=None\n",
    "        print('performing TDA on ',df_name, ' span: ', span)#, 'step: ', step)\n",
    "\n",
    "        for fi in tqdm(range(0, len(df_monolog), chunk_size), desc=\"Processing Chunks\"):\n",
    "            if fi+chunk_size>=len(df_monolog):\n",
    "                df_subset=df_monolog[fi:].reset_index(drop=True)\n",
    "            else:\n",
    "                df_subset=df_monolog[fi:fi+chunk_size].reset_index(drop=True)\n",
    "\n",
    "\n",
    "           # drugs=list(set(df_subset['Drug']))\n",
    "            #Participants=list(set(df_subset['Participant']))\n",
    "            df_subset=get_rips_time(df_subset,embeddings=embeddings)\n",
    "           # df_subset=get_rips_time_centroid(df_subset,embeddings=embeddings)\n",
    "            #df_monolog=get_simplices_over_time(df_monolog,simplex_tree_type='simplex_tree')\n",
    "            #print('got RIPS')\n",
    "            df_with_graph=get_rips_complex_G(df_subset)\n",
    "            #print('got G')\n",
    "            #df_with_graph['euler'] = df_with_graph['rt_simplex_tree'].apply(lambda st: compute_euler_characteristic(st, max_dim=4))\n",
    "            # Apply the function to each graph in df_with_graph\n",
    "            graph_metrics = df_with_graph['graph'].apply(compute_graph_metrics)\n",
    "            graph_metrics_df = pd.DataFrame(graph_metrics.tolist())\n",
    "            df_with_graph = pd.concat([df_with_graph, graph_metrics_df], axis=1)\n",
    "                        # Create a new DataFrame\n",
    "\n",
    "            # We'll accumulate new rows in a list of dicts\n",
    "            new_rows = []\n",
    "            dimensions = [0, 1, 2]\n",
    "\n",
    "            for idx, row in df_with_graph.iterrows():\n",
    "                embed = row[embeddings]  # Adjust as needed\n",
    "                # We’ll store births, deaths, pers LENGTHS in a dict keyed by dimension\n",
    "                dim_dict = {\n",
    "                    dim: {'births': [], 'deaths': [], 'pers': []}\n",
    "                    for dim in dimensions\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "                # Build the Rips Complex for *this row only*\n",
    "                rips_complex =row['rips']\n",
    "                try:\n",
    "                    simplex_tree = \\\n",
    "                    rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                persistence = simplex_tree.persistence()\n",
    "\n",
    "                # Collect intervals by dimension\n",
    "                for dim, (b, d) in persistence:\n",
    "                    if d == float('inf'):\n",
    "                        continue\n",
    "                    if dim in dimensions:\n",
    "                        dim_dict[dim]['births'].append(b)\n",
    "                        dim_dict[dim]['deaths'].append(d)\n",
    "                        dim_dict[dim]['pers'].append(d - b)\n",
    "\n",
    "\n",
    "                row_dict = row.to_dict()  # Start with original row's columns\n",
    "\n",
    "                for dim in dimensions:\n",
    "                    bdp = dim_dict[dim]\n",
    "                    stats_dict = compute_distribution_stats(bdp['births'], bdp['deaths'], bdp['pers'])\n",
    "                    # prefix each stat key with dim\n",
    "                    for stat_key, stat_val in stats_dict.items():\n",
    "                        row_dict[f\"{stat_key}_dim{dim}\"] = stat_val\n",
    "\n",
    "                # Add row_dict to new_rows\n",
    "                new_rows.append(row_dict)\n",
    "\n",
    "            # Create a new DataFrame\n",
    "            df_with_tda = pd.DataFrame(new_rows)\n",
    "            df_with_tda.to_csv(data_save_dir_name + f'{df_name}_{fi}_{span}_TDA_results.csv')\n",
    "            del df_subset\n",
    "            del df_with_graph\n",
    "            gc.collect()\n",
    "        print(f'completed! {df_name} span: {span}')\n",
    "        list_files=sorted(os.listdir(data_save_dir_name))\n",
    "        ddfs=[]\n",
    "        for f in list_files:\n",
    "            ddfs.append(pd.read_csv(data_save_dir_name+f))\n",
    "        data=pd.concat(ddfs)\n",
    "        data.to_csv(data_save_dir+f'{df_name}_{span}_{pooling_method}_utterance_TDA_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4303ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for SP in [1,2,3]:\n",
    "    for name in ['PEM_df','SER_IPSP','SER_monologs','MASM', 'cleaned_DEI']:\n",
    "        data_save_dir=working_dir+'TDA_output/'\n",
    "        data_save_dir_name=working_dir+f'TDA_output/{name}_{SP}_mean/'\n",
    "        list_files=sorted(os.listdir(data_save_dir_name))\n",
    "        dfs=[]\n",
    "        for f in list_files:\n",
    "            dfs.append(pd.read_csv(data_save_dir_name+f))\n",
    "        data=pd.concat(dfs)\n",
    "        data.to_csv(data_save_dir+f'{name}_{SP}_back_utterance_TDA_results.csv')\n",
    "        print(name, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9913cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MODE\n"
     ]
    }
   ],
   "source": [
    "test_mode=False\n",
    "plot=False\n",
    "if test_mode:\n",
    "    save=False\n",
    "else:\n",
    "    save=True\n",
    "print('TEST MODE')\n",
    "\n",
    "threshold=244\n",
    "# infile = open(f'/home/ll16598/Documents/POSTDOC/Context-DATM/sentenceBERT_cluster_dicts_{window}_{embedding_step}/cluster_dictionary_{save_thresh}','rb')\n",
    "# cluster_dictionary=pickle.load(infile)\n",
    "# infile.close()\n",
    "\n",
    "user='luke'\n",
    "if user=='luke':\n",
    "    working_dir='/home/ll16598/Documents/POSTDOC/'\n",
    "    dir_atom_dfs='/home/ll16598/Documents/POSTDOC/TDA/TDA_cluster/atom_assigned_dfs'\n",
    "    dir_array='/home/ll16598/Documents/POSTDOC/TDA/TDA_cluster/vector_assigned_dfs'\n",
    "elif user=='cluster':\n",
    "    working_dir='/N/u/lleckie/Quartz/TDA/'\n",
    "    dir_atom_dfs=working_dir+'/atom_assigned_dfs'\n",
    "\n",
    "    dir_array=working_dir+'vector_assigned_dfs'\n",
    "\n",
    "#df_drug=pd.read_csv(f'.{}/df_monolog_{threshold}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36998127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define a helper function to transform a single row's embeddings\n",
    "ML=3\n",
    "embeddings='sentence_embeddings'\n",
    "reduce_dims=True\n",
    "SPARSE=True\n",
    "sparse_param=0.5\n",
    "dims_simplex=3\n",
    "chunk_size=5\n",
    "df_monologs=pd.read_csv(f'{dir_atom_dfs}/df_monolog_{threshold}.csv')\n",
    "df_SER2=pd.read_csv(f'{dir_atom_dfs}/df_SER2_{threshold}.csv')\n",
    "df_PEM=pd.read_csv(f'{dir_atom_dfs}/df_PEM.csv')\n",
    "df_SER_MA=pd.read_csv(f'{dir_atom_dfs}/SER1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a66125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "130\n",
      "performing TDA on  PEM_df  window:  utterances\n",
      "rips mem 0 285.8203125\n",
      "simplex mem 346.5078125\n",
      "persistence mem 402.87109375\n",
      "rips max mem 402.87109375\n",
      "rips mem 1 402.87109375\n",
      "simplex mem 432.5625\n",
      "persistence mem 432.5625\n",
      "rips max mem 432.5625\n",
      "rips mem 2 432.5625\n",
      "simplex mem 432.5625\n",
      "persistence mem 432.5625\n",
      "rips max mem 432.5625\n",
      "rips mem 3 432.5625\n",
      "simplex mem 432.5625\n",
      "persistence mem 432.5625\n",
      "rips max mem 432.5625\n",
      "rips mem 4 432.5625\n",
      "simplex mem 432.5625\n",
      "persistence mem 432.5625\n",
      "rips max mem 432.5625\n",
      "got RIPS\n",
      "got G\n",
      "[0, 5, 7, 8, 18, 23, 28, 38, 39, 76, 43, 24, 37, 67, 55, 59, 66, 40, 58, 44, 77, 1, 2, 3, 4, 13, 27, 30, 31, 53, 60, 19, 61, 62, 63, 26, 68, 75, 33, 57, 29, 36, 71, 11, 12, 20, 21, 22, 64, 14, 10, 52, 25, 16, 46, 47, 81, 65, 17, 32, 35, 9, 56, 80, 42, 15, 69, 48, 70, 45, 54, 6, 78, 34, 72, 73, 74, 49, 51, 41, 79, 50]\n",
      "0    {'shortest_path_unweighted': 2, 'nodes': 82, '...\n",
      "1    {'shortest_path_unweighted': 2, 'nodes': 75, '...\n",
      "2    {'shortest_path_unweighted': 1, 'nodes': 71, '...\n",
      "3    {'shortest_path_unweighted': 2, 'nodes': 68, '...\n",
      "4    {'shortest_path_unweighted': 1, 'nodes': 52, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 440.87109375\n",
      "simplex mem 440.87109375\n",
      "persistence mem 440.87109375\n",
      "rips max mem 440.87109375\n",
      "rips mem 1 440.87109375\n",
      "simplex mem 440.87109375\n",
      "persistence mem 447.95703125\n",
      "rips max mem 447.95703125\n",
      "rips mem 2 447.95703125\n",
      "simplex mem 412.10546875\n",
      "persistence mem 411.97265625\n",
      "rips max mem 411.97265625\n",
      "rips mem 3 411.97265625\n",
      "simplex mem 411.97265625\n",
      "persistence mem 411.97265625\n",
      "rips max mem 411.97265625\n",
      "rips mem 4 411.97265625\n",
      "simplex mem 411.97265625\n",
      "persistence mem 411.97265625\n",
      "rips max mem 411.97265625\n",
      "got RIPS\n",
      "got G\n",
      "[0, 2, 3, 4, 11, 12, 14, 28, 32, 34, 35, 37, 38, 51, 52, 53, 54, 56, 5, 17, 25, 22, 57, 47, 39, 42, 27, 33, 15, 20, 18, 16, 48, 1, 13, 10, 8, 21, 29, 30, 55, 6, 7, 9, 23, 24, 19, 26, 41, 43, 31, 50, 36, 49, 40, 44, 45, 46]\n",
      "0    {'shortest_path_unweighted': 1, 'nodes': 58, '...\n",
      "1    {'shortest_path_unweighted': 1, 'nodes': 87, '...\n",
      "2    {'shortest_path_unweighted': 1, 'nodes': 70, '...\n",
      "3    {'shortest_path_unweighted': 1, 'nodes': 65, '...\n",
      "4    {'shortest_path_unweighted': 1, 'nodes': 55, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 451.4375\n",
      "simplex mem 451.171875\n",
      "persistence mem 451.16796875\n",
      "rips max mem 451.16796875\n",
      "rips mem 1 451.16796875\n",
      "simplex mem 451.16796875\n",
      "persistence mem 451.16796875\n",
      "rips max mem 451.16796875\n",
      "rips mem 2 451.16796875\n",
      "simplex mem 451.16796875\n",
      "persistence mem 499.71484375\n",
      "rips max mem 499.71484375\n",
      "rips mem 3 499.71484375\n",
      "simplex mem 465.62109375\n",
      "persistence mem 465.62109375\n",
      "rips max mem 465.62109375\n",
      "rips mem 4 465.62109375\n",
      "simplex mem 465.62109375\n",
      "persistence mem 465.62109375\n",
      "rips max mem 465.62109375\n",
      "got RIPS\n",
      "got G\n",
      "[0, 1, 2, 3, 10, 11, 16, 22, 23, 25, 39, 42, 15, 20, 57, 5, 18, 21, 40, 6, 43, 46, 48, 53, 63, 55, 4, 26, 27, 60, 66, 36, 47, 58, 8, 9, 41, 51, 52, 17, 28, 30, 35, 38, 44, 7, 49, 50, 54, 56, 29, 24, 61, 32, 12, 14, 62, 64, 65, 19, 31, 45, 33, 67, 13, 34, 59]\n",
      "0    {'shortest_path_unweighted': 2, 'nodes': 67, '...\n",
      "1    {'shortest_path_unweighted': 1, 'nodes': 51, '...\n",
      "2    {'shortest_path_unweighted': 2, 'nodes': 94, '...\n",
      "3    {'shortest_path_unweighted': 2, 'nodes': 76, '...\n",
      "4    {'shortest_path_unweighted': 2, 'nodes': 76, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 522.99609375\n",
      "simplex mem 522.99609375\n",
      "persistence mem 522.99609375\n",
      "rips max mem 522.99609375\n",
      "rips mem 1 522.99609375\n",
      "simplex mem 522.99609375\n",
      "persistence mem 522.99609375\n",
      "rips max mem 522.99609375\n",
      "rips mem 2 522.99609375\n",
      "simplex mem 522.99609375\n",
      "persistence mem 540.0078125\n",
      "rips max mem 540.0078125\n",
      "rips mem 3 540.0078125\n",
      "simplex mem 573.5234375\n",
      "persistence mem 573.5234375\n",
      "rips max mem 573.5234375\n",
      "rips mem 4 573.5234375\n",
      "simplex mem 573.5234375\n",
      "persistence mem 573.5234375\n",
      "rips max mem 573.5234375\n",
      "got RIPS\n",
      "got G\n",
      "[0, 1, 3, 5, 15, 16, 17, 21, 23, 24, 25, 27, 28, 30, 31, 33, 36, 40, 41, 42, 44, 45, 46, 47, 49, 50, 51, 52, 54, 62, 67, 72, 77, 32, 48, 57, 2, 7, 18, 19, 20, 4, 10, 43, 22, 6, 34, 61, 68, 29, 35, 39, 37, 26, 38, 55, 70, 63, 56, 8, 53, 73, 76, 75, 11, 59, 71, 69, 65, 9, 66, 64, 12, 14, 60, 74, 58, 13]\n",
      "0    {'shortest_path_unweighted': 1, 'nodes': 78, '...\n",
      "1    {'shortest_path_unweighted': 2, 'nodes': 42, '...\n",
      "2    {'shortest_path_unweighted': 2, 'nodes': 88, '...\n",
      "3    {'shortest_path_unweighted': 2, 'nodes': 88, '...\n",
      "4    {'shortest_path_unweighted': 1, 'nodes': 54, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 573.08203125\n",
      "simplex mem 573.08203125\n",
      "persistence mem 573.08203125\n",
      "rips max mem 573.08203125\n",
      "rips mem 1 573.08203125\n",
      "simplex mem 650.6796875\n",
      "persistence mem 650.6796875\n",
      "rips max mem 650.6796875\n",
      "rips mem 2 650.6796875\n",
      "simplex mem 654.8046875\n",
      "persistence mem 745.17578125\n",
      "rips max mem 745.17578125\n",
      "rips mem 3 745.17578125\n",
      "simplex mem 654.94140625\n",
      "persistence mem 654.94140625\n",
      "rips max mem 654.94140625\n",
      "rips mem 4 654.94140625\n",
      "simplex mem 654.94140625\n",
      "persistence mem 654.94140625\n",
      "rips max mem 654.94140625\n",
      "got RIPS\n",
      "got G\n",
      "[0, 1, 39, 57, 10, 17, 24, 34, 41, 48, 71, 75, 76, 77, 87, 100, 8, 18, 20, 43, 56, 65, 66, 80, 81, 83, 90, 38, 19, 86, 2, 3, 5, 9, 88, 96, 42, 73, 51, 72, 62, 69, 59, 60, 40, 101, 53, 27, 4, 7, 11, 12, 85, 29, 78, 91, 16, 13, 14, 64, 94, 99, 46, 92, 79, 58, 74, 26, 25, 63, 68, 98, 6, 31, 55, 67, 103, 33, 89, 95, 54, 84, 30, 32, 35, 15, 97, 23, 47, 37, 45, 61, 82, 21, 22, 28, 102, 44, 52, 50, 36, 49, 70]\n",
      "0    {'shortest_path_unweighted': 3, 'nodes': 103, ...\n",
      "1    {'shortest_path_unweighted': 1, 'nodes': 92, '...\n",
      "2    {'shortest_path_unweighted': 2, 'nodes': 110, ...\n",
      "3    {'shortest_path_unweighted': 1, 'nodes': 78, '...\n",
      "4    {'shortest_path_unweighted': 2, 'nodes': 67, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 656.734375\n",
      "simplex mem 656.734375\n",
      "persistence mem 656.734375\n",
      "rips max mem 656.734375\n",
      "rips mem 1 656.734375\n",
      "simplex mem 656.734375\n",
      "persistence mem 656.734375\n",
      "rips max mem 656.734375\n",
      "rips mem 2 656.734375\n",
      "simplex mem 656.734375\n",
      "persistence mem 656.734375\n",
      "rips max mem 656.734375\n",
      "rips mem 3 656.734375\n",
      "simplex mem 656.734375\n",
      "persistence mem 656.734375\n",
      "rips max mem 656.734375\n",
      "rips mem 4 656.734375\n",
      "simplex mem 656.734375\n",
      "persistence mem 656.734375\n",
      "rips max mem 656.734375\n",
      "got RIPS\n",
      "got G\n",
      "[0, 1, 2, 8, 9, 30, 42, 45, 47, 52, 3, 28, 46, 50, 64, 35, 7, 20, 23, 38, 39, 43, 56, 26, 44, 13, 24, 41, 5, 58, 14, 21, 32, 57, 10, 12, 49, 51, 54, 18, 4, 15, 17, 33, 60, 16, 19, 27, 37, 53, 55, 34, 11, 6, 22, 25, 36, 40, 48, 59, 63, 62]\n",
      "0    {'shortest_path_unweighted': 1, 'nodes': 62, '...\n",
      "1    {'shortest_path_unweighted': 2, 'nodes': 54, '...\n",
      "2    {'shortest_path_unweighted': 2, 'nodes': 63, '...\n",
      "3    {'shortest_path_unweighted': 1, 'nodes': 77, '...\n",
      "4    {'shortest_path_unweighted': 2, 'nodes': 69, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 654.73046875\n",
      "simplex mem 654.73046875\n",
      "persistence mem 654.73046875\n",
      "rips max mem 654.73046875\n",
      "rips mem 1 654.73046875\n",
      "simplex mem 654.73046875\n",
      "persistence mem 849.34765625\n",
      "rips max mem 849.34765625\n",
      "rips mem 2 849.34765625\n",
      "simplex mem 654.73046875\n",
      "persistence mem 654.73046875\n",
      "rips max mem 654.73046875\n",
      "rips mem 3 654.73046875\n",
      "simplex mem 654.73046875\n",
      "persistence mem 654.73046875\n",
      "rips max mem 654.73046875\n",
      "rips mem 4 654.73046875\n",
      "simplex mem 654.73046875\n",
      "persistence mem 654.73046875\n",
      "rips max mem 654.73046875\n",
      "got RIPS\n",
      "got G\n",
      "[0, 1, 2, 3, 5, 6, 8, 14, 15, 16, 17, 18, 22, 23, 29, 30, 36, 43, 49, 50, 51, 67, 13, 19, 24, 25, 55, 60, 62, 7, 37, 27, 40, 61, 57, 10, 11, 12, 45, 26, 31, 34, 41, 58, 54, 63, 39, 42, 9, 52, 53, 46, 68, 65, 28, 48, 47, 38, 66, 33, 20, 21, 4, 35, 44, 32]\n",
      "0    {'shortest_path_unweighted': 2, 'nodes': 66, '...\n",
      "1    {'shortest_path_unweighted': 1, 'nodes': 120, ...\n",
      "2    {'shortest_path_unweighted': 1, 'nodes': 63, '...\n",
      "3    {'shortest_path_unweighted': 1, 'nodes': 44, '...\n",
      "4    {'shortest_path_unweighted': 1, 'nodes': 85, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 654.609375\n",
      "simplex mem 654.609375\n",
      "persistence mem 654.609375\n",
      "rips max mem 654.609375\n",
      "rips mem 1 654.609375\n",
      "simplex mem 654.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistence mem 654.16015625\n",
      "rips max mem 654.16015625\n",
      "rips mem 2 654.16015625\n",
      "simplex mem 654.16015625\n",
      "persistence mem 654.14453125\n",
      "rips max mem 654.14453125\n",
      "rips mem 3 654.14453125\n",
      "simplex mem 654.14453125\n",
      "persistence mem 654.14453125\n",
      "rips max mem 654.14453125\n",
      "rips mem 4 654.14453125\n",
      "simplex mem 654.14453125\n",
      "persistence mem 654.14453125\n",
      "rips max mem 654.14453125\n",
      "got RIPS\n",
      "got G\n",
      "[0, 4, 22, 23, 26, 27, 28, 30, 33, 37, 49, 53, 58, 59, 62, 66, 69, 75, 43, 79, 73, 52, 77, 32, 78, 1, 2, 3, 7, 9, 11, 18, 20, 21, 44, 45, 50, 57, 63, 64, 65, 68, 76, 46, 17, 13, 51, 6, 12, 16, 55, 56, 61, 72, 47, 54, 71, 81, 15, 14, 74, 60, 67, 70, 82, 80, 42, 19, 29, 5, 38, 10, 34, 31, 36, 8, 35, 48, 24, 39, 41, 40]\n",
      "0    {'shortest_path_unweighted': 2, 'nodes': 82, '...\n",
      "1    {'shortest_path_unweighted': 2, 'nodes': 68, '...\n",
      "2    {'shortest_path_unweighted': 1, 'nodes': 89, '...\n",
      "3    {'shortest_path_unweighted': 2, 'nodes': 17, '...\n",
      "4    {'shortest_path_unweighted': 1, 'nodes': 59, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 653.19921875\n",
      "simplex mem 653.19921875\n",
      "persistence mem 653.03515625\n",
      "rips max mem 653.03515625\n",
      "rips mem 1 653.03515625\n",
      "simplex mem 653.0234375\n",
      "persistence mem 685.48828125\n",
      "rips max mem 685.48828125\n",
      "rips mem 2 685.48828125\n",
      "simplex mem 653.0234375\n",
      "persistence mem 652.765625\n",
      "rips max mem 652.765625\n",
      "rips mem 3 652.765625\n",
      "simplex mem 652.74609375\n",
      "persistence mem 652.671875\n",
      "rips max mem 652.671875\n",
      "rips mem 4 652.671875\n",
      "simplex mem 652.671875\n",
      "persistence mem 652.671875\n",
      "rips max mem 652.671875\n",
      "got RIPS\n",
      "got G\n",
      "[0, 1, 2, 23, 27, 29, 39, 45, 46, 47, 50, 51, 55, 60, 65, 66, 72, 78, 79, 80, 81, 85, 86, 87, 43, 58, 52, 61, 83, 11, 12, 22, 64, 49, 54, 56, 68, 62, 69, 71, 73, 84, 88, 77, 38, 82, 89, 14, 67, 48, 40, 53, 63, 3, 4, 6, 18, 19, 26, 30, 34, 70, 74, 7, 20, 21, 75, 9, 24, 25, 16, 32, 37, 36, 76, 15, 33, 41, 8, 5, 10, 57, 13, 42, 28, 17, 35, 44, 31]\n",
      "0    {'shortest_path_unweighted': 2, 'nodes': 89, '...\n",
      "1    {'shortest_path_unweighted': 1, 'nodes': 101, ...\n",
      "2    {'shortest_path_unweighted': 2, 'nodes': 75, '...\n",
      "3    {'shortest_path_unweighted': 1, 'nodes': 78, '...\n",
      "4    {'shortest_path_unweighted': 1, 'nodes': 33, '...\n",
      "Name: graph, dtype: object\n",
      "completed! PEM_df window: utterances\n",
      "rips mem 0 610.43359375\n",
      "simplex mem 609.83203125\n",
      "persistence mem 609.6328125\n",
      "rips max mem 609.6328125\n",
      "rips mem 1 609.6328125\n",
      "simplex mem 609.6328125\n",
      "persistence mem 609.6328125\n",
      "rips max mem 609.6328125\n",
      "rips mem 2 609.6328125\n",
      "simplex mem 644.64453125\n",
      "persistence mem 897.00390625\n",
      "rips max mem 897.00390625\n",
      "rips mem 3 897.00390625\n",
      "simplex mem 796.19921875\n",
      "persistence mem 843.6328125\n",
      "rips max mem 843.6328125\n",
      "rips mem 4 843.6328125\n",
      "simplex mem 796.19921875\n",
      "persistence mem 796.19921875\n",
      "rips max mem 796.19921875\n",
      "got RIPS\n",
      "got G\n",
      "[0, 1, 2, 4, 7, 10, 15, 16, 19, 21, 25, 27, 28, 29, 33, 35, 36, 40, 41, 43, 46, 50, 52, 57, 59, 60, 61, 64, 70, 75, 76, 79, 80, 81, 83, 90, 97, 84, 12, 24, 26, 38, 48, 82, 63, 17, 18, 58, 6, 67, 56, 14, 31, 45, 51, 86, 87, 96, 88, 30, 44, 34, 71, 11, 77, 62, 91, 72, 8, 55, 66, 3, 32, 95, 22, 47, 94, 53, 78, 20, 5, 85, 93, 65, 23, 13, 42, 49, 54, 92, 9, 73, 37, 39]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_with_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnodes())\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#df_with_graph['euler'] = df_with_graph['rt_simplex_tree'].apply(lambda st: compute_euler_characteristic(st, max_dim=4))\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Apply the function to each graph in df_with_graph\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m graph_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mdf_with_graph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgraph\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_graph_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(graph_metrics)\n\u001b[1;32m    105\u001b[0m graph_metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(graph_metrics\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn [2], line 251\u001b[0m, in \u001b[0;36mcompute_graph_metrics\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    248\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_triangles\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nx\u001b[38;5;241m.\u001b[39mtriangles(G)\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Each triangle counted 3 times\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Number of tetrahedra (4-cliques)\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_tetrahedra\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mavg_tetr_cc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure avg_tetr_cc is defined\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Clustering Coefficient\u001b[39;00m\n\u001b[1;32m    255\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclustering_coefficient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39maverage_clustering(G, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [2], line 187\u001b[0m, in \u001b[0;36mavg_tetr_cc\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    184\u001b[0m tetrahedra \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mislice(itertools\u001b[38;5;241m.\u001b[39mgroupby(\n\u001b[1;32m    185\u001b[0m     nx\u001b[38;5;241m.\u001b[39menumerate_all_cliques(g), \u001b[38;5;28mlen\u001b[39m), \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     tetrahedra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtetrahedra\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/networkx/algorithms/clique.py:96\u001b[0m, in \u001b[0;36menumerate_all_cliques\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m base\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, u \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cnbrs):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Use generators to reduce memory consumption.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     queue\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     95\u001b[0m         (\n\u001b[0;32m---> 96\u001b[0m             \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;28mfilter\u001b[39m(nbrs[u]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, islice(cnbrs, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import shutil\n",
    "df_names=['PEM_df','SER_monologs', 'SER_IPSP', 'SER1']\n",
    "data_save_dir=working_dir+'TDA_output/'\n",
    "os.makedirs(data_save_dir, exist_ok=True)\n",
    "\n",
    "completed_files=os.listdir(data_save_dir)\n",
    "# for overlap in [0.1,0.2,0.4]:\n",
    "#     for window in [60,80,100,120,140,160,180,200]:\n",
    "\n",
    "# for overlap in [0.1,0.2,0.4]:\n",
    "#     for window in [60,80,100,120,140,160,180,200]:\n",
    "        \n",
    "for overlap in [0.1]:\n",
    "    for window in ['utterances']:\n",
    "        layers='last'\n",
    "\n",
    "        dfs=[df_PEM, df_monologs, df_SER2, df_SER_MA]\n",
    "\n",
    "        for df_no, df_monolog in enumerate(dfs):\n",
    "\n",
    "            df_name=df_names[df_no]\n",
    "#             if newfilename in completed_files:\n",
    "#                 print(f'Already completed {newfilename}')\n",
    "#                 continue\n",
    "            data_save_dir_name=working_dir+f'TDA_output/{df_name}/'\n",
    "            if os.path.exists(data_save_dir_name):\n",
    "                shutil.rmtree(data_save_dir_name) \n",
    "            os.makedirs(data_save_dir_name, exist_ok=True)\n",
    "\n",
    "            if window=='utterances':\n",
    "                with open(f'{dir_array}/utterance_{df_name}_sentence_embeddings_arrays.pkl', 'rb') as f:\n",
    "                    embeds= pickle.load(f)   \n",
    "                newfilename=f'{df_name}_utterance_distance_results.csv'\n",
    "\n",
    "            else:\n",
    "                step=int(window*overlap)#4\n",
    "                with open(f'{dir_array}/{window}_{step}_{df_name}_sentence_embeddings_arrays.pkl', 'rb') as f:\n",
    "                    embeds = pickle.load(f)   \n",
    "                newfilename=f'{df_name}_{window}_{step}_distance_results.csv'\n",
    "            \n",
    "            if len(embeds)!=len(df_monolog):\n",
    "                raise Exception('MISMATCH IN LENGTH')\n",
    "                \n",
    "            df_monolog['sentence_embeddings'] = embeds\n",
    "\n",
    "\n",
    "            print(len(embeds))\n",
    "            print(len(df_monolog))\n",
    "            if len(embeds)!=len(df_monolog):\n",
    "                raise Exception('MISMATCH IN LENGTH')\n",
    "\n",
    "            df_monolog['sentence_embeddings'] = embeds\n",
    "            df_monolog['length'] = [len(i) for i in embeds]\n",
    "            if test_mode:\n",
    "                df_monolog=df_monolog[0:10]\n",
    "            df_monolog = df_monolog[\n",
    "                df_monolog[\"sentence_embeddings\"].apply(\n",
    "                    lambda x: (\n",
    "                        not isinstance(x, float)               # exclude floats\n",
    "                        and isinstance(x, (list, tuple, np.ndarray))  # must be list/tuple/np.ndarray\n",
    "                        and len(x) >= 3                        # length >= 3\n",
    "                    )\n",
    "                )]\n",
    "\n",
    "            # 4) Apply that function to each row -> produce a new column\n",
    "            if reduce_dims:\n",
    "                all_vecs = []\n",
    "                for row in df_monolog['sentence_embeddings']:\n",
    "                    arr = np.array(row)  \n",
    "                    all_vecs.append(arr)\n",
    "                big_matrix = np.concatenate(all_vecs, axis=0)\n",
    "                pca = PCA(n_components=50)\n",
    "                pca.fit(big_matrix)\n",
    "                def transform_embeddings(emb_list):\n",
    "                    emb_array = np.array(emb_list)   # shape (k_i, 384)\n",
    "                    emb_pca = pca.transform(emb_array)  # shape (k_i, 50)\n",
    "                    return emb_pca\n",
    "                df_monolog['sentence_embeddings'] = df_monolog['sentence_embeddings'].apply(transform_embeddings)\n",
    "            df_monolog['token_embeddings']=None\n",
    "            print('performing TDA on ',df_name, ' window: ', window)#, 'step: ', step)\n",
    "            \n",
    "            for fi in range(0, len(df_monolog), chunk_size):\n",
    "                \n",
    "                if fi+chunk_size>=len(df_monolog):\n",
    "                    df_subset=df_monolog[fi:].reset_index(drop=True)\n",
    "                else:\n",
    "                    df_subset=df_monolog[fi:fi+chunk_size].reset_index(drop=True)\n",
    "                \n",
    "\n",
    "                drugs=list(set(df_subset['Drug']))\n",
    "                #Participants=list(set(df_subset['Participant']))\n",
    "                df_subset=get_rips_time(df_subset,embeddings=embeddings)\n",
    "               # df_subset=get_rips_time_centroid(df_subset,embeddings=embeddings)\n",
    "                #df_monolog=get_simplices_over_time(df_monolog,simplex_tree_type='simplex_tree')\n",
    "                print('got RIPS')\n",
    "                df_with_graph=get_rips_complex_G(df_subset)\n",
    "                print('got G')\n",
    "                print(df_with_graph['graph'][0].nodes())\n",
    "                #df_with_graph['euler'] = df_with_graph['rt_simplex_tree'].apply(lambda st: compute_euler_characteristic(st, max_dim=4))\n",
    "                # Apply the function to each graph in df_with_graph\n",
    "                graph_metrics = df_with_graph['graph'].apply(compute_graph_metrics)\n",
    "\n",
    "                print(graph_metrics)\n",
    "                graph_metrics_df = pd.DataFrame(graph_metrics.tolist())\n",
    "                df_with_graph = pd.concat([df_with_graph, graph_metrics_df], axis=1)\n",
    "                            # Create a new DataFrame\n",
    "\n",
    "                # We'll accumulate new rows in a list of dicts\n",
    "                new_rows = []\n",
    "                dimensions = [0, 1, 2]\n",
    "\n",
    "                for idx, row in df_with_graph.iterrows():\n",
    "                    embed = row[embeddings]  # Adjust as needed\n",
    "                    # We’ll store births, deaths, pers LENGTHS in a dict keyed by dimension\n",
    "                    dim_dict = {\n",
    "                        dim: {'births': [], 'deaths': [], 'pers': []}\n",
    "                        for dim in dimensions\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "                    # Build the Rips Complex for *this row only*\n",
    "                    rips_complex =row['rips']\n",
    "                    try:\n",
    "                        simplex_tree = \\\n",
    "                        rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                    persistence = simplex_tree.persistence()\n",
    "\n",
    "                    # Collect intervals by dimension\n",
    "                    for dim, (b, d) in persistence:\n",
    "                        if d == float('inf'):\n",
    "                            continue\n",
    "                        if dim in dimensions:\n",
    "                            dim_dict[dim]['births'].append(b)\n",
    "                            dim_dict[dim]['deaths'].append(d)\n",
    "                            dim_dict[dim]['pers'].append(d - b)\n",
    "\n",
    "\n",
    "                    row_dict = row.to_dict()  # Start with original row's columns\n",
    "\n",
    "                    for dim in dimensions:\n",
    "                        bdp = dim_dict[dim]\n",
    "                        stats_dict = compute_distribution_stats(bdp['births'], bdp['deaths'], bdp['pers'])\n",
    "                        # prefix each stat key with dim\n",
    "                        for stat_key, stat_val in stats_dict.items():\n",
    "                            row_dict[f\"{stat_key}_dim{dim}\"] = stat_val\n",
    "\n",
    "                    # Add row_dict to new_rows\n",
    "                    new_rows.append(row_dict)\n",
    "\n",
    "                # Create a new DataFrame\n",
    "                df_with_tda = pd.DataFrame(new_rows)\n",
    "                if window=='utterances':\n",
    "                    df_with_tda.to_csv(data_save_dir_name + f'{df_name}_{fi}_{window}_TDA_results.csv')\n",
    "                else:\n",
    "                    df_with_tda.to_csv(data_save_dir_name + f'{df_name}_{fi}_{window}_{step}_TDA_results.csv')\n",
    "                print(f'completed! {df_name} window: {window}')\n",
    "                del df_subset\n",
    "                del df_with_graph\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6f12745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using max pooling\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94ef75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "130\n",
      "performing TDA on  PEM_df  span:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:   0%|                                 | 0/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 83\u001b[0m\n\u001b[1;32m     78\u001b[0m      df_subset\u001b[38;5;241m=\u001b[39mdf_monolog[fi:fi\u001b[38;5;241m+\u001b[39mchunk_size]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# drugs=list(set(df_subset['Drug']))\u001b[39;00m\n\u001b[1;32m     82\u001b[0m  \u001b[38;5;66;03m#Participants=list(set(df_subset['Participant']))\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m  df_subset\u001b[38;5;241m=\u001b[39m\u001b[43mget_rips_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# df_subset=get_rips_time_centroid(df_subset,embeddings=embeddings)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m  \u001b[38;5;66;03m#df_monolog=get_simplices_over_time(df_monolog,simplex_tree_type='simplex_tree')\u001b[39;00m\n\u001b[1;32m     86\u001b[0m  \u001b[38;5;66;03m#print('got RIPS')\u001b[39;00m\n\u001b[1;32m     87\u001b[0m  df_with_graph\u001b[38;5;241m=\u001b[39mget_rips_complex_G(df_subset)\n",
      "Cell \u001b[0;32mIn [38], line 590\u001b[0m, in \u001b[0;36mget_rips_time\u001b[0;34m(df, embeddings, step)\u001b[0m\n\u001b[1;32m    588\u001b[0m             df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscales_dim\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m scales\n\u001b[1;32m    589\u001b[0m             df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malive_dim\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m alive_components\n\u001b[0;32m--> 590\u001b[0m         rips_complex_max \u001b[38;5;241m=\u001b[39m \u001b[43mgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRipsComplex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_edge_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m         current_mem_mb \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m    593\u001b[0m       \u001b[38;5;66;03m#  print('rips max mem', current_mem_mb)\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m#         if current_mem_mb > memory_threshold_mb:\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;66;03m#             continue\u001b[39;00m\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;66;03m#simplex_tree_max = rips_complex.create_simplex_tree(max_dimension=dims_simplex)\u001b[39;00m\n",
      "File \u001b[0;32mrips_complex.pyx:73\u001b[0m, in \u001b[0;36mgudhi.rips_complex.RipsComplex.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aee518af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEM_df 130\n",
      "SER_IPSP 107\n",
      "SER_monologs 100\n",
      "MASM 66\n",
      "cleaned_DEI 73\n"
     ]
    }
   ],
   "source": [
    "SP=2\n",
    "for name in ['PEM_df','SER_IPSP','SER_monologs','MASM', 'cleaned_DEI']:\n",
    "    data_save_dir=working_dir+'TDA_output/'\n",
    "    data_save_dir_name=working_dir+f'TDA_output/{name}_{SP}/'\n",
    "    list_files=sorted(os.listdir(data_save_dir_name))\n",
    "    dfs=[]\n",
    "    for f in list_files:\n",
    "        dfs.append(pd.read_csv(data_save_dir_name+f))\n",
    "    data=pd.concat(dfs)\n",
    "    data.to_csv(data_save_dir+f'{name}_{SP}_back_utterance_TDA_results.csv')\n",
    "    print(name, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e6ce221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names=['PEM_df','SER_monologs', 'SER_IPSP', 'SER1', 'MASM', 'cleaned_DEI']\n",
    "    data_save_dir=working_dir+'TDA_output/'\n",
    "    data_save_dir_name=working_dir+f'TDA_output/{name}_{1}/'\n",
    "    list_files=sorted(os.listdir(data_save_dir_name))\n",
    "    dfs=[]\n",
    "    for f in list_files:\n",
    "        dfs.append(pd.read_csv(data_save_dir_name+f))\n",
    "    data=pd.concat(dfs)\n",
    "    data.to_csv(data_save_dir+f'{name}_{1}_utterance_TDA_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dca2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEM_df 130\n",
      "SER_IPSP 107\n",
      "SER_monologs 100\n",
      "MASM 66\n",
      "cleaned_DEI 73\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fe8de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['PEM_df','SER_monologs', 'SER_IPSP', 'SER1']:\n",
    "    data_save_dir=working_dir+'TDA_output/'\n",
    "    data_save_dir_name=working_dir+f'TDA_output/{name}/'\n",
    "    list_files=sorted(os.listdir(data_save_dir_name))\n",
    "    dfs=[]\n",
    "    for f in list_files:\n",
    "        dfs.append(pd.read_csv(data_save_dir_name+f))\n",
    "    data=pd.concat(dfs)\n",
    "    data.to_csv(data_save_dir+f'{name}_utterances_TDA_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6d1309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ll16598/Documents/POSTDOC/TDA_output/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50def7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_dir_name=working_dir+f'TDA_output/{df_name}/'\n",
    "os.makedirs(data_save_dir_name, exist_ok=True)\n",
    "\n",
    "df_monolog['token_embeddings']=None\n",
    "print('performing TDA on ',df_name, ' window: ', window, 'step: ', step)\n",
    "\n",
    "for fi in range(0, len(df_monolog), chunk_size):\n",
    "\n",
    "    if fi+chunk_size>=len(df_monolog):\n",
    "        df_subset=df_monolog[fi:]\n",
    "    else:\n",
    "        df_subset=df_monolog[fi:fi+chunk_size]\n",
    "\n",
    "    df_subset=get_rips_time(df_subset,embeddings=embeddings)\n",
    "    df_subset=get_rips_time_centroid(df_subset,embeddings=embeddings)\n",
    "    print('completed rips')\n",
    "    df_subset=get_simplices_over_time(df_subset,simplex_tree_type='simplex_tree')\n",
    "    for D in [2, 3, 4]:\n",
    "        # Explode the lists in the columns for the current dimension.\n",
    "        try:\n",
    "            df_exploded = df_subset.explode([f\"simplex_time_dim{D}_filtration\", f\"simplex_time_dim{D}_count\"])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "       # Convert the exploded columns to numeric.\n",
    "        df_exploded[f\"simplex_time_dim{D}_filtration\"] = pd.to_numeric(df_exploded[f\"simplex_time_dim{D}_filtration\"])\n",
    "        df_exploded[f\"simplex_time_dim{D}_count\"] = pd.to_numeric(df_exploded[f\"simplex_time_dim{D}_count\"])\n",
    "\n",
    "        # Group by \"Drug\" and the filtration values, and compute the mean and standard error for the counts.\n",
    "        grouped = df_exploded.groupby([\"Drug\", f\"simplex_time_dim{D}_filtration\"], as_index=False).agg(\n",
    "            alive_mean=(f\"simplex_time_dim{D}_count\", \"mean\"),\n",
    "            alive_se=(f\"simplex_time_dim{D}_count\", sem)  # standard error\n",
    "        )\n",
    "        if save:\n",
    "            df_exploded.to_csv(data_save_dir_name + f'{fi}_{df_name}_{window}_{step}_{D}_skeleton_simplices_over_time.csv', index=False)\n",
    "\n",
    "\n",
    "        if plot:\n",
    "            import matplotlib.pyplot as plt\n",
    "            # Create a plot for the current dimension.\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "            # Iterate over each drug group and plot mean ± SE.\n",
    "            for drug_level, df_sub in grouped.groupby(\"Drug\"):\n",
    "                ax.errorbar(\n",
    "                    df_sub[f\"simplex_time_dim{D}_filtration\"],\n",
    "                    df_sub[\"alive_mean\"],\n",
    "                    yerr=df_sub[\"alive_se\"],\n",
    "                    label=f\"Drug={drug_level}\",\n",
    "                    marker='o',\n",
    "                    capsize=3\n",
    "                )\n",
    "\n",
    "            ax.set_xlabel(\"Filtration Value (Distance Threshold)\")\n",
    "            ax.set_ylabel(\"Number of Alive Components (Mean ± SE)\")\n",
    "            ax.set_title(f\"Dimension {D} Alive Components Over Filtration Value by Drug\")\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "\n",
    "    for D in [0,1,2]:\n",
    "        df_exploded = df_subset.explode([f\"scales_dim{D}\", f'alive_dim{D}'])\n",
    "        df_exploded[f\"scales_dim{D}\"] = pd.to_numeric(df_exploded[f\"scales_dim{D}\"])\n",
    "        df_exploded[f'alive_dim{D}'] = pd.to_numeric(df_exploded[f'alive_dim{D}'])\n",
    "        grouped = df_exploded.groupby([\"Drug\", f\"scales_dim{D}\"], as_index=False).agg(\n",
    "            alive_mean=(f'alive_dim{D}', \"mean\"),\n",
    "            alive_se=(f'alive_dim{D}', sem)  # standard error\n",
    "        )\n",
    "        df_exploded.to_csv(data_save_dir_name+f'{fi}_{df_name}_{window}_{step}_{D}_simplices_over_time.csv')\n",
    "        if plot:\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "            # We'll iterate over each drug and plot mean ± SE\n",
    "            for drug_level, df_sub in grouped.groupby(\"Drug\"):\n",
    "                ax.errorbar(\n",
    "                    df_sub[f\"scales_dim{D}\"], \n",
    "                    df_sub[\"alive_mean\"], \n",
    "                    yerr=df_sub[\"alive_se\"], \n",
    "                    label=f\"Drug={drug_level}\",\n",
    "                    marker='o',\n",
    "                    capsize=3\n",
    "                )\n",
    "\n",
    "            ax.set_xlabel(\"Scale (distance threshold)\")\n",
    "            ax.set_ylabel(\"Number of Alive Components (Mean ± SE)\")\n",
    "            ax.set_title(\"Connected Components Over Scale by Drug\")\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "\n",
    "    df_with_graph=get_rips_complex_G(df_subset)\n",
    "    #df_with_graph['euler'] = df_with_graph['rt_rips'].apply(lambda st: compute_euler_characteristic(st, max_dim=4))\n",
    "    # Apply the function to each graph in df_with_graph\n",
    "    graph_metrics = df_with_graph['graph'].apply(compute_graph_metrics)\n",
    "    graph_metrics_df = pd.DataFrame(graph_metrics.tolist())\n",
    "    df_with_graph = pd.concat([df_with_graph, graph_metrics_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    dimensions = [0, 1, 2]\n",
    "\n",
    "    # We'll accumulate new rows in a list of dicts\n",
    "    new_rows = []\n",
    "\n",
    "    for idx, row in df_subset.iterrows():\n",
    "        embed = row[embeddings]  # Adjust as needed\n",
    "        # We’ll store births, deaths, pers LENGTHS in a dict keyed by dimension\n",
    "        dim_dict = {\n",
    "            dim: {'births': [], 'deaths': [], 'pers': []}\n",
    "            for dim in dimensions\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        # Build the Rips Complex for *this row only*\n",
    "        rips_complex =row['rips']\n",
    "        try:\n",
    "            simplex_tree = rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        persistence = simplex_tree.persistence()\n",
    "\n",
    "        # Collect intervals by dimension\n",
    "        for dim, (b, d) in persistence:\n",
    "            if d == float('inf'):\n",
    "                continue\n",
    "            if dim in dimensions:\n",
    "                dim_dict[dim]['births'].append(b)\n",
    "                dim_dict[dim]['deaths'].append(d)\n",
    "                dim_dict[dim]['pers'].append(d - b)\n",
    "\n",
    "\n",
    "        row_dict = row.to_dict()  # Start with original row's columns\n",
    "\n",
    "        for dim in dimensions:\n",
    "            bdp = dim_dict[dim]\n",
    "            stats_dict = compute_distribution_stats(bdp['births'], bdp['deaths'], bdp['pers'])\n",
    "            # prefix each stat key with dim\n",
    "            for stat_key, stat_val in stats_dict.items():\n",
    "                row_dict[f\"{stat_key}_dim{dim}\"] = stat_val\n",
    "\n",
    "        # Add row_dict to new_rows\n",
    "        new_rows.append(row_dict)\n",
    "\n",
    "    # Create a new DataFrame\n",
    "    print('completed',f'{df_name}_{window}_{step}')\n",
    "    df_with_tda = pd.DataFrame(new_rows)\n",
    "    if reduce_dims:\n",
    "        df_with_tda.to_csv(data_save_dir_name + f'{fi}_D50_{SP}{df_name}_{window}_{step}_TDA_results.csv')\n",
    "    else:\n",
    "        df_with_tda.to_csv(data_save_dir_name + f'{fi}_{SP}{df_name}_{window}_{step}_TDA_results.csv')\n",
    "    print(f'completed! {df_name} window: {window} step size: {step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12afe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_names=['SER_monologs', 'PEM_df', 'SER_IPSP', 'SER1']\n",
    "data_save_dir=working_dir+'TDA_output/'\n",
    "os.makedirs(data_save_dir, exist_ok=True)\n",
    "\n",
    "completed_files=os.listdir(data_save_dir)\n",
    "# for overlap in [0.1,0.2,0.4]:\n",
    "#     for window in [60,80,100,120,140,160,180,200]:\n",
    "\n",
    "# for overlap in [0.1,0.2,0.4]:\n",
    "#     for window in [60,80,100,120,140,160,180,200]:\n",
    "        \n",
    "for overlap in [0.1]:\n",
    "    for window in [100]:\n",
    "        step=int(window*overlap)#4\n",
    "        layers='last'\n",
    "\n",
    "        dfs=[df_monologs, df_PEM, df_SER2, df_SER_MA]\n",
    "\n",
    "        for df_no, df_monolog in enumerate(dfs):\n",
    "\n",
    "            df_name=df_names[df_no]\n",
    "            newfilename=f'{df_name}_{window}_{step}_TDA_results.csv'\n",
    "#             if newfilename in completed_files:\n",
    "#                 print(f'Already completed {newfilename}')\n",
    "#                 continue\n",
    "        \n",
    "            with open(f'{dir_array}/{window}_{step}_{df_name}_sentence_embeddings_arrays.pkl', 'rb') as f:\n",
    "                embeds = pickle.load(f)\n",
    "            \n",
    "            df_monolog['sentence_embeddings'] = embeds\n",
    "\n",
    "\n",
    "            print(len(embeds))\n",
    "            print(len(df_monolog))\n",
    "            if len(embeds)!=len(df_monolog):\n",
    "                raise Exception('MISMATCH IN LENGTH')\n",
    "\n",
    "            df_monolog['sentence_embeddings'] = embeds\n",
    "            df_monolog['length'] = [len(i) for i in embeds]\n",
    "            if test_mode:\n",
    "                df_monolog=df_monolog[0:10]\n",
    "            df_monolog = df_monolog[\n",
    "                df_monolog[\"sentence_embeddings\"].apply(\n",
    "                    lambda x: (\n",
    "                        not isinstance(x, float)               # exclude floats\n",
    "                        and isinstance(x, (list, tuple, np.ndarray))  # must be list/tuple/np.ndarray\n",
    "                        and len(x) >= 3                        # length >= 3\n",
    "                    )\n",
    "                )]\n",
    "\n",
    "            # 4) Apply that function to each row -> produce a new column\n",
    "            if reduce_dims:\n",
    "                all_vecs = []\n",
    "                for row in df_monolog['sentence_embeddings']:\n",
    "                    arr = np.array(row)  \n",
    "                    all_vecs.append(arr)\n",
    "                big_matrix = np.concatenate(all_vecs, axis=0)\n",
    "                pca = PCA(n_components=50)\n",
    "                pca.fit(big_matrix)\n",
    "                def transform_embeddings(emb_list):\n",
    "                    emb_array = np.array(emb_list)   # shape (k_i, 384)\n",
    "                    emb_pca = pca.transform(emb_array)  # shape (k_i, 50)\n",
    "                    return emb_pca\n",
    "                df_monolog['sentence_embeddings'] = df_monolog['sentence_embeddings'].apply(transform_embeddings)\n",
    "            df_monolog['token_embeddings']=None\n",
    "            print('performing TDA on ',df_name, ' window: ', window, 'step: ', step)\n",
    "\n",
    "            for fi in range(0, len(df_monolog), chunk_size):\n",
    "                \n",
    "                if fi+chunk_size>=len(df_monolog):\n",
    "                    df_subset=df_monolog[fi:]\n",
    "                else:\n",
    "                    df_subset=df_monolog[fi:fi+chunk_size]\n",
    "                \n",
    "\n",
    "                drugs=list(set(df_subset['Drug']))\n",
    "                Participants=list(set(df_subset['Participant']))\n",
    "                df_subset=get_rips_time(df_subset,embeddings=embeddings)\n",
    "               # df_subset=get_rips_time_centroid(df_subset,embeddings=embeddings)\n",
    "                #df_monolog=get_simplices_over_time(df_monolog,simplex_tree_type='simplex_tree')\n",
    "\n",
    "                df_with_graph=get_rips_complex_G(df_subset)\n",
    "                df_with_graph['euler'] = df_with_graph['rt_simplex_tree'].apply(lambda st: compute_euler_characteristic(st, max_dim=4))\n",
    "                # Apply the function to each graph in df_with_graph\n",
    "                graph_metrics = df_with_graph['graph'].apply(compute_graph_metrics)\n",
    "                print(graph_metrics)\n",
    "                graph_metrics_df = pd.DataFrame(graph_metrics.tolist())\n",
    "                df_with_graph = pd.concat([df_with_graph, graph_metrics_df], axis=1)\n",
    "                            # Create a new DataFrame\n",
    "                df_with_graph.to_csv(data_save_dir + f'{df_name}_{fi}_{window}_{step}_TDA_results.csv')\n",
    "                df_subset=None\n",
    "                df_with_graph=None\n",
    "                print(f'completed! {df_name} window: {window} step size: {step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5048858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rips_time(df, embeddings='sentence_embeddings', step=0.025):\n",
    "    \"\"\"\n",
    "    For each row in df, build a Rips complex, extract dimension-D intervals\n",
    "    (e.g., D=0 => connected components, D=1 => loops, etc.),\n",
    "    then compute how many such features are 'alive' at increments of 'step'.\n",
    "\n",
    "    Creates two new columns in df:\n",
    "    - f\"scales_dim{D}\": The scale values\n",
    "    - f\"alive_dim{D}\": The counts of alive features at each scale\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid SettingWithCopy warnings\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Prepare two new columns (lists)\n",
    "    df[f'scales_dim0'] = None\n",
    "    df[f'alive_dim0'] = None\n",
    "    df[f'scales_dim1'] = None\n",
    "    df[f'alive_dim1'] = None\n",
    "    df[f'scales_dim2'] = None\n",
    "    df[f'alive_dim2'] = None\n",
    "    df['rt'] = None\n",
    "    df['simplex_tree']=None\n",
    "    df[\"rt_rips\"]=None\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Get the embeddings for this row\n",
    "        embed = row[embeddings]\n",
    "        if not isinstance(embed, (list, np.ndarray)) or len(embed) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Build the Rips Complex\n",
    "        rips_complex = gd.RipsComplex(points=embed, max_edge_length=3)\n",
    "        simplex_tree = rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "\n",
    "        # Extract dimension-D intervals from persistence\n",
    "        persistence = simplex_tree.persistence()\n",
    "        for D in [0,1,2]:\n",
    "            births_dimD = []\n",
    "            deaths_dimD = []\n",
    "\n",
    "            for dim, (b, d) in persistence:\n",
    "                if dim == D and d != float('inf'):  # ignoring infinite intervals\n",
    "                    births_dimD.append(b)\n",
    "                    deaths_dimD.append(d)\n",
    "\n",
    "            # Compute how many features are alive at each scale\n",
    "            scales, alive_components = get_alive_components_over_scales(births_dimD, deaths_dimD, step=step)\n",
    "            if len(deaths_dimD)>0:\n",
    "                df.at[idx, f\"rt\"] = max(deaths_dimD)\n",
    "\n",
    "            # Store these lists in the new columns\n",
    "            df.at[idx, f\"scales_dim{D}\"] = scales\n",
    "            df.at[idx, f\"alive_dim{D}\"] = alive_components\n",
    "        try:\n",
    "            rips_complex_max = gd.RipsComplex(points=embed, max_edge_length=df[\"rt\"].loc[idx])\n",
    "        except Exception as e:\n",
    "            print(idx, df[\"rt\"].loc[idx], embed)\n",
    "        simplex_tree_max = rips_complex.create_simplex_tree(max_dimension=dims_simplex)\n",
    "        df.at[idx, f\"simplex_tree\"] = simplex_tree\n",
    "        df.at[idx, f\"rt_simplex_tree\"] = simplex_tree_max\n",
    "        df.at[idx, f\"rt_rips\"] = rips_complex_max\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rips_simplicial_complex(rt_rips, simp_tree, dataset_name, entry, max_edge_length=5.0,SAVE=False):\n",
    "    \"\"\"\n",
    "    1) Builds a Rips complex (via GUDHI) from a set of high-dimensional points.\n",
    "    2) Extracts simplices (up to dimension 2) from the simplex tree.\n",
    "       - Edges (1-simplices) and triangles (2-simplices).\n",
    "    3) Uses PCA to reduce the points to 3D.\n",
    "    4) Plots a 3D visualization:\n",
    "       - Nodes are shown as a scatter plot.\n",
    "       - Edges are drawn as lines.\n",
    "       - Triangles are drawn as filled polygons (using Poly3DCollection).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed : np.ndarray of shape (N, D)\n",
    "        The high-dimensional point cloud.\n",
    "    max_edge_length : float\n",
    "        The maximum edge length used in the Rips complex.\n",
    "    \"\"\"\n",
    "#     # 1) Build the Rips complex and create the simplex tree\n",
    "#     rips_complex = gd.RipsComplex(points=embed, max_edge_length=max_edge_length)\n",
    "    simplex_tree = rt_rips.create_simplex_tree(max_dimension=5)\n",
    "    \n",
    "#     # 2) Extract simplices:\n",
    "    edges = []\n",
    "    triangles = []\n",
    "    \n",
    "    # get_skeleton(2) returns all simplices up to dimension 2\n",
    "    for simplex, fvalue in simplex_tree.get_skeleton(4):\n",
    "        if len(simplex) == 2:\n",
    "            # 1-simplices: edges\n",
    "            edges.append(simplex)\n",
    "        elif len(simplex) == 3:\n",
    "            # 2-simplices: triangles\n",
    "            triangles.append(simplex)\n",
    "    G=nx.Graph()        \n",
    "    for simplex, fvalue in simplex_tree.get_skeleton(4):\n",
    "        if len(simplex) >= 2:\n",
    "            for (i, j) in itertools.combinations(simplex, 2):\n",
    "                G.add_edge(i, j, weight=fvalue)\n",
    "    metrics=compute_graph_metrics(G)\n",
    "    print(metrics)\n",
    "    # 3) Use PCA to reduce the point cloud to 3D\n",
    "    pca = PCA(n_components=3)\n",
    "    coords_3d = pca.fit_transform(embed)  # shape (N, 3)\n",
    "    n_points = coords_3d.shape[0]\n",
    "    \n",
    "    # Prepare colormap for nodes (using 'magma_r')\n",
    "    norm = plt.Normalize(vmin=0, vmax=n_points - 1)\n",
    "    cmap = plt.get_cmap('plasma_r')\n",
    "    node_colors = cmap(norm(np.arange(n_points)))\n",
    "    \n",
    "    # 4) Create the 3D plot\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot nodes\n",
    "    sc = ax.scatter(coords_3d[:, 0], coords_3d[:, 1], coords_3d[:, 2],\n",
    "                    c=node_colors, s=30, alpha=0.9)\n",
    "    \n",
    "    # Plot edges as lines\n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        x_vals = [coords_3d[i, 0], coords_3d[j, 0]]\n",
    "        y_vals = [coords_3d[i, 1], coords_3d[j, 1]]\n",
    "        z_vals = [coords_3d[i, 2], coords_3d[j, 2]]\n",
    "        # Optionally, color edge based on one endpoint's index or the average.\n",
    "        avg_idx = int(np.mean(edge))\n",
    "        edge_color = cmap(norm(avg_idx))\n",
    "        ax.plot(x_vals, y_vals, z_vals, color=edge_color, alpha=0.8, linewidth=1.5)\n",
    "    \n",
    "    # Plot triangles as filled faces\n",
    "    face_polys = []\n",
    "    face_colors = []\n",
    "    for tri in triangles:\n",
    "        # Get the 3 vertices for this triangle\n",
    "        pts = [coords_3d[idx] for idx in tri]\n",
    "        face_polys.append(pts)\n",
    "        # Color can be computed from the average index of the triangle's vertices\n",
    "        avg_idx = int(np.mean(tri))\n",
    "        face_colors.append(cmap(norm(avg_idx)))\n",
    "    \n",
    "    # Create a Poly3DCollection for the triangles with a set transparency (alpha)\n",
    "    poly_collection = Poly3DCollection(face_polys, alpha=0.3, edgecolor='k')\n",
    "    poly_collection.set_facecolor(face_colors)\n",
    "    ax.add_collection3d(poly_collection)\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(f\"\", pad=20)\n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")\n",
    "    ax.set_zlabel(\"PCA 3\")\n",
    "    \n",
    "    # Add colorbar for node indices\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, pad=0.1)\n",
    "    cbar.set_label(\"Node Index\")\n",
    "        # Define three different viewing angles\n",
    "    angles = [(15, 180), (30, 90), (45, 0)]  # (elevation, azimuth) in degrees\n",
    "    dir_fig_save=working_dir+f'rips_skeletons/{dataset_name}_{window}_{step}/'\n",
    "    os.makedirs(dir_fig_save, exist_ok=True)\n",
    "\n",
    "    # Save figures from different angles\n",
    "    for i, (elev, azim) in enumerate(angles):\n",
    "        ax.view_init(elev=elev, azim=azim)  # Set camera angle\n",
    "        filename = dir_fig_save+f\"{entry}_{i}.png\"\n",
    "        if SAVE:\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')  # Save figure\n",
    "       # print(f\"Saved: {filename}\")\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "idx=2\n",
    "embed=df_with_graph['sentence_embeddings'][idx]\n",
    "simp_tre=df_with_graph['rt_rips'][idx]\n",
    "rt=df_with_graph['rt'][idx]\n",
    "visualize_rips_simplicial_complex(simp_tre, embed, 'x', 1, max_edge_length=rt,SAVE=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0448f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing TDA on  SER_IPSP  window:  120 step:  24\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'simplex_time_dim2_filtration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'simplex_time_dim2_filtration'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 60\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Assuming df_monolog is your DataFrame and data_save_dir, df_name, window, and step are defined.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# For each dimension (2, 3, 4) we explode the corresponding columns and then group by Drug and filtration values.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m D \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Explode the lists in the columns for the current dimension.\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     df_exploded \u001b[38;5;241m=\u001b[39m \u001b[43mdf_monolog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimplex_time_dim\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mD\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_filtration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimplex_time_dim\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mD\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Convert the exploded columns to numeric.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     df_exploded[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplex_time_dim\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_filtration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df_exploded[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplex_time_dim\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_filtration\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/core/frame.py:8884\u001b[0m, in \u001b[0;36mDataFrame.explode\u001b[0;34m(self, column, ignore_index)\u001b[0m\n\u001b[1;32m   8882\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   8883\u001b[0m     mylen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m (is_list_like(x) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 8884\u001b[0m     counts0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(mylen)\n\u001b[1;32m   8885\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m   8886\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(counts0 \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m[c]\u001b[38;5;241m.\u001b[39mapply(mylen)):\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/SICC_AUG_22/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'simplex_time_dim2_filtration'"
     ]
    }
   ],
   "source": [
    "\n",
    "            \n",
    "            # Assuming df_monolog is your DataFrame and data_save_dir, df_name, window, and step are defined.\n",
    "            # For each dimension (2, 3, 4) we explode the corresponding columns and then group by Drug and filtration values.\n",
    "\n",
    "            for D in [2, 3, 4]:\n",
    "                # Explode the lists in the columns for the current dimension.\n",
    "                df_exploded = df_monolog.explode([f\"simplex_time_dim{D}_filtration\", f\"simplex_time_dim{D}_count\"])\n",
    "\n",
    "                # Convert the exploded columns to numeric.\n",
    "                df_exploded[f\"simplex_time_dim{D}_filtration\"] = pd.to_numeric(df_exploded[f\"simplex_time_dim{D}_filtration\"])\n",
    "                df_exploded[f\"simplex_time_dim{D}_count\"] = pd.to_numeric(df_exploded[f\"simplex_time_dim{D}_count\"])\n",
    "\n",
    "                # Group by \"Drug\" and the filtration values, and compute the mean and standard error for the counts.\n",
    "                grouped = df_exploded.groupby([\"Drug\", f\"simplex_time_dim{D}_filtration\"], as_index=False).agg(\n",
    "                    alive_mean=(f\"simplex_time_dim{D}_count\", \"mean\"),\n",
    "                    alive_se=(f\"simplex_time_dim{D}_count\", sem)  # standard error\n",
    "                )\n",
    "                if save:\n",
    "                    df_exploded.to_csv(data_save_dir + f'{df_name}_{window}_{step}_{D}_skeleton_simplices_over_time.csv', index=False)\n",
    "\n",
    "\n",
    "                if plot:\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    # Create a plot for the current dimension.\n",
    "                    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "                    # Iterate over each drug group and plot mean ± SE.\n",
    "                    for drug_level, df_sub in grouped.groupby(\"Drug\"):\n",
    "                        ax.errorbar(\n",
    "                            df_sub[f\"simplex_time_dim{D}_filtration\"],\n",
    "                            df_sub[\"alive_mean\"],\n",
    "                            yerr=df_sub[\"alive_se\"],\n",
    "                            label=f\"Drug={drug_level}\",\n",
    "                            marker='o',\n",
    "                            capsize=3\n",
    "                        )\n",
    "\n",
    "                    ax.set_xlabel(\"Filtration Value (Distance Threshold)\")\n",
    "                    ax.set_ylabel(\"Number of Alive Components (Mean ± SE)\")\n",
    "                    ax.set_title(f\"Dimension {D} Alive Components Over Filtration Value by Drug\")\n",
    "                    ax.legend()\n",
    "                    plt.show()\n",
    "\n",
    "            for D in [0,1,2]:\n",
    "                df_exploded = df_monolog.explode([f\"scales_dim{D}\", f'alive_dim{D}'])\n",
    "                df_exploded[f\"scales_dim{D}\"] = pd.to_numeric(df_exploded[f\"scales_dim{D}\"])\n",
    "                df_exploded[f'alive_dim{D}'] = pd.to_numeric(df_exploded[f'alive_dim{D}'])\n",
    "                grouped = df_exploded.groupby([\"Drug\", f\"scales_dim{D}\"], as_index=False).agg(\n",
    "                    alive_mean=(f'alive_dim{D}', \"mean\"),\n",
    "                    alive_se=(f'alive_dim{D}', sem)  # standard error\n",
    "                )\n",
    "                df_exploded.to_csv(data_save_dir+f'{df_name}_{window}_{step}_{D}_simplices_over_time.csv')\n",
    "                if plot:\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "                    # We'll iterate over each drug and plot mean ± SE\n",
    "                    for drug_level, df_sub in grouped.groupby(\"Drug\"):\n",
    "                        ax.errorbar(\n",
    "                            df_sub[f\"scales_dim{D}\"], \n",
    "                            df_sub[\"alive_mean\"], \n",
    "                            yerr=df_sub[\"alive_se\"], \n",
    "                            label=f\"Drug={drug_level}\",\n",
    "                            marker='o',\n",
    "                            capsize=3\n",
    "                        )\n",
    "\n",
    "                    ax.set_xlabel(\"Scale (distance threshold)\")\n",
    "                    ax.set_ylabel(\"Number of Alive Components (Mean ± SE)\")\n",
    "                    ax.set_title(\"Connected Components Over Scale by Drug\")\n",
    "                    ax.legend()\n",
    "                    plt.show()\n",
    "            \n",
    "\n",
    "            dimensions = [0, 1, 2]\n",
    "\n",
    "            # We'll accumulate new rows in a list of dicts\n",
    "            new_rows = []\n",
    "\n",
    "            for idx, row in df_with_graph.iterrows():\n",
    "                embed = row[embeddings]  # Adjust as needed\n",
    "                # We’ll store births, deaths, pers LENGTHS in a dict keyed by dimension\n",
    "                dim_dict = {\n",
    "                    dim: {'births': [], 'deaths': [], 'pers': []}\n",
    "                    for dim in dimensions\n",
    "                }\n",
    "\n",
    "                \n",
    "                    \n",
    "                # Build the Rips Complex for *this row only*\n",
    "                rips_complex = gd.RipsComplex(points=embed, max_edge_length=5.0)\n",
    "                simplex_tree = rips_complex.create_simplex_tree(max_dimension=3)\n",
    "                persistence = simplex_tree.persistence()\n",
    "\n",
    "                # Collect intervals by dimension\n",
    "                for dim, (b, d) in persistence:\n",
    "                    if d == float('inf'):\n",
    "                        continue\n",
    "                    if dim in dimensions:\n",
    "                        dim_dict[dim]['births'].append(b)\n",
    "                        dim_dict[dim]['deaths'].append(d)\n",
    "                        dim_dict[dim]['pers'].append(d - b)\n",
    "                        \n",
    "                        \n",
    "                row_dict = row.to_dict()  # Start with original row's columns\n",
    "\n",
    "                for dim in dimensions:\n",
    "                    bdp = dim_dict[dim]\n",
    "                    stats_dict = compute_distribution_stats(bdp['births'], bdp['deaths'], bdp['pers'])\n",
    "                    # prefix each stat key with dim\n",
    "                    for stat_key, stat_val in stats_dict.items():\n",
    "                        row_dict[f\"{stat_key}_dim{dim}\"] = stat_val\n",
    "\n",
    "                # Add row_dict to new_rows\n",
    "                new_rows.append(row_dict)\n",
    "\n",
    "            # Create a new DataFrame\n",
    "            df_with_tda = pd.DataFrame(new_rows)\n",
    "            df_with_tda.to_csv(data_save_dir + f'{df_name}_{window}_{step}_TDA_results.csv')\n",
    "            print(f'completed! {df_name} window: {window} step size: {step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a77671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
